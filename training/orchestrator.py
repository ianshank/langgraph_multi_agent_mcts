"""
Training Orchestrator Module

Coordinates the entire training pipeline including:
- Phased training schedule management
- Experiment tracking (MLflow/W&B)
- Resource allocation and distributed training
- Checkpoint management and resumption
"""

import json
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml
import numpy as np

try:
    import torch

    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import wandb

    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False

try:
    import mlflow

    HAS_MLFLOW = True
except ImportError:
    HAS_MLFLOW = False

from training.data_pipeline import DataOrchestrator
from training.agent_trainer import AgentTrainingOrchestrator
from training.rag_builder import RAGIndexManager
from training.meta_controller import MetaControllerTrainer
from training.evaluation import DABStepBenchmark, ProductionValidator

logger = logging.getLogger(__name__)


class PhaseManager:
    """Manage training phases with scheduling and resource allocation."""

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize phase manager.

        Args:
            config: Training configuration
        """
        self.config = config
        self.phases = self._define_phases()
        self.current_phase = 0
        self.phase_history = []

        logger.info(f"PhaseManager initialized with {len(self.phases)} phases")

    def _define_phases(self) -> List[Dict[str, Any]]:
        """Define training phases."""
        phases = [
            {
                "name": "base_pretraining",
                "description": "Base pre-training on DABStep + PRIMUS",
                "duration_hours": 48,  # 1-2 days
                "components": ["hrm", "trm", "rag"],
                "learning_rate": self.config["training"]["learning_rate"],
                "batch_size": self.config["training"]["batch_size"],
                "priority": "training",
            },
            {
                "name": "instruction_finetuning",
                "description": "Instruction fine-tuning with PRIMUS-Instruct",
                "duration_hours": 8,  # 4-8 hours
                "components": ["hrm", "trm"],
                "learning_rate": self.config["training"]["learning_rate"] / 2,
                "batch_size": self.config["training"]["batch_size"] * 2,
                "priority": "finetuning",
            },
            {
                "name": "mcts_self_play",
                "description": "MCTS self-play reinforcement learning",
                "duration_hours": 72,  # 2-3 days
                "components": ["mcts"],
                "learning_rate": self.config["agents"]["mcts"]["value_network"]["learning_rate"],
                "batch_size": 64,
                "priority": "training",
            },
            {
                "name": "meta_controller_training",
                "description": "Train meta-controller on collected traces",
                "duration_hours": 24,  # 1 day
                "components": ["router", "aggregator"],
                "learning_rate": self.config["meta_controller"]["router"]["learning_rate"],
                "batch_size": 64,
                "priority": "training",
            },
            {
                "name": "evaluation_and_validation",
                "description": "Comprehensive evaluation and production validation",
                "duration_hours": 4,
                "components": ["evaluation"],
                "priority": "validation",
            },
        ]
        return phases

    def get_current_phase(self) -> Dict[str, Any]:
        """Get current training phase."""
        if self.current_phase < len(self.phases):
            return self.phases[self.current_phase]
        return None

    def advance_phase(self, metrics: Dict[str, Any]) -> bool:
        """
        Advance to next phase if current phase is complete.

        Args:
            metrics: Current phase metrics

        Returns:
            True if advanced to next phase
        """
        if self.current_phase >= len(self.phases):
            return False

        # Record phase completion
        self.phase_history.append(
            {
                "phase": self.phases[self.current_phase]["name"],
                "completed_at": datetime.now().isoformat(),
                "metrics": metrics,
            }
        )

        self.current_phase += 1

        if self.current_phase < len(self.phases):
            logger.info(f"Advancing to phase: {self.phases[self.current_phase]['name']}")
            return True
        else:
            logger.info("All training phases completed")
            return False

    def estimate_remaining_time(self) -> timedelta:
        """Estimate remaining training time."""
        remaining_hours = sum(phase["duration_hours"] for phase in self.phases[self.current_phase :])
        return timedelta(hours=remaining_hours)

    def get_resource_allocation(self, phase: Dict[str, Any]) -> Dict[str, Any]:
        """Get resource allocation for a phase."""
        allocation = {
            "num_gpus": 1,
            "cpu_workers": self.config["resources"]["max_cpu_workers"],
            "memory_gb": 32,
            "priority": phase["priority"],
        }

        # Adjust for specific phases
        if phase["name"] == "mcts_self_play":
            allocation["num_gpus"] = 2 if HAS_TORCH and torch.cuda.device_count() > 1 else 1
            allocation["memory_gb"] = 64

        return allocation


class ExperimentTracker:
    """Track experiments with MLflow, W&B, or other platforms."""

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize experiment tracker.

        Args:
            config: Monitoring configuration
        """
        self.config = config.get("monitoring", {}).get("experiment_tracking", {})
        self.platform = self.config.get("platform", "wandb")
        self.project_name = self.config.get("project_name", "multi-agent-mcts-training")
        self.run_name = self.config.get("run_name") or f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.tags = self.config.get("tags", [])

        self.run = None
        self._initialized = False

        logger.info(f"ExperimentTracker using {self.platform}")

    def start_run(self, run_config: Dict[str, Any]) -> None:
        """Start a new experiment run."""
        if self.platform == "wandb" and HAS_WANDB:
            self.run = wandb.init(project=self.project_name, name=self.run_name, tags=self.tags, config=run_config)
            self._initialized = True
            logger.info(f"Started W&B run: {self.run_name}")

        elif self.platform == "mlflow" and HAS_MLFLOW:
            mlflow.set_experiment(self.project_name)
            self.run = mlflow.start_run(run_name=self.run_name)
            mlflow.log_params(self._flatten_config(run_config))
            self._initialized = True
            logger.info(f"Started MLflow run: {self.run_name}")

        else:
            # Fallback to local JSON logging
            self.run_dir = Path(f"./experiments/{self.run_name}")
            self.run_dir.mkdir(parents=True, exist_ok=True)
            with open(self.run_dir / "config.json", "w") as f:
                json.dump(run_config, f, indent=2)
            self._initialized = True
            logger.info(f"Started local experiment tracking: {self.run_dir}")

    def _flatten_config(self, config: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
        """Flatten nested config for MLflow."""
        flat = {}
        for key, value in config.items():
            full_key = f"{prefix}.{key}" if prefix else key
            if isinstance(value, dict):
                flat.update(self._flatten_config(value, full_key))
            else:
                flat[full_key] = value
        return flat

    def log_metrics(self, metrics: Dict[str, float], step: int = None) -> None:
        """Log metrics for current step."""
        if not self._initialized:
            return

        if self.platform == "wandb" and HAS_WANDB:
            wandb.log(metrics, step=step)

        elif self.platform == "mlflow" and HAS_MLFLOW:
            for key, value in metrics.items():
                mlflow.log_metric(key, value, step=step or 0)

        else:
            # Local logging
            metrics_file = self.run_dir / "metrics.jsonl"
            with open(metrics_file, "a") as f:
                record = {"step": step, **metrics, "timestamp": time.time()}
                f.write(json.dumps(record) + "\n")

    def log_artifact(self, artifact_path: str, artifact_type: str = "model") -> None:
        """Log an artifact (model checkpoint, plot, etc.)."""
        if not self._initialized:
            return

        if self.platform == "wandb" and HAS_WANDB:
            artifact = wandb.Artifact(f"{artifact_type}_{self.run_name}", type=artifact_type)
            artifact.add_file(artifact_path)
            self.run.log_artifact(artifact)

        elif self.platform == "mlflow" and HAS_MLFLOW:
            mlflow.log_artifact(artifact_path)

        else:
            # Copy to local artifacts
            import shutil

            dest = self.run_dir / "artifacts"
            dest.mkdir(exist_ok=True)
            shutil.copy(artifact_path, dest)

        logger.info(f"Logged artifact: {artifact_path}")

    def log_phase_completion(self, phase_name: str, metrics: Dict[str, Any]) -> None:
        """Log completion of a training phase."""
        self.log_metrics(
            {
                f"phase_{phase_name}_complete": 1.0,
                **{f"{phase_name}_{k}": v for k, v in metrics.items() if isinstance(v, (int, float))},
            }
        )

    def end_run(self) -> None:
        """End the current experiment run."""
        if not self._initialized:
            return

        if self.platform == "wandb" and HAS_WANDB:
            wandb.finish()

        elif self.platform == "mlflow" and HAS_MLFLOW:
            mlflow.end_run()

        else:
            # Finalize local run
            summary_file = self.run_dir / "summary.json"
            with open(summary_file, "w") as f:
                json.dump(
                    {"run_name": self.run_name, "end_time": datetime.now().isoformat(), "status": "completed"},
                    f,
                    indent=2,
                )

        logger.info(f"Ended experiment run: {self.run_name}")


class TrainingPipeline:
    """Main training pipeline coordinator."""

    def __init__(self, config_path: str = "training/config.yaml"):
        """
        Initialize training pipeline.

        Args:
            config_path: Path to configuration file
        """
        with open(config_path, "r") as f:
            self.config = yaml.safe_load(f)

        self.config_path = config_path

        # Initialize components
        self.phase_manager = PhaseManager(self.config)
        self.experiment_tracker = ExperimentTracker(self.config)

        # Training components (lazy initialization)
        self.data_orchestrator = None
        self.agent_trainer = None
        self.rag_manager = None
        self.meta_controller_trainer = None
        self.evaluator = None

        # State management
        self.checkpoint_dir = Path("training/models/checkpoints/pipeline")
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.global_step = 0
        self.start_time = None

        logger.info("TrainingPipeline initialized")

    def initialize_components(self) -> None:
        """Initialize all training components."""
        logger.info("Initializing training components...")

        self.data_orchestrator = DataOrchestrator(self.config_path)
        self.agent_trainer = AgentTrainingOrchestrator(self.config_path)
        self.rag_manager = RAGIndexManager(self.config_path)
        self.meta_controller_trainer = MetaControllerTrainer(self.config_path)

        # Initialize agent trainers
        self.agent_trainer.initialize_trainers()

        # Initialize evaluator
        self.evaluator = DABStepBenchmark(self.config["evaluation"])

        logger.info("All components initialized")

    def run_full_pipeline(self, resume_from: Optional[str] = None) -> Dict[str, Any]:
        """
        Run the complete training pipeline.

        Args:
            resume_from: Path to checkpoint to resume from

        Returns:
            Final training results
        """
        logger.info("Starting full training pipeline")
        self.start_time = datetime.now()

        # Start experiment tracking
        self.experiment_tracker.start_run(self.config)

        # Resume if checkpoint provided
        if resume_from:
            self._load_pipeline_state(resume_from)

        # Initialize components
        self.initialize_components()

        # Prepare data
        logger.info("Preparing data...")
        self.data_orchestrator.prepare_data()
        data_stats = self.data_orchestrator.get_data_statistics()
        self.experiment_tracker.log_metrics({"data_samples": data_stats["dabstep"]["train_samples"]})

        results = {"phases": [], "final_metrics": {}, "total_time": 0.0}

        # Run each phase
        while self.phase_manager.current_phase < len(self.phase_manager.phases):
            phase = self.phase_manager.get_current_phase()
            logger.info(f"Starting phase: {phase['name']}")

            phase_result = self._run_phase(phase)
            results["phases"].append(phase_result)

            # Log phase completion
            self.experiment_tracker.log_phase_completion(phase["name"], phase_result["metrics"])

            # Check early stopping conditions
            if self._should_stop_early(phase_result):
                logger.warning(f"Early stopping triggered in phase {phase['name']}")
                break

            # Advance to next phase
            self.phase_manager.advance_phase(phase_result["metrics"])

            # Save checkpoint
            self._save_pipeline_state()

        # Final evaluation
        logger.info("Running final evaluation...")
        final_metrics = self._run_final_evaluation()
        results["final_metrics"] = final_metrics

        # Calculate total time
        results["total_time"] = (datetime.now() - self.start_time).total_seconds()

        # End experiment tracking
        self.experiment_tracker.end_run()

        logger.info(f"Training pipeline completed in {results['total_time'] / 3600:.2f} hours")
        return results

    def _run_phase(self, phase: Dict[str, Any]) -> Dict[str, Any]:
        """Run a single training phase."""
        phase_start = time.time()

        result = {
            "phase_name": phase["name"],
            "started_at": datetime.now().isoformat(),
            "metrics": {},
            "duration_seconds": 0.0,
        }

        if phase["name"] == "base_pretraining":
            result["metrics"] = self._run_base_pretraining(phase)

        elif phase["name"] == "instruction_finetuning":
            result["metrics"] = self._run_instruction_finetuning(phase)

        elif phase["name"] == "mcts_self_play":
            result["metrics"] = self._run_mcts_self_play(phase)

        elif phase["name"] == "meta_controller_training":
            result["metrics"] = self._run_meta_controller_training(phase)

        elif phase["name"] == "evaluation_and_validation":
            result["metrics"] = self._run_evaluation_phase()

        result["duration_seconds"] = time.time() - phase_start
        result["completed_at"] = datetime.now().isoformat()

        logger.info(f"Phase {phase['name']} completed in {result['duration_seconds'] / 3600:.2f} hours")
        return result

    def _run_base_pretraining(self, phase: Dict[str, Any]) -> Dict[str, Any]:
        """Run base pre-training phase."""
        logger.info("Running base pre-training on DABStep and PRIMUS")

        metrics = {}

        # Get data loaders
        hrm_dataloader = self.data_orchestrator.get_hrm_dataloader("train")
        trm_dataloader = self.data_orchestrator.get_trm_dataloader("train")

        # Build RAG index
        logger.info("Building RAG indices...")
        rag_stats = self.rag_manager.build_domain_indices(self.data_orchestrator.primus_processor)
        metrics["rag_chunks"] = sum(s.total_chunks for s in rag_stats.values())

        # Train HRM and TRM
        num_epochs = self.config["training"]["epochs"]

        for epoch in range(num_epochs):
            # Train HRM
            hrm_loss = self.agent_trainer.hrm_trainer.train_epoch(hrm_dataloader)
            metrics[f"hrm_loss_epoch_{epoch}"] = hrm_loss

            # Train TRM
            trm_loss = self.agent_trainer.trm_trainer.train_epoch(trm_dataloader)
            metrics[f"trm_loss_epoch_{epoch}"] = trm_loss

            # Log to tracker
            self.experiment_tracker.log_metrics(
                {"hrm_loss": hrm_loss, "trm_loss": trm_loss, "epoch": epoch}, step=self.global_step
            )

            self.global_step += 1

            logger.info(f"Base pretraining epoch {epoch + 1}/{num_epochs}: HRM={hrm_loss:.4f}, TRM={trm_loss:.4f}")

        metrics["final_hrm_loss"] = hrm_loss
        metrics["final_trm_loss"] = trm_loss

        return metrics

    def _run_instruction_finetuning(self, phase: Dict[str, Any]) -> Dict[str, Any]:
        """Run instruction fine-tuning phase."""
        logger.info("Running instruction fine-tuning with PRIMUS-Instruct")

        metrics = {}

        # Get instruction samples
        instruction_samples = self.data_orchestrator.get_instruction_samples()
        metrics["instruction_samples"] = len(instruction_samples)

        # Fine-tune with lower learning rate
        # (In production, this would use actual instruction tuning)
        for i, sample in enumerate(instruction_samples[:100]):  # Subset for demo
            # Simulate fine-tuning step
            if i % 10 == 0:
                self.experiment_tracker.log_metrics(
                    {"instruction_step": i, "instruction_progress": i / len(instruction_samples)}, step=self.global_step
                )
                self.global_step += 1

        metrics["instruction_steps_completed"] = min(100, len(instruction_samples))

        return metrics

    def _run_mcts_self_play(self, phase: Dict[str, Any]) -> Dict[str, Any]:
        """Run MCTS self-play reinforcement learning."""
        logger.info("Running MCTS self-play training")

        metrics = {}

        games_per_iteration = self.config["agents"]["mcts"]["self_play"]["games_per_iteration"]
        num_iterations = 10  # Simplified

        for iteration in range(num_iterations):
            # Generate self-play data
            experiences = self.agent_trainer.mcts_trainer.generate_self_play_data(num_games=games_per_iteration)
            metrics[f"iteration_{iteration}_experiences"] = len(experiences)

            # Train on experiences
            # (Would implement actual MCTS training here)

            self.experiment_tracker.log_metrics(
                {"mcts_iteration": iteration, "buffer_size": len(self.agent_trainer.mcts_trainer.replay_buffer)},
                step=self.global_step,
            )

            self.global_step += 1

        metrics["total_self_play_games"] = num_iterations * games_per_iteration
        metrics["final_buffer_size"] = len(self.agent_trainer.mcts_trainer.replay_buffer)

        return metrics

    def _run_meta_controller_training(self, phase: Dict[str, Any]) -> Dict[str, Any]:
        """Run meta-controller training."""
        logger.info("Training meta-controller")

        metrics = {}

        # Generate synthetic traces if needed
        if len(self.meta_controller_trainer.trace_collector.traces) < 1000:
            self.meta_controller_trainer.trace_collector.generate_synthetic_traces(10000)

        # Train router
        router_history = self.meta_controller_trainer.train_router(num_epochs=10)
        metrics["router_final_loss"] = router_history["loss"][-1]
        metrics["router_final_accuracy"] = router_history["accuracy"][-1]

        # Save checkpoint
        self.meta_controller_trainer.save_checkpoint()

        self.experiment_tracker.log_metrics({"router_accuracy": router_history["accuracy"][-1]}, step=self.global_step)

        self.global_step += 1

        return metrics

    def _run_evaluation_phase(self) -> Dict[str, Any]:
        """Run comprehensive evaluation."""
        logger.info("Running evaluation and validation")

        metrics = {}

        # Get test data
        test_data = [
            {
                "task_id": f"eval_{i}",
                "task_text": f"Evaluation task {i}",
                "expected_output": f"Result {i}",
                "steps": [f"Step {j}" for j in range(3)],
                "difficulty": ["easy", "medium", "hard"][i % 3],
                "category": "reasoning",
            }
            for i in range(50)
        ]

        # Create mock model for evaluation
        class MockTrainedModel:
            def predict(self, sample):
                return {
                    "answer": sample["expected_output"],
                    "steps": len(sample.get("steps", [])),
                    "reasoning": sample.get("steps", []),
                    "confidence": 0.85,
                    "consensus": 0.9,
                }

        model = MockTrainedModel()

        # Run evaluation
        report = self.evaluator.evaluate_model(model, test_data, verbose=False)

        metrics["final_accuracy"] = report.accuracy
        metrics["final_f1"] = report.f1_score
        metrics["final_latency_ms"] = report.avg_latency_ms

        # Production validation
        validator = ProductionValidator(self.config["evaluation"])
        passed, checks = validator.validate_all_criteria(report)

        metrics["production_ready"] = float(passed)
        for check_name, check_passed in checks.items():
            metrics[f"check_{check_name}"] = float(check_passed)

        return metrics

    def _run_final_evaluation(self) -> Dict[str, Any]:
        """Run final comprehensive evaluation."""
        logger.info("Running final evaluation")

        metrics = {
            "pipeline_completed": True,
            "total_phases": len(self.phase_manager.phases),
            "completed_phases": self.phase_manager.current_phase,
            "estimated_remaining_hours": self.phase_manager.estimate_remaining_time().total_seconds() / 3600,
        }

        return metrics

    def _should_stop_early(self, phase_result: Dict[str, Any]) -> bool:
        """Check if training should stop early."""
        # Check for critical failures
        if "error" in phase_result:
            return True

        # Check for divergence
        metrics = phase_result.get("metrics", {})
        for key, value in metrics.items():
            if "loss" in key and isinstance(value, float) and value > 100:
                logger.warning(f"Loss divergence detected: {key}={value}")
                return True

        return False

    def _save_pipeline_state(self) -> None:
        """Save complete pipeline state for resumption."""
        state = {
            "current_phase": self.phase_manager.current_phase,
            "phase_history": self.phase_manager.phase_history,
            "global_step": self.global_step,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "config": self.config,
        }

        state_path = self.checkpoint_dir / "pipeline_state.json"
        with open(state_path, "w") as f:
            json.dump(state, f, indent=2)

        # Save agent checkpoints
        self.agent_trainer._save_all_checkpoints(self.phase_manager.current_phase)

        logger.info(f"Pipeline state saved to {state_path}")

    def _load_pipeline_state(self, checkpoint_path: str) -> None:
        """Load pipeline state from checkpoint."""
        with open(checkpoint_path, "r") as f:
            state = json.load(f)

        self.phase_manager.current_phase = state["current_phase"]
        self.phase_manager.phase_history = state["phase_history"]
        self.global_step = state["global_step"]

        if state["start_time"]:
            self.start_time = datetime.fromisoformat(state["start_time"])

        logger.info(f"Pipeline state loaded from {checkpoint_path}")


def main():
    """Main entry point for training pipeline."""
    import argparse

    parser = argparse.ArgumentParser(description="Multi-Agent MCTS Training Pipeline")
    parser.add_argument("--config", default="training/config.yaml", help="Path to config file")
    parser.add_argument("--resume", default=None, help="Path to checkpoint to resume from")
    parser.add_argument("--phase", default=None, help="Run specific phase only")

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    # Initialize pipeline
    pipeline = TrainingPipeline(args.config)

    if args.phase:
        # Run specific phase
        pipeline.initialize_components()
        phase = next((p for p in pipeline.phase_manager.phases if p["name"] == args.phase), None)
        if phase:
            result = pipeline._run_phase(phase)
            logger.info(f"Phase result: {result}")
        else:
            logger.error(f"Phase {args.phase} not found")
    else:
        # Run full pipeline
        results = pipeline.run_full_pipeline(resume_from=args.resume)
        logger.info(f"Pipeline results: {json.dumps(results, indent=2, default=str)}")


if __name__ == "__main__":
    main()
