"""
Advanced Embedding System with State-of-the-Art Models

Supports multiple embedding models and strategies:
- Voyage AI embeddings (voyage-large-2-instruct) - Top MTEB 2024
- Cohere Embed v3 (with Matryoshka compression)
- OpenAI text-embedding-3-large
- BGE-large-en-v1.5 (for fine-tuning)
- Sentence-transformers (fallback)

Features:
- Matryoshka embeddings (flexible dimensions)
- Batch processing for efficiency
- Caching to avoid re-embedding
- Async API calls
- Automatic fallback on errors
"""

import asyncio
import hashlib
import json
import logging
import os
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np

try:
    import voyageai

    HAS_VOYAGE = True
except ImportError:
    HAS_VOYAGE = False

try:
    import cohere

    HAS_COHERE = True
except ImportError:
    HAS_COHERE = False

try:
    import openai

    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from sentence_transformers import SentenceTransformer

    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False

logger = logging.getLogger(__name__)


@dataclass
class EmbeddingResult:
    """Result from embedding operation."""

    embeddings: np.ndarray
    model: str
    dimension: int
    cached: bool
    latency_ms: float
    metadata: dict[str, Any]


class BaseEmbedder(ABC):
    """Base class for all embedders."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize embedder.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.model_name = config.get("model", "")
        self.dimension = config.get("dimension", 1024)
        self.batch_size = config.get("batch_size", 32)
        self.cache_dir = Path(config.get("cache_dir", "./cache/embeddings"))
        self.cache_enabled = config.get("cache_enabled", True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed a list of texts.

        Args:
            texts: List of text strings

        Returns:
            Array of embeddings with shape (len(texts), dimension)
        """
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """Check if the embedder is available and configured."""
        pass

    def _get_cache_key(self, text: str) -> str:
        """Generate cache key for text."""
        content = f"{self.model_name}:{self.dimension}:{text}"
        return hashlib.sha256(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> np.ndarray | None:
        """Load embedding from cache."""
        if not self.cache_enabled:
            return None

        cache_file = self.cache_dir / f"{cache_key}.npy"
        if cache_file.exists():
            try:
                return np.load(cache_file)
            except Exception as e:
                logger.warning(f"Failed to load from cache: {e}")
        return None

    def _save_to_cache(self, cache_key: str, embedding: np.ndarray) -> None:
        """Save embedding to cache."""
        if not self.cache_enabled:
            return

        cache_file = self.cache_dir / f"{cache_key}.npy"
        try:
            np.save(cache_file, embedding)
        except Exception as e:
            logger.warning(f"Failed to save to cache: {e}")

    def embed_with_cache(self, texts: list[str]) -> EmbeddingResult:
        """
        Embed texts with caching support.

        Args:
            texts: List of text strings

        Returns:
            EmbeddingResult with embeddings and metadata
        """
        start_time = time.time()
        cached_embeddings = []
        texts_to_embed = []
        cache_keys = []

        # Check cache
        for text in texts:
            cache_key = self._get_cache_key(text)
            cache_keys.append(cache_key)
            cached = self._load_from_cache(cache_key)

            if cached is not None:
                cached_embeddings.append((len(cached_embeddings), cached))
            else:
                texts_to_embed.append((len(texts_to_embed), text))

        # Embed uncached texts
        if texts_to_embed:
            indices, uncached_texts = zip(*texts_to_embed, strict=False)
            new_embeddings = self.embed(list(uncached_texts))

            # Save to cache
            for i, (idx, _text) in enumerate(texts_to_embed):
                cache_key = cache_keys[idx]
                self._save_to_cache(cache_key, new_embeddings[i])
        else:
            new_embeddings = np.array([])

        # Combine cached and new embeddings
        all_embeddings = np.zeros((len(texts), self.dimension), dtype=np.float32)

        for idx, embedding in cached_embeddings:
            all_embeddings[idx] = embedding

        if len(texts_to_embed) > 0:
            for i, (idx, _) in enumerate(texts_to_embed):
                all_embeddings[idx] = new_embeddings[i]

        latency_ms = (time.time() - start_time) * 1000
        cached_count = len(cached_embeddings)
        total_count = len(texts)

        return EmbeddingResult(
            embeddings=all_embeddings,
            model=self.model_name,
            dimension=self.dimension,
            cached=cached_count == total_count,
            latency_ms=latency_ms,
            metadata={
                "total_texts": total_count,
                "cached_count": cached_count,
                "new_embeddings": total_count - cached_count,
                "cache_hit_rate": cached_count / total_count if total_count > 0 else 0,
            },
        )


class VoyageEmbedder(BaseEmbedder):
    """Voyage AI embeddings via API."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize Voyage embedder.

        Args:
            config: Configuration dictionary
        """
        super().__init__(config)
        self.model_name = config.get("model", "voyage-large-2-instruct")
        self.api_key = config.get("api_key") or os.environ.get("VOYAGE_API_KEY")
        self.input_type = config.get("input_type", "document")  # document or query
        self.truncate = config.get("truncate", True)

        # Voyage model dimensions
        self.dimension_map = {
            "voyage-large-2-instruct": 1024,
            "voyage-large-2": 1536,
            "voyage-2": 1024,
            "voyage-law-2": 1024,
            "voyage-code-2": 1536,
        }
        self.dimension = self.dimension_map.get(self.model_name, 1024)

        if HAS_VOYAGE and self.api_key:
            self.client = voyageai.Client(api_key=self.api_key)
            logger.info(f"VoyageEmbedder initialized: {self.model_name}")
        else:
            self.client = None
            if not HAS_VOYAGE:
                logger.warning("voyageai not installed. Install with: pip install voyageai")
            if not self.api_key:
                logger.warning("VOYAGE_API_KEY not set")

    def is_available(self) -> bool:
        """Check if Voyage is available."""
        return HAS_VOYAGE and self.client is not None

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed texts using Voyage AI.

        Args:
            texts: List of text strings

        Returns:
            Array of embeddings
        """
        if not self.is_available():
            raise RuntimeError("Voyage AI not available")

        try:
            # Batch processing to respect API limits
            all_embeddings = []
            for i in range(0, len(texts), self.batch_size):
                batch = texts[i : i + self.batch_size]
                result = self.client.embed(
                    batch, model=self.model_name, input_type=self.input_type, truncate=self.truncate
                )
                all_embeddings.extend(result.embeddings)

            return np.array(all_embeddings, dtype=np.float32)

        except Exception as e:
            logger.error(f"Voyage embedding failed: {e}")
            raise


class CohereEmbedder(BaseEmbedder):
    """Cohere Embed v3 with Matryoshka support."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize Cohere embedder.

        Args:
            config: Configuration dictionary
        """
        super().__init__(config)
        self.model_name = config.get("model", "embed-english-v3.0")
        self.api_key = config.get("api_key") or os.environ.get("COHERE_API_KEY")
        self.input_type = config.get("input_type", "search_document")  # search_document or search_query
        self.embedding_types = config.get("embedding_types", ["float"])
        self.truncate = config.get("truncate", "END")

        # Matryoshka dimensions (Cohere v3 supports flexible dimensions)
        self.full_dimension = 1024
        self.dimension = config.get("dimension", 1024)

        # Validate dimension for Matryoshka
        valid_dimensions = [1024, 512, 256, 128, 64]
        if self.dimension not in valid_dimensions:
            logger.warning(
                f"Dimension {self.dimension} not in recommended Matryoshka dimensions {valid_dimensions}. Using 1024."
            )
            self.dimension = 1024

        if HAS_COHERE and self.api_key:
            self.client = cohere.Client(api_key=self.api_key)
            logger.info(f"CohereEmbedder initialized: {self.model_name}, dimension={self.dimension}")
        else:
            self.client = None
            if not HAS_COHERE:
                logger.warning("cohere not installed. Install with: pip install cohere")
            if not self.api_key:
                logger.warning("COHERE_API_KEY not set")

    def is_available(self) -> bool:
        """Check if Cohere is available."""
        return HAS_COHERE and self.client is not None

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed texts using Cohere.

        Args:
            texts: List of text strings

        Returns:
            Array of embeddings
        """
        if not self.is_available():
            raise RuntimeError("Cohere not available")

        try:
            # Batch processing
            all_embeddings = []
            for i in range(0, len(texts), self.batch_size):
                batch = texts[i : i + self.batch_size]
                response = self.client.embed(
                    texts=batch,
                    model=self.model_name,
                    input_type=self.input_type,
                    embedding_types=self.embedding_types,
                    truncate=self.truncate,
                )

                # Extract float embeddings
                if hasattr(response, "embeddings"):
                    embeddings = response.embeddings.float
                else:
                    embeddings = response["embeddings"]["float"]

                all_embeddings.extend(embeddings)

            embeddings_array = np.array(all_embeddings, dtype=np.float32)

            # Apply Matryoshka compression if needed
            if self.dimension < self.full_dimension:
                embeddings_array = embeddings_array[:, : self.dimension]

            return embeddings_array

        except Exception as e:
            logger.error(f"Cohere embedding failed: {e}")
            raise


class OpenAIEmbedder(BaseEmbedder):
    """OpenAI embeddings."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize OpenAI embedder.

        Args:
            config: Configuration dictionary
        """
        super().__init__(config)
        self.model_name = config.get("model", "text-embedding-3-large")
        self.api_key = config.get("api_key") or os.environ.get("OPENAI_API_KEY")

        # OpenAI model dimensions
        self.dimension_map = {
            "text-embedding-3-large": 3072,
            "text-embedding-3-small": 1536,
            "text-embedding-ada-002": 1536,
        }
        self.full_dimension = self.dimension_map.get(self.model_name, 3072)

        # Support dimension reduction for text-embedding-3-* models
        self.dimension = config.get("dimension", self.full_dimension)
        if "text-embedding-3" in self.model_name:
            # Can reduce to any dimension
            valid_range = range(64, self.full_dimension + 1)
            if self.dimension not in valid_range:
                logger.warning(f"Dimension {self.dimension} out of range. Using {self.full_dimension}")
                self.dimension = self.full_dimension
        else:
            self.dimension = self.full_dimension

        if HAS_OPENAI and self.api_key:
            self.client = openai.OpenAI(api_key=self.api_key)
            logger.info(f"OpenAIEmbedder initialized: {self.model_name}, dimension={self.dimension}")
        else:
            self.client = None
            if not HAS_OPENAI:
                logger.warning("openai not installed. Install with: pip install openai")
            if not self.api_key:
                logger.warning("OPENAI_API_KEY not set")

    def is_available(self) -> bool:
        """Check if OpenAI is available."""
        return HAS_OPENAI and self.client is not None

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed texts using OpenAI.

        Args:
            texts: List of text strings

        Returns:
            Array of embeddings
        """
        if not self.is_available():
            raise RuntimeError("OpenAI not available")

        try:
            # Batch processing
            all_embeddings = []
            for i in range(0, len(texts), self.batch_size):
                batch = texts[i : i + self.batch_size]

                # Add dimension parameter for text-embedding-3-* models
                kwargs = {"input": batch, "model": self.model_name}
                if "text-embedding-3" in self.model_name and self.dimension != self.full_dimension:
                    kwargs["dimensions"] = self.dimension

                response = self.client.embeddings.create(**kwargs)

                embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(embeddings)

            return np.array(all_embeddings, dtype=np.float32)

        except Exception as e:
            logger.error(f"OpenAI embedding failed: {e}")
            raise


class BGEEmbedder(BaseEmbedder):
    """BGE-large-en-v1.5 embeddings (HuggingFace, for fine-tuning)."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize BGE embedder.

        Args:
            config: Configuration dictionary
        """
        super().__init__(config)
        self.model_name = config.get("model", "BAAI/bge-large-en-v1.5")
        self.dimension = 1024
        self.normalize = config.get("normalize", True)
        self.query_instruction = config.get(
            "query_instruction", "Represent this sentence for searching relevant passages:"
        )

        if HAS_SENTENCE_TRANSFORMERS:
            try:
                self.model = SentenceTransformer(self.model_name)
                self.dimension = self.model.get_sentence_embedding_dimension()
                logger.info(f"BGEEmbedder initialized: {self.model_name}")
            except Exception as e:
                logger.error(f"Failed to load BGE model: {e}")
                self.model = None
        else:
            self.model = None
            logger.warning("sentence-transformers not installed. Install with: pip install sentence-transformers")

    def is_available(self) -> bool:
        """Check if BGE is available."""
        return self.model is not None

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed texts using BGE.

        Args:
            texts: List of text strings

        Returns:
            Array of embeddings
        """
        if not self.is_available():
            raise RuntimeError("BGE model not available")

        try:
            embeddings = self.model.encode(
                texts, batch_size=self.batch_size, normalize_embeddings=self.normalize, convert_to_numpy=True
            )
            return embeddings.astype(np.float32)

        except Exception as e:
            logger.error(f"BGE embedding failed: {e}")
            raise


class SentenceTransformerEmbedder(BaseEmbedder):
    """Sentence-transformers embeddings (fallback)."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize Sentence-Transformer embedder.

        Args:
            config: Configuration dictionary
        """
        super().__init__(config)
        self.model_name = config.get("model", "sentence-transformers/all-MiniLM-L6-v2")
        self.normalize = config.get("normalize", True)

        if HAS_SENTENCE_TRANSFORMERS:
            try:
                self.model = SentenceTransformer(self.model_name)
                self.dimension = self.model.get_sentence_embedding_dimension()
                logger.info(f"SentenceTransformerEmbedder initialized: {self.model_name}")
            except Exception as e:
                logger.error(f"Failed to load model: {e}")
                self.model = None
        else:
            self.model = None
            logger.warning("sentence-transformers not installed. Install with: pip install sentence-transformers")

    def is_available(self) -> bool:
        """Check if model is available."""
        return self.model is not None

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed texts using sentence-transformers.

        Args:
            texts: List of text strings

        Returns:
            Array of embeddings
        """
        if not self.is_available():
            raise RuntimeError("Sentence-Transformer model not available")

        try:
            embeddings = self.model.encode(
                texts, batch_size=self.batch_size, normalize_embeddings=self.normalize, convert_to_numpy=True
            )
            return embeddings.astype(np.float32)

        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            raise


class EnsembleEmbedder(BaseEmbedder):
    """Combine multiple embedding models."""

    def __init__(self, config: dict[str, Any], embedders: list[BaseEmbedder] | None = None):
        """
        Initialize ensemble embedder.

        Args:
            config: Configuration dictionary
            embedders: List of embedders to combine
        """
        super().__init__(config)
        self.embedders = embedders or []
        self.combination_method = config.get("combination_method", "concat")  # concat, mean, weighted
        self.weights = config.get("weights")

        if self.combination_method == "weighted" and not self.weights:
            self.weights = [1.0 / len(self.embedders)] * len(self.embedders)

        # Calculate combined dimension
        if self.combination_method == "concat":
            self.dimension = sum(e.dimension for e in self.embedders)
        else:
            self.dimension = self.embedders[0].dimension if self.embedders else 1024

        self.model_name = f"ensemble_{len(self.embedders)}_models"
        logger.info(f"EnsembleEmbedder initialized with {len(self.embedders)} embedders")

    def is_available(self) -> bool:
        """Check if at least one embedder is available."""
        return any(e.is_available() for e in self.embedders)

    def add_embedder(self, embedder: BaseEmbedder) -> None:
        """Add an embedder to the ensemble."""
        self.embedders.append(embedder)

        # Recalculate dimension
        if self.combination_method == "concat":
            self.dimension = sum(e.dimension for e in self.embedders)

        # Update weights
        if self.combination_method == "weighted":
            self.weights = [1.0 / len(self.embedders)] * len(self.embedders)

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Embed texts using ensemble of models.

        Args:
            texts: List of text strings

        Returns:
            Array of combined embeddings
        """
        if not self.embedders:
            raise RuntimeError("No embedders in ensemble")

        embeddings_list = []
        available_weights = []

        for i, embedder in enumerate(self.embedders):
            if embedder.is_available():
                try:
                    emb = embedder.embed(texts)
                    embeddings_list.append(emb)
                    if self.combination_method == "weighted":
                        available_weights.append(self.weights[i])
                except Exception as e:
                    logger.warning(f"Embedder {embedder.model_name} failed: {e}")

        if not embeddings_list:
            raise RuntimeError("All embedders failed")

        # Combine embeddings
        if self.combination_method == "concat":
            return np.concatenate(embeddings_list, axis=1)
        elif self.combination_method == "mean":
            return np.mean(embeddings_list, axis=0)
        elif self.combination_method == "weighted":
            # Normalize weights
            total_weight = sum(available_weights)
            normalized_weights = [w / total_weight for w in available_weights]

            weighted_sum = np.zeros_like(embeddings_list[0])
            for emb, weight in zip(embeddings_list, normalized_weights, strict=False):
                weighted_sum += emb * weight
            return weighted_sum
        else:
            raise ValueError(f"Unknown combination method: {self.combination_method}")


class AsyncEmbedder:
    """Async wrapper for embedders."""

    def __init__(self, embedder: BaseEmbedder):
        """
        Initialize async embedder.

        Args:
            embedder: Base embedder to wrap
        """
        self.embedder = embedder

    async def embed_async(self, texts: list[str]) -> EmbeddingResult:
        """
        Embed texts asynchronously.

        Args:
            texts: List of text strings

        Returns:
            EmbeddingResult
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.embedder.embed_with_cache, texts)

    async def embed_batch_async(self, text_batches: list[list[str]]) -> list[EmbeddingResult]:
        """
        Embed multiple batches asynchronously.

        Args:
            text_batches: List of text batches

        Returns:
            List of EmbeddingResults
        """
        tasks = [self.embed_async(batch) for batch in text_batches]
        return await asyncio.gather(*tasks)


class EmbedderFactory:
    """Factory for creating embedders."""

    @staticmethod
    def create_embedder(config: dict[str, Any]) -> BaseEmbedder:
        """
        Create embedder from configuration.

        Args:
            config: Configuration dictionary with 'model' key

        Returns:
            Configured embedder instance
        """
        model = config.get("model", "sentence-transformers/all-MiniLM-L6-v2")

        # Determine embedder type from model name
        if "voyage" in model.lower():
            return VoyageEmbedder(config)
        elif "embed-" in model.lower() and "cohere" in model.lower() or "cohere" in config.get("provider", "").lower():
            return CohereEmbedder(config)
        elif "text-embedding" in model.lower() or "openai" in config.get("provider", "").lower():
            return OpenAIEmbedder(config)
        elif "bge" in model.lower():
            return BGEEmbedder(config)
        elif "sentence-transformers" in model or "/" in model:
            return SentenceTransformerEmbedder(config)
        else:
            logger.warning(f"Unknown model type: {model}. Using sentence-transformers fallback.")
            return SentenceTransformerEmbedder(config)

    @staticmethod
    def create_with_fallback(configs: list[dict[str, Any]]) -> BaseEmbedder:
        """
        Create embedder with automatic fallback.

        Args:
            configs: List of configurations, ordered by preference

        Returns:
            First available embedder
        """
        for config in configs:
            try:
                embedder = EmbedderFactory.create_embedder(config)
                if embedder.is_available():
                    logger.info(f"Using embedder: {embedder.model_name}")
                    return embedder
                else:
                    logger.warning(f"Embedder {config.get('model')} not available, trying next...")
            except Exception as e:
                logger.warning(f"Failed to create embedder {config.get('model')}: {e}")

        # Final fallback to simple random embeddings
        logger.error("All embedders failed, using random embeddings")
        return RandomEmbedder(configs[0] if configs else {"dimension": 384})


class RandomEmbedder(BaseEmbedder):
    """Random embeddings (for testing/fallback only)."""

    def __init__(self, config: dict[str, Any]):
        super().__init__(config)
        self.model_name = "random"
        self.dimension = config.get("dimension", 384)
        logger.warning("Using RandomEmbedder - for testing only!")

    def is_available(self) -> bool:
        return True

    def embed(self, texts: list[str]) -> np.ndarray:
        """Generate random embeddings."""
        return np.random.randn(len(texts), self.dimension).astype(np.float32)


def save_embeddings_metadata(embeddings: EmbeddingResult, output_path: Path) -> None:
    """
    Save embedding metadata to disk.

    Args:
        embeddings: EmbeddingResult to save
        output_path: Path to save metadata
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    metadata = {
        "model": embeddings.model,
        "dimension": embeddings.dimension,
        "cached": embeddings.cached,
        "latency_ms": embeddings.latency_ms,
        "metadata": embeddings.metadata,
        "shape": embeddings.embeddings.shape,
    }

    with open(output_path, "w") as f:
        json.dump(metadata, f, indent=2)


def load_embeddings(embeddings_path: Path, metadata_path: Path | None = None) -> EmbeddingResult:
    """
    Load embeddings from disk.

    Args:
        embeddings_path: Path to embeddings .npy file
        metadata_path: Path to metadata .json file

    Returns:
        EmbeddingResult
    """
    embeddings = np.load(embeddings_path)

    if metadata_path and metadata_path.exists():
        with open(metadata_path) as f:
            metadata = json.load(f)
    else:
        metadata = {
            "model": "unknown",
            "dimension": embeddings.shape[1],
            "cached": False,
            "latency_ms": 0.0,
            "metadata": {},
        }

    return EmbeddingResult(
        embeddings=embeddings,
        model=metadata["model"],
        dimension=metadata["dimension"],
        cached=metadata.get("cached", False),
        latency_ms=metadata.get("latency_ms", 0.0),
        metadata=metadata.get("metadata", {}),
    )


if __name__ == "__main__":
    # Test the embedders
    logging.basicConfig(level=logging.INFO)

    test_texts = [
        "This is a test document about cybersecurity.",
        "The MITRE ATT&CK framework provides comprehensive threat intelligence.",
        "Machine learning models require high-quality training data.",
    ]

    # Test configuration
    config = {
        "dimension": 1024,
        "batch_size": 32,
        "cache_dir": "./cache/embeddings",
        "cache_enabled": True,
    }

    # Test available embedders
    embedders_to_test = []

    # Voyage
    if os.environ.get("VOYAGE_API_KEY"):
        voyage_config = {**config, "model": "voyage-large-2-instruct"}
        voyage_embedder = VoyageEmbedder(voyage_config)
        if voyage_embedder.is_available():
            embedders_to_test.append(("Voyage", voyage_embedder))

    # Cohere
    if os.environ.get("COHERE_API_KEY"):
        cohere_config = {**config, "model": "embed-english-v3.0", "dimension": 1024}
        cohere_embedder = CohereEmbedder(cohere_config)
        if cohere_embedder.is_available():
            embedders_to_test.append(("Cohere", cohere_embedder))

    # OpenAI
    if os.environ.get("OPENAI_API_KEY"):
        openai_config = {**config, "model": "text-embedding-3-large", "dimension": 1024}
        openai_embedder = OpenAIEmbedder(openai_config)
        if openai_embedder.is_available():
            embedders_to_test.append(("OpenAI", openai_embedder))

    # BGE
    bge_config = {**config, "model": "BAAI/bge-large-en-v1.5"}
    bge_embedder = BGEEmbedder(bge_config)
    if bge_embedder.is_available():
        embedders_to_test.append(("BGE", bge_embedder))

    # Sentence-transformers fallback
    st_config = {**config, "model": "sentence-transformers/all-MiniLM-L6-v2", "dimension": 384}
    st_embedder = SentenceTransformerEmbedder(st_config)
    if st_embedder.is_available():
        embedders_to_test.append(("SentenceTransformers", st_embedder))

    # Test each embedder
    for name, embedder in embedders_to_test:
        logger.info(f"\nTesting {name}...")
        try:
            result = embedder.embed_with_cache(test_texts)
            logger.info(f"  Shape: {result.embeddings.shape}")
            logger.info(f"  Model: {result.model}")
            logger.info(f"  Dimension: {result.dimension}")
            logger.info(f"  Cached: {result.cached}")
            logger.info(f"  Latency: {result.latency_ms:.2f}ms")
            logger.info(f"  Cache hit rate: {result.metadata['cache_hit_rate']:.2%}")
        except Exception as e:
            logger.error(f"  Failed: {e}")

    # Test ensemble
    if len(embedders_to_test) >= 2:
        logger.info("\nTesting Ensemble...")
        ensemble_config = {**config, "combination_method": "mean"}
        ensemble = EnsembleEmbedder(ensemble_config, [e for _, e in embedders_to_test[:2]])
        try:
            result = ensemble.embed_with_cache(test_texts)
            logger.info(f"  Shape: {result.embeddings.shape}")
            logger.info(f"  Dimension: {result.dimension}")
        except Exception as e:
            logger.error(f"  Failed: {e}")

    logger.info("\nEmbedding tests complete!")
