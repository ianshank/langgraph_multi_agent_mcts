# Multi-Agent MCTS Training Pipeline Configuration

data:
  # DABStep multi-step reasoning dataset
  dabstep:
    path: "adyen/DABstep"
    cache_dir: "./cache/dabstep"
    max_samples: null  # Use all samples (450+ tasks)
    streaming: false
    train_split: 0.8
    val_split: 0.1
    test_split: 0.1
    seed: 42

  # PRIMUS cybersecurity document corpus
  primus_seed:
    path: "trendmicro-ailab/Primus-Seed"
    cache_dir: "./cache/primus_seed"
    categories:
      - "mitre"
      - "cyber_companies"
      - "wikipedia"
    max_documents: null  # Use all 674k documents
    streaming: true  # Required for large dataset

  # PRIMUS instruction tuning dataset
  primus_instruct:
    path: "trendmicro-ailab/Primus-Instruct"
    cache_dir: "./cache/primus_instruct"
    max_samples: null  # Use all 835 samples

  # ARC (Abstraction and Reasoning Corpus) for HRM training
  arc:
    path: "barc0/abstraction_and_reasoning_corpus"
    cache_dir: "./cache/arc"
    enabled: false  # Enable for HRM hierarchical reasoning training
    split: "train"  # train, evaluation, test
    max_samples: 1000  # ~1,000 training examples available
    trust_remote_code: true

  # GSM8K (Grade School Math) for mathematical reasoning
  gsm8k:
    path: "openai/gsm8k"
    cache_dir: "./cache/gsm8k"
    enabled: false  # Enable for TRM iterative refinement training
    config: "main"  # main or socratic
    split: "train"  # train (7.5k) or test (1k)
    max_samples: null  # Use all samples

  # IDoFT (Illinois Dataset of Flaky Tests) for quality engineering
  idoft:
    enabled: false  # Enable for quality engineering training
    data_path: null  # Path to local IDoFT clone (if available)
    cache_dir: "./cache/idoft"
    categories:  # Filter by flaky test categories
      - "async_wait"
      - "concurrency"
      - "time_dependent"
      - "unordered_collection"
      - "test_order_dependency"
    max_samples: null

  # HumanEval for code generation evaluation
  humaneval:
    path: "openai/openai_humaneval"
    cache_dir: "./cache/humaneval"
    enabled: false  # Enable for code generation training
    split: "test"  # HumanEval typically only has test split (164 problems)

  # Chess Games for MCTS strategic planning training
  chess_games:
    path: "angeluriot/chess_games"
    cache_dir: "./cache/chess_games"
    enabled: false  # Enable for MCTS policy training
    min_elo: 2000  # Minimum player ELO rating
    max_samples: 10000  # Limit samples (14M available)
    streaming: true  # Required for large dataset

  # BIG-Bench Hard for complex reasoning evaluation
  bigbench_hard:
    path: "maveriq/bigbenchhard"
    cache_dir: "./cache/bigbench_hard"
    enabled: false  # Enable for reasoning evaluation
    split: "train"  # train or validation
    tasks: null  # Specific tasks to load (null for all 23 tasks)
    max_samples: null

  # Data augmentation settings
  augmentation:
    enabled: true
    task_permutations: 3
    difficulty_scaling: true
    paraphrase_probability: 0.3

training:
  # General training parameters
  batch_size: 32
  learning_rate: 2e-5
  weight_decay: 0.01
  epochs: 10
  max_steps: null  # Use epochs if null
  warmup_ratio: 0.1
  gradient_accumulation_steps: 4
  fp16: true  # Mixed precision training
  gradient_clip_norm: 1.0

  # LoRA configuration
  lora:
    rank: 16
    alpha: 32
    dropout: 0.1
    target_modules:
      - "query"
      - "key"
      - "value"
      - "dense"

  # Curriculum learning
  curriculum:
    enabled: true
    phases:
      - difficulty: "easy"
        epochs: 3
      - difficulty: "medium"
        epochs: 4
      - difficulty: "hard"
        epochs: 3

  # Checkpointing
  checkpointing:
    save_strategy: "steps"
    save_steps: 1000
    save_total_limit: 5
    resume_from_checkpoint: null

  # Distributed training
  distributed:
    enabled: false
    backend: "nccl"
    world_size: null  # Auto-detect

agents:
  # Hierarchical Reasoning Model
  hrm:
    model_name: "microsoft/deberta-v3-base"
    max_decomposition_depth: 5
    min_subtasks: 2
    max_subtasks: 10
    learning_rate: 2e-5
    lora_rank: 16
    hidden_size: 768
    num_labels: 3  # START, CONTINUE, END decomposition

  # Task Refinement Model
  trm:
    model_name: "microsoft/deberta-v3-base"
    max_refinement_iterations: 3
    convergence_threshold: 0.95
    learning_rate: 2e-5
    lora_rank: 16
    hidden_size: 768
    improvement_threshold: 0.01

  # Monte Carlo Tree Search
  mcts:
    simulations: 200
    exploration_constant: 1.414
    discount_factor: 0.99
    value_network:
      hidden_layers: [256, 128, 64]
      activation: "relu"
      learning_rate: 1e-3
    policy_network:
      hidden_layers: [256, 128]
      temperature: 1.0
      learning_rate: 1e-3
    self_play:
      games_per_iteration: 100
      buffer_size: 10000
      update_frequency: 10

rag:
  # Vector embeddings
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  embedding_dim: 384

  # Advanced embeddings configuration
  embeddings:
    # Primary model (Voyage AI - Top MTEB 2024)
    model: "voyage-large-2-instruct"  # voyage-large-2-instruct, embed-english-v3.0, text-embedding-3-large, BAAI/bge-large-en-v1.5
    provider: "voyage"  # voyage, cohere, openai, huggingface
    dimension: 1024  # Full dimension (supports Matryoshka: 1024, 512, 256, 128 for Cohere/OpenAI)
    batch_size: 32
    cache_enabled: true
    cache_dir: "./cache/embeddings"

    # Fallback chain (tried in order if primary fails)
    fallback_models:
      - model: "embed-english-v3.0"
        provider: "cohere"
        dimension: 1024
      - model: "text-embedding-3-large"
        provider: "openai"
        dimension: 1024
      - model: "BAAI/bge-large-en-v1.5"
        provider: "huggingface"
        dimension: 1024
      - model: "sentence-transformers/all-MiniLM-L6-v2"
        provider: "huggingface"
        dimension: 384

    # Model-specific settings
    voyage:
      input_type: "document"  # document or query
      truncate: true

    cohere:
      input_type: "search_document"  # search_document or search_query
      truncate: "END"  # END, START, NONE
      embedding_types: ["float"]

    openai:
      model: "text-embedding-3-large"  # text-embedding-3-large, text-embedding-3-small

    # Ensemble settings
    ensemble:
      enabled: false
      models: ["voyage-large-2-instruct", "embed-english-v3.0"]
      combination_method: "mean"  # concat, mean, weighted
      weights: null  # Auto-calculated if null

    # Matryoshka embedding dimensions
    matryoshka:
      enabled: true
      dimensions: [1024, 512, 256, 128]
      default_dimension: 1024

  # Chunking strategy
  chunk_size: 512
  chunk_overlap: 50
  chunk_strategy: "semantic"  # semantic, fixed, recursive

  # Pinecone configuration
  pinecone:
    api_key: null  # Set via environment variable PINECONE_API_KEY
    environment: "us-east-1"  # AWS region for serverless
    cloud: "aws"  # Cloud provider: aws, gcp, azure
    index_name: "multi-agent-mcts-rag"
    namespace: "training"  # Namespace for vector isolation
    metric: "cosine"
    batch_size: 100  # Vectors per upsert batch

  # Index configuration (legacy for migration)
  index_type: "pinecone"  # pinecone, faiss (deprecated)
  index_path: "./cache/rag_index"  # For local BM25 cache
  num_neighbors: 10
  similarity_metric: "cosine"

  # Hybrid search
  hybrid_search:
    enabled: true
    bm25_weight: 0.3
    dense_weight: 0.7

  # Domain-specific indices (via Pinecone namespaces)
  domain_indices:
    - name: "cybersecurity"
      categories: ["mitre", "cyber_companies"]
      namespace: "cybersecurity"
    - name: "tactical"
      categories: ["wikipedia"]
      namespace: "tactical"
    - name: "general"
      categories: ["all"]
      namespace: "general"

meta_controller:
  # Router configuration
  router:
    model_type: "mlp"  # mlp, transformer
    input_features: 12
    hidden_layers: [128, 64, 32]
    num_agents: 3  # HRM, TRM, MCTS
    dropout: 0.2
    learning_rate: 1e-3

  # Aggregator configuration
  aggregator:
    method: "weighted_voting"  # weighted_voting, attention, ensemble
    confidence_threshold: 0.7
    fallback_strategy: "consensus"

  # Trace collection
  trace_collection:
    buffer_size: 50000
    sample_rate: 1.0
    features:
      - "task_complexity"
      - "agent_confidence"
      - "iteration_count"
      - "consensus_score"
      - "latency"
      - "memory_usage"

evaluation:
  # Test configuration
  test_size: 0.1
  validation_frequency: 100  # steps

  # Metrics to track
  metrics:
    - "accuracy"
    - "f1_score"
    - "precision"
    - "recall"
    - "latency_ms"
    - "memory_mb"
    - "consensus_rate"
    - "decomposition_depth"
    - "refinement_iterations"

  # Benchmarking
  benchmarks:
    dabstep:
      enabled: true
      report_path: "./reports/dabstep_benchmark.json"

  # Thresholds for success
  success_criteria:
    hrm_accuracy: 0.85
    trm_avg_iterations: 3.0
    mcts_win_rate: 0.75
    rag_precision_at_10: 0.90
    router_accuracy: 0.80

  # Adversarial testing
  adversarial:
    enabled: true
    attack_types:
      - "prompt_injection"
      - "input_perturbation"
      - "edge_cases"

monitoring:
  # Experiment tracking
  experiment_tracking:
    platform: "wandb"  # wandb, mlflow, braintrust
    project_name: "multi-agent-mcts-training"
    run_name: null  # Auto-generated
    tags: ["training", "multi-agent", "mcts"]

  # Logging configuration
  logging:
    level: "INFO"
    format: "json"
    log_dir: "./logs"
    max_bytes: 10485760  # 10MB
    backup_count: 5

  # Alerting
  alerts:
    loss_spike_threshold: 2.0
    gradient_explosion_threshold: 100.0
    oom_warning_threshold: 0.9  # 90% GPU memory

  # Profiling
  profiling:
    enabled: false
    cpu_profiling: false
    memory_profiling: true
    profile_steps: [10, 100, 1000]

continual_learning:
  # Production interaction logging
  logging:
    enabled: true
    storage: "./cache/production_logs"
    use_sqlite: true  # Use SQLite for queryable storage
    use_compression: true  # Compress JSON backups
    buffer_size: 1000  # Batch write buffer
    sanitize_pii: true  # Remove PII from logs
    use_langsmith: false  # Enable LangSmith integration
    langsmith_project: "production-feedback"

    # Data quality validation
    min_query_length: 3
    max_query_length: 5000
    min_response_length: 1
    max_response_length: 10000
    blocked_patterns: []  # Regex patterns to block

  # Failure pattern analysis
  failure_analysis:
    enabled: true
    min_cluster_size: 5  # Minimum failures to form a pattern
    similarity_threshold: 0.7  # For clustering similar failures
    analyze_frequency: "daily"  # How often to run analysis

  # Active learning for data selection
  active_learning:
    enabled: true
    budget_per_cycle: 50  # Number of samples to select for annotation
    selection_strategy: "hybrid"  # uncertainty, diversity, hybrid, failure
    diversity_weight: 0.3  # Weight for diversity in hybrid mode

  # Incremental retraining pipeline
  retraining:
    schedule: "weekly"  # daily, weekly, monthly
    min_new_samples: 100  # Minimum samples before retraining
    validation_split: 0.2  # Train/val split
    enable_ab_test: true  # A/B test new vs old model

    # Model update targets
    update_meta_controller: true  # Update routing model
    update_rag_index: true  # Add new documents to RAG
    update_benchmarks: true  # Re-run evaluation suite

  # Feedback collection (legacy compatibility)
  feedback:
    enabled: true
    buffer_size: 100000
    sample_rate: 0.1

  # Incremental training (EWC-based)
  incremental:
    retrain_threshold: 1000  # samples
    forgetting_prevention: "elastic_weight_consolidation"
    ewc_lambda: 1000.0

  # Data drift detection
  drift_detection:
    enabled: true
    window_size: 1000
    threshold: 0.1
    detection_method: "kolmogorov_smirnov"  # kolmogorov_smirnov, population_stability_index

  # A/B testing
  ab_testing:
    enabled: false
    traffic_split: 0.1  # 10% to new model
    min_samples: 1000
    confidence_level: 0.95

  # Metrics to track over time
  metrics_tracking:
    enabled: true
    track_satisfaction: true  # User satisfaction scores
    track_retrieval_quality: true  # Retrieval precision/recall
    track_agent_accuracy: true  # Agent selection accuracy
    track_latency: true  # Response latency distribution
    track_failure_rate: true  # Failure rate by category

    # Trending windows
    short_term_window: 24  # hours
    medium_term_window: 168  # 1 week in hours
    long_term_window: 720  # 30 days in hours

  # Privacy and security
  privacy:
    anonymize_user_ids: true
    hash_session_ids: true
    redact_sensitive_fields: true
    retention_days: 90  # Delete data after 90 days

  # Storage optimization
  storage:
    compress_old_logs: true  # Compress logs older than 7 days
    archive_threshold_days: 30  # Archive to cold storage
    max_db_size_gb: 10  # Maximum SQLite DB size

# Research corpus from arXiv
research_corpus:
  # Paper fetching configuration
  categories:
    - "cs.AI"      # Artificial Intelligence
    - "cs.LG"      # Machine Learning
    - "cs.CL"      # Computation and Language (NLP)
    - "cs.NE"      # Neural and Evolutionary Computing

  keywords:
    - "MCTS"
    - "Monte Carlo Tree Search"
    - "AlphaZero"
    - "MuZero"
    - "reinforcement learning"
    - "multi-agent"
    - "multi-agent systems"
    - "LLM reasoning"
    - "chain-of-thought"
    - "tree-of-thought"
    - "self-improvement"
    - "Constitutional AI"
    - "RLHF"
    - "reinforcement learning from human feedback"
    - "DPO"
    - "direct preference optimization"
    - "tree search"
    - "policy gradient"
    - "actor-critic"

  # Date range for papers (YYYY-MM-DD)
  date_start: "2020-01-01"
  date_end: "2025-12-31"  # Current year

  # Fetching limits
  max_results: 1000  # Total maximum papers to fetch
  max_per_keyword: 100  # Max papers per keyword
  max_per_category: 500  # Max papers per category

  # Rate limiting (arXiv API guidelines)
  rate_limit_delay: 3.0  # seconds between requests
  retry_attempts: 3

  # Processing configuration
  chunk_size: 512  # Characters per chunk
  chunk_overlap: 50  # Overlap between chunks
  include_citations: true
  extract_sections: true

  # Cache configuration
  cache_dir: "./cache/research_corpus"
  enable_caching: true

  # Integration with RAG pipeline
  auto_index: false  # Automatically add to Pinecone index
  pinecone_namespace: "arxiv_research"
  batch_size: 100  # Batch size for vector indexing

# Code corpus builder configuration
code_corpus:
  # Repository settings
  cache_dir: "./cache/code_repos"
  output_dir: "./cache/code_corpus"
  use_github_api: true
  github_token: null  # Set via GITHUB_TOKEN environment variable
  shallow_clone: true  # Use shallow clone to save space

  # Parsing settings
  min_function_lines: 3
  max_function_lines: 200
  extract_tests: true
  extract_examples: true
  find_tests: true
  include_tests: false  # Don't include test files in initial parse

  # Quality filtering
  min_quality_score: 0.5

  # Integration with RAG pipeline
  auto_index: false  # Automatically add to Pinecone index
  pinecone_namespace: "code-corpus"
  batch_size: 100  # Batch size for vector indexing

  # Repositories to process (priority order)
  repositories:
    high_priority:
      - "deepmind/mctx"
      - "langchain-ai/langgraph"
      - "openai/gym"
      - "karpathy/nanoGPT"
    medium_priority:
      - "facebookresearch/ReAgent"
      - "google-deepmind/alphatensor"
      - "microsoft/DeepSpeed"
    low_priority:
      - "huggingface/transformers"

# Resource constraints
resources:
  max_gpu_memory: null  # Auto-detect
  max_cpu_workers: 8
  prefetch_factor: 2
  pin_memory: true

# Multi-Modal Knowledge Base Configuration
multimodal:
  # Vision model for image description
  vision_model:
    model: "claude-3-5-sonnet-20241022"  # or "gpt-4-vision-preview"
    provider: "anthropic"  # anthropic or openai
    max_tokens: 1024
    temperature: 0.3

  # Image extraction and processing
  image_processor:
    min_size: [100, 100]  # Minimum image dimensions in pixels
    max_size: [4096, 4096]  # Maximum dimensions (will resize)
    formats: ["png", "jpg", "jpeg"]
    images_dir: "./cache/images"
    backend: "pymupdf"  # pymupdf or pdf2image

  # Multi-modal embeddings (CLIP)
  embeddings:
    clip_model: "openai/clip-vit-large-patch14"  # CLIP model for image-text embeddings
    text_model: "sentence-transformers/all-MiniLM-L6-v2"  # Text-only embeddings
    use_clip: true  # Enable cross-modal search
    cache_dir: "./cache/clip_models"

  # Code extraction
  code_extractor:
    extract_pseudocode: true
    min_code_lines: 3
    context_window: 200  # Characters of context around code
    supported_languages:
      - "python"
      - "javascript"
      - "java"
      - "cpp"
      - "pseudocode"

  # Visual index (Pinecone)
  visual_index:
    api_key: null  # Set via PINECONE_API_KEY env var
    index_name: "multi-agent-mcts-rag"  # Same index, different namespace
    namespace: "images"  # Namespace for image vectors
    batch_size: 100

  # Namespaces for different content types
  text_namespace: "multimodal_text"
  code_namespace: "multimodal_code"
  image_namespace: "images"

  # Processing options
  processing:
    concurrent_images: 5  # Parallel image description generation
    cache_descriptions: true
    cache_embeddings: true
    skip_large_images: true  # Skip images > max_size

  # OCR settings (optional)
  ocr:
    enabled: false
    engine: "pytesseract"  # pytesseract or easyocr
    languages: ["eng"]

  # Image classification
  classification:
    auto_classify: true  # Automatically classify image types
    types:
      - "architecture"
      - "flowchart"
      - "plot"
      - "table"
      - "tree"
      - "neural_network"
      - "algorithm"
      - "equation"

# Knowledge Graph Configuration
knowledge_graph:
  # Backend selection
  backend: "networkx"  # networkx (development) or neo4j (production)
  storage: "./cache/knowledge_graph"

  # Neo4j configuration (if using Neo4j backend)
  neo4j:
    uri: "bolt://localhost:7687"
    user: "neo4j"
    password: null  # Set via NEO4J_PASSWORD environment variable

  # Knowledge extraction settings
  extraction:
    llm_model: "gpt-4-turbo-preview"  # LLM for entity/relationship extraction
    confidence_threshold: 0.7  # Minimum confidence for extracted relationships
    api_key: null  # Set via OPENAI_API_KEY environment variable
    rate_limit_delay: 1.0  # Seconds between LLM requests
    batch_size: 10  # Number of papers to process in batch

    # Extraction features
    extract_from_papers: true
    extract_from_code: true
    extract_algorithms: true
    extract_metrics: true
    extract_architectures: true
    extract_techniques: true

  # Graph building settings
  building:
    auto_merge_duplicates: true  # Automatically merge duplicate concepts
    min_confidence: 0.5  # Minimum confidence to add to graph
    max_concepts_per_paper: 20  # Limit concepts extracted per paper
    enable_validation: true  # Validate extracted relationships

  # Hybrid retrieval configuration
  hybrid_retrieval:
    enabled: true
    expansion_depth: 2  # How many hops to traverse in graph
    vector_weight: 0.6  # Weight for vector similarity
    graph_weight: 0.4  # Weight for graph relevance
    max_expanded_concepts: 50  # Limit graph expansion

  # Graph query settings
  query:
    max_path_length: 5  # Maximum path length for path finding
    max_related_depth: 3  # Maximum depth for related concepts
    cache_queries: true  # Cache frequent queries
    enable_inference: false  # Enable graph reasoning/inference

  # Question answering settings
  qa:
    enabled: true
    confidence_threshold: 0.7  # Minimum confidence for QA answers
    max_context_concepts: 10  # Maximum concepts to include in answer
    use_llm_generation: false  # Use LLM to generate natural language answers

  # Integration settings
  integration:
    sync_with_rag: true  # Automatically sync concepts with RAG index
    pinecone_namespace: "knowledge_graph"  # Namespace for KG concepts
    update_frequency: "daily"  # How often to rebuild graph

  # Export and visualization
  export:
    auto_export: false  # Automatically export graph after building
    export_formats: ["json", "graphml"]  # Formats to export
    export_dir: "./cache/knowledge_graph/exports"

  # Performance settings
  performance:
    cache_size: 1000  # LRU cache size for concepts
    parallel_extraction: true  # Parallel paper processing
    max_workers: 4  # Number of parallel workers
    memory_limit_gb: 8  # Memory limit for graph operations

  # Monitoring and logging
  monitoring:
    track_extraction_quality: true
    log_low_confidence: true  # Log low-confidence extractions
    alert_on_errors: true

  # Concept types to extract
  concept_types:
    - "algorithm"
    - "technique"
    - "architecture"
    - "metric"
    - "framework"
    - "dataset"
    - "model"
    - "method"
    - "approach"

  # Relationship types to extract (see RelationType enum)
  relationship_types:
    - "is_a"
    - "uses"
    - "improves"
    - "extends"
    - "implemented_in"
    - "compared_to"
    - "requires"
    - "part_of"
    - "related_to"
    - "influences"
    - "precedes"
