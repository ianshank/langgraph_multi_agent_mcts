"""
Multi-Modal Knowledge Base for LangGraph Multi-Agent MCTS Training

Handles text, images, diagrams, and code from research papers and documentation.
Integrates with existing RAG infrastructure and Pinecone vector storage.

Features:
- PDF image extraction (diagrams, plots, architectures)
- Vision model integration (GPT-4V, Claude 3.5 Sonnet)
- CLIP for cross-modal embeddings
- Code block extraction and indexing
- Multi-modal RAG with text + images + code
- Integration with existing research corpus builder
"""

import base64
import hashlib
import io
import logging
import os
import re
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any

import numpy as np
import yaml
from PIL import Image

# Optional imports with fallbacks
try:
    import fitz  # PyMuPDF

    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

try:
    from pdf2image import convert_from_path

    HAS_PDF2IMAGE = True
except ImportError:
    HAS_PDF2IMAGE = False

try:
    from transformers import CLIPModel, CLIPProcessor

    HAS_CLIP = True
except ImportError:
    HAS_CLIP = False

try:
    from pinecone import Pinecone

    HAS_PINECONE = True
except ImportError:
    HAS_PINECONE = False

try:
    from sentence_transformers import SentenceTransformer

    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False

from training.data_pipeline import DocumentChunk

logger = logging.getLogger(__name__)


class ImageType(Enum):
    """Classification of image/diagram types."""

    ARCHITECTURE_DIAGRAM = "architecture"
    FLOWCHART = "flowchart"
    PLOT_CHART = "plot"
    TABLE = "table"
    TREE_VISUALIZATION = "tree"
    NEURAL_NETWORK = "neural_network"
    ALGORITHM = "algorithm"
    EQUATION = "equation"
    SCREENSHOT = "screenshot"
    OTHER = "other"


@dataclass
class ExtractedImage:
    """Represents an extracted image from a document."""

    image_id: str
    doc_id: str
    page_number: int
    image_data: bytes
    format: str  # png, jpg, etc.
    width: int
    height: int
    caption: str | None = None
    ocr_text: str | None = None
    description: str | None = None  # Generated by vision model
    image_type: ImageType = ImageType.OTHER
    metadata: dict[str, Any] = field(default_factory=dict)

    def to_pil_image(self) -> Image.Image:
        """Convert to PIL Image."""
        return Image.open(io.BytesIO(self.image_data))

    def to_base64(self) -> str:
        """Convert to base64 for API calls."""
        return base64.b64encode(self.image_data).decode("utf-8")

    def save(self, path: Path) -> None:
        """Save image to file."""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "wb") as f:
            f.write(self.image_data)


@dataclass
class ExtractedCode:
    """Represents extracted code from a document."""

    code_id: str
    doc_id: str
    code: str
    language: str | None = None
    section: str | None = None  # Section in paper where code appears
    context: str | None = None  # Surrounding text
    is_pseudocode: bool = False
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class MultiModalSearchResult:
    """Multi-modal search result containing text, images, and code."""

    result_id: str
    result_type: str  # "text", "image", "code"
    score: float
    content: str | ExtractedImage | ExtractedCode
    metadata: dict[str, Any] = field(default_factory=dict)


class ImageProcessor:
    """Extract and process images from PDFs and documents."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize image processor.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.min_size = tuple(config.get("min_size", [100, 100]))
        self.max_size = tuple(config.get("max_size", [4096, 4096]))
        self.formats = config.get("formats", ["png", "jpg", "jpeg"])
        self.images_dir = Path(config.get("images_dir", "./cache/images"))
        self.images_dir.mkdir(parents=True, exist_ok=True)

        # Check available backends
        self.backend = "pymupdf" if HAS_PYMUPDF else "pdf2image" if HAS_PDF2IMAGE else None
        if not self.backend:
            logger.warning("No PDF processing library available. Install PyMuPDF or pdf2image")

        logger.info(f"ImageProcessor initialized with backend: {self.backend}")

    def extract_images_from_pdf(self, pdf_path: str | Path) -> list[ExtractedImage]:
        """
        Extract all images from a PDF file.

        Args:
            pdf_path: Path to PDF file

        Returns:
            List of extracted images
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF not found: {pdf_path}")

        doc_id = pdf_path.stem

        if self.backend == "pymupdf":
            return self._extract_with_pymupdf(pdf_path, doc_id)
        elif self.backend == "pdf2image":
            return self._extract_with_pdf2image(pdf_path, doc_id)
        else:
            logger.error("No PDF backend available")
            return []

    def _extract_with_pymupdf(self, pdf_path: Path, doc_id: str) -> list[ExtractedImage]:
        """Extract images using PyMuPDF (faster, better quality)."""
        images = []

        try:
            doc = fitz.open(pdf_path)

            for page_num in range(len(doc)):
                page = doc[page_num]
                image_list = page.get_images(full=True)

                for img_index, img in enumerate(image_list):
                    xref = img[0]
                    base_image = doc.extract_image(xref)

                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]

                    # Convert to PIL for size checking
                    pil_img = Image.open(io.BytesIO(image_bytes))
                    width, height = pil_img.size

                    # Filter by size
                    if width < self.min_size[0] or height < self.min_size[1]:
                        continue

                    # Resize if too large
                    if width > self.max_size[0] or height > self.max_size[1]:
                        pil_img.thumbnail(self.max_size, Image.Resampling.LANCZOS)
                        # Convert back to bytes
                        img_buffer = io.BytesIO()
                        pil_img.save(img_buffer, format="PNG")
                        image_bytes = img_buffer.getvalue()
                        image_ext = "png"
                        width, height = pil_img.size

                    # Generate image ID
                    image_hash = hashlib.md5(image_bytes).hexdigest()[:8]
                    image_id = f"{doc_id}_p{page_num}_i{img_index}_{image_hash}"

                    # Try to extract caption from nearby text
                    caption = self._extract_caption(page, page_num)

                    extracted = ExtractedImage(
                        image_id=image_id,
                        doc_id=doc_id,
                        page_number=page_num,
                        image_data=image_bytes,
                        format=image_ext,
                        width=width,
                        height=height,
                        caption=caption,
                        metadata={
                            "extraction_method": "pymupdf",
                            "xref": xref,
                        },
                    )

                    images.append(extracted)

            doc.close()
            logger.info(f"Extracted {len(images)} images from {pdf_path.name} using PyMuPDF")

        except Exception as e:
            logger.error(f"Error extracting images with PyMuPDF: {e}")

        return images

    def _extract_with_pdf2image(self, pdf_path: Path, doc_id: str) -> list[ExtractedImage]:
        """Extract images by converting PDF pages to images."""
        images = []

        try:
            # Convert PDF to images (one per page)
            pil_images = convert_from_path(pdf_path, dpi=150)

            for page_num, pil_img in enumerate(pil_images):
                width, height = pil_img.size

                # Convert to bytes
                img_buffer = io.BytesIO()
                pil_img.save(img_buffer, format="PNG")
                image_bytes = img_buffer.getvalue()

                # Generate image ID
                image_hash = hashlib.md5(image_bytes).hexdigest()[:8]
                image_id = f"{doc_id}_page{page_num}_{image_hash}"

                extracted = ExtractedImage(
                    image_id=image_id,
                    doc_id=doc_id,
                    page_number=page_num,
                    image_data=image_bytes,
                    format="png",
                    width=width,
                    height=height,
                    metadata={
                        "extraction_method": "pdf2image",
                        "full_page": True,
                    },
                )

                images.append(extracted)

            logger.info(f"Extracted {len(images)} page images from {pdf_path.name}")

        except Exception as e:
            logger.error(f"Error extracting images with pdf2image: {e}")

        return images

    def _extract_caption(self, page, page_num: int) -> str | None:
        """Extract caption text near images."""
        try:
            text = page.get_text()
            # Look for "Figure X:" or "Fig. X:" patterns
            caption_patterns = [
                r"Figure\s+\d+[:\.].*?(?=\n\n|\n[A-Z]|$)",
                r"Fig\.\s+\d+[:\.].*?(?=\n\n|\n[A-Z]|$)",
            ]

            for pattern in caption_patterns:
                match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
                if match:
                    return match.group(0).strip()
        except Exception:
            pass

        return None

    def save_image(self, image: ExtractedImage, filename: str | None = None) -> Path:
        """
        Save extracted image to disk.

        Args:
            image: ExtractedImage instance
            filename: Optional custom filename

        Returns:
            Path to saved image
        """
        if filename is None:
            filename = f"{image.image_id}.{image.format}"

        save_path = self.images_dir / filename
        image.save(save_path)
        logger.debug(f"Saved image to {save_path}")

        return save_path


class VisionModelAdapter:
    """Adapter for vision models (GPT-4V, Claude 3.5 Sonnet) to generate image descriptions."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize vision model adapter.

        Args:
            config: Configuration with model settings
        """
        self.config = config
        self.model_name = config.get("model", "claude-3-5-sonnet-20241022")
        self.provider = self._detect_provider(self.model_name)
        self.max_tokens = int(config.get("max_tokens", 1024))
        self.temperature = float(config.get("temperature", 0.3))

        # Initialize LLM client
        self._client = None
        self._initialize_client()

        logger.info(f"VisionModelAdapter initialized with {self.provider}: {self.model_name}")

    def _detect_provider(self, model_name: str) -> str:
        """Detect provider from model name."""
        if "gpt-4" in model_name.lower() or "vision" in model_name.lower():
            return "openai"
        elif "claude" in model_name.lower():
            return "anthropic"
        else:
            return "unknown"

    def _initialize_client(self):
        """Initialize the appropriate LLM client."""
        try:
            if self.provider == "openai":
                from src.adapters.llm.openai_client import OpenAIClient

                self._client = OpenAIClient(
                    model=self.model_name,
                    timeout=120.0,
                )
            elif self.provider == "anthropic":
                from src.adapters.llm.anthropic_client import AnthropicClient

                self._client = AnthropicClient(
                    model=self.model_name,
                    timeout=120.0,
                )
            else:
                logger.warning(f"Unknown provider: {self.provider}")
        except Exception as e:
            logger.error(f"Failed to initialize vision model client: {e}")
            self._client = None

    async def generate_image_description(
        self,
        image: ExtractedImage,
        context: str | None = None,
    ) -> str:
        """
        Generate detailed description of an image using vision model.

        Args:
            image: ExtractedImage instance
            context: Optional context from paper (title, abstract, etc.)

        Returns:
            Generated description
        """
        if not self._client:
            logger.warning("Vision model client not initialized")
            return ""

        try:
            # Build prompt
            prompt = self._build_description_prompt(image, context)

            # Prepare image for API
            image_b64 = image.to_base64()

            # Call vision model
            if self.provider == "openai":
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/{image.format};base64,{image_b64}"},
                            },
                        ],
                    }
                ]
            elif self.provider == "anthropic":
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": f"image/{image.format}",
                                    "data": image_b64,
                                },
                            },
                            {"type": "text", "text": prompt},
                        ],
                    }
                ]
            else:
                return ""

            response = await self._client.generate(
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )

            description = response.text.strip()
            logger.debug(f"Generated description for {image.image_id}: {description[:100]}...")

            return description

        except Exception as e:
            logger.error(f"Error generating image description: {e}")
            return ""

    def _build_description_prompt(self, image: ExtractedImage, context: str | None) -> str:
        """Build prompt for vision model."""
        prompt_parts = [
            "Analyze this image from a research paper and provide a detailed technical description.",
            "",
            "Focus on:",
            "1. Type of diagram/visualization (architecture, flowchart, plot, algorithm, etc.)",
            "2. Key components and their relationships",
            "3. Technical details and labels visible",
            "4. Main insights or findings shown",
            "5. Relevant technical terminology",
        ]

        if image.caption:
            prompt_parts.extend(
                [
                    "",
                    f"Image caption: {image.caption}",
                ]
            )

        if context:
            prompt_parts.extend(
                [
                    "",
                    f"Paper context: {context[:500]}",
                ]
            )

        prompt_parts.extend(
            [
                "",
                "Provide a concise but comprehensive description suitable for retrieval and understanding.",
            ]
        )

        return "\n".join(prompt_parts)

    def classify_image_type(self, description: str, caption: str | None = None) -> ImageType:
        """
        Classify image type based on description and caption.

        Args:
            description: Generated description
            caption: Optional caption text

        Returns:
            ImageType classification
        """
        text = (description + " " + (caption or "")).lower()

        # Classification rules
        if any(term in text for term in ["architecture", "system design", "pipeline", "framework"]):
            return ImageType.ARCHITECTURE_DIAGRAM
        elif any(term in text for term in ["flowchart", "flow diagram", "process flow", "decision tree"]):
            return ImageType.FLOWCHART
        elif any(term in text for term in ["plot", "graph", "chart", "performance", "accuracy", "loss"]):
            return ImageType.PLOT_CHART
        elif any(term in text for term in ["table", "matrix", "comparison"]):
            return ImageType.TABLE
        elif any(term in text for term in ["tree", "mcts", "search tree", "game tree"]):
            return ImageType.TREE_VISUALIZATION
        elif any(term in text for term in ["neural network", "layers", "neurons", "cnn", "rnn", "transformer"]):
            return ImageType.NEURAL_NETWORK
        elif any(term in text for term in ["algorithm", "pseudocode", "procedure"]):
            return ImageType.ALGORITHM
        elif any(term in text for term in ["equation", "formula", "mathematical"]):
            return ImageType.EQUATION
        else:
            return ImageType.OTHER


class MultiModalEmbedder:
    """Embed images and text in shared semantic space using CLIP."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize multi-modal embedder.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.clip_model_name = config.get("clip_model", "openai/clip-vit-large-patch14")
        self.text_model_name = config.get("text_model", "sentence-transformers/all-MiniLM-L6-v2")
        self.use_clip = config.get("use_clip", True)

        # Initialize CLIP
        self.clip_model = None
        self.clip_processor = None
        if self.use_clip and HAS_CLIP:
            try:
                self.clip_model = CLIPModel.from_pretrained(self.clip_model_name)
                self.clip_processor = CLIPProcessor.from_pretrained(self.clip_model_name)
                self.clip_embedding_dim = self.clip_model.config.projection_dim
                logger.info(f"CLIP model loaded: {self.clip_model_name}")
            except Exception as e:
                logger.warning(f"Failed to load CLIP model: {e}")
                self.use_clip = False

        # Initialize text embedder
        self.text_embedder = None
        if HAS_SENTENCE_TRANSFORMERS:
            try:
                self.text_embedder = SentenceTransformer(self.text_model_name)
                self.text_embedding_dim = self.text_embedder.get_sentence_embedding_dimension()
                logger.info(f"Text embedder loaded: {self.text_model_name}")
            except Exception as e:
                logger.warning(f"Failed to load text embedder: {e}")

    def embed_image(self, image: ExtractedImage | Image.Image) -> np.ndarray:
        """
        Generate embedding for an image using CLIP.

        Args:
            image: ExtractedImage or PIL Image

        Returns:
            Image embedding vector
        """
        if not self.use_clip or not self.clip_model:
            logger.warning("CLIP not available, using random embeddings")
            return np.random.randn(512).astype(np.float32)

        try:
            # Convert to PIL if needed
            if isinstance(image, ExtractedImage):
                pil_image = image.to_pil_image()
            else:
                pil_image = image

            # Process image
            inputs = self.clip_processor(images=pil_image, return_tensors="pt")

            # Generate embedding
            with np.errstate(all="ignore"):
                image_features = self.clip_model.get_image_features(**inputs)
                embedding = image_features.detach().numpy()[0]

            # Normalize
            embedding = embedding / (np.linalg.norm(embedding) + 1e-8)

            return embedding

        except Exception as e:
            logger.error(f"Error generating image embedding: {e}")
            return np.random.randn(512).astype(np.float32)

    def embed_text(self, text: str, use_clip: bool = False) -> np.ndarray:
        """
        Generate embedding for text.

        Args:
            text: Text to embed
            use_clip: If True, use CLIP text encoder instead of sentence transformer

        Returns:
            Text embedding vector
        """
        if use_clip and self.clip_model:
            try:
                inputs = self.clip_processor(text=[text], return_tensors="pt", padding=True)
                text_features = self.clip_model.get_text_features(**inputs)
                embedding = text_features.detach().numpy()[0]
                embedding = embedding / (np.linalg.norm(embedding) + 1e-8)
                return embedding
            except Exception as e:
                logger.error(f"Error with CLIP text embedding: {e}")

        # Fall back to sentence transformer
        if self.text_embedder:
            try:
                embedding = self.text_embedder.encode(text, convert_to_numpy=True)
                return embedding
            except Exception as e:
                logger.error(f"Error generating text embedding: {e}")

        # Last resort: random
        dim = self.text_embedding_dim if self.text_embedder else 384
        return np.random.randn(dim).astype(np.float32)

    def cross_modal_similarity(self, image: ExtractedImage, text: str) -> float:
        """
        Calculate similarity between image and text.

        Args:
            image: ExtractedImage instance
            text: Text query

        Returns:
            Similarity score (0-1)
        """
        image_emb = self.embed_image(image)
        text_emb = self.embed_text(text, use_clip=True)

        # Cosine similarity
        similarity = np.dot(image_emb, text_emb) / (np.linalg.norm(image_emb) * np.linalg.norm(text_emb) + 1e-8)

        return float(max(0.0, min(1.0, (similarity + 1) / 2)))  # Normalize to 0-1


class CodeBlockExtractor:
    """Extract and process code blocks from text documents."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize code block extractor.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.extract_pseudocode = config.get("extract_pseudocode", True)
        self.min_code_lines = config.get("min_code_lines", 3)
        self.context_window = config.get("context_window", 200)  # characters

        # Language patterns
        self.language_keywords = {
            "python": ["def ", "import ", "class ", "if __name__"],
            "javascript": ["function ", "const ", "let ", "=>"],
            "java": ["public class", "private ", "void "],
            "cpp": ["#include", "std::", "namespace"],
            "pseudocode": ["Algorithm", "Input:", "Output:", "for each", "while "],
        }

        logger.info("CodeBlockExtractor initialized")

    def extract_code_blocks(self, text: str, doc_id: str) -> list[ExtractedCode]:
        """
        Extract code blocks from text.

        Args:
            text: Document text
            doc_id: Document identifier

        Returns:
            List of extracted code blocks
        """
        code_blocks = []

        # Extract markdown code blocks
        markdown_blocks = self._extract_markdown_code(text, doc_id)
        code_blocks.extend(markdown_blocks)

        # Extract LaTeX listings
        latex_blocks = self._extract_latex_code(text, doc_id)
        code_blocks.extend(latex_blocks)

        # Extract algorithm environments
        algorithm_blocks = self._extract_algorithms(text, doc_id)
        code_blocks.extend(algorithm_blocks)

        # Deduplicate by code content
        seen = set()
        unique_blocks = []
        for block in code_blocks:
            code_hash = hashlib.md5(block.code.encode()).hexdigest()
            if code_hash not in seen:
                seen.add(code_hash)
                unique_blocks.append(block)

        logger.info(f"Extracted {len(unique_blocks)} unique code blocks from {doc_id}")
        return unique_blocks

    def _extract_markdown_code(self, text: str, doc_id: str) -> list[ExtractedCode]:
        """Extract code from markdown code blocks."""
        blocks = []

        # Pattern: ```language\ncode\n```
        pattern = r"```(\w+)?\n(.*?)```"
        matches = re.finditer(pattern, text, re.DOTALL)

        for i, match in enumerate(matches):
            language = match.group(1) or "unknown"
            code = match.group(2).strip()

            # Filter by line count
            if code.count("\n") + 1 < self.min_code_lines:
                continue

            # Detect language if not specified
            if language == "unknown":
                language = self._detect_language(code)

            # Extract context
            context = self._extract_context(text, match.start())

            code_id = f"{doc_id}_code_md_{i}"

            blocks.append(
                ExtractedCode(
                    code_id=code_id,
                    doc_id=doc_id,
                    code=code,
                    language=language,
                    context=context,
                    is_pseudocode=(language.lower() == "pseudocode"),
                    metadata={
                        "extraction_method": "markdown",
                        "start_pos": match.start(),
                    },
                )
            )

        return blocks

    def _extract_latex_code(self, text: str, doc_id: str) -> list[ExtractedCode]:
        """Extract code from LaTeX listings environment."""
        blocks = []

        # Pattern: \begin{lstlisting}[language=X]...\end{lstlisting}
        pattern = r"\\begin\{lstlisting\}(?:\[language=(\w+)\])?(.*?)\\end\{lstlisting\}"
        matches = re.finditer(pattern, text, re.DOTALL | re.IGNORECASE)

        for i, match in enumerate(matches):
            language = match.group(1) or "unknown"
            code = match.group(2).strip()

            if code.count("\n") + 1 < self.min_code_lines:
                continue

            if language == "unknown":
                language = self._detect_language(code)

            context = self._extract_context(text, match.start())
            code_id = f"{doc_id}_code_latex_{i}"

            blocks.append(
                ExtractedCode(
                    code_id=code_id,
                    doc_id=doc_id,
                    code=code,
                    language=language,
                    context=context,
                    metadata={
                        "extraction_method": "latex",
                        "start_pos": match.start(),
                    },
                )
            )

        return blocks

    def _extract_algorithms(self, text: str, doc_id: str) -> list[ExtractedCode]:
        """Extract algorithms from algorithm environments."""
        blocks = []

        # Pattern: \begin{algorithm}...\end{algorithm}
        pattern = r"\\begin\{algorithm\}(.*?)\\end\{algorithm\}"
        matches = re.finditer(pattern, text, re.DOTALL | re.IGNORECASE)

        for i, match in enumerate(matches):
            code = match.group(1).strip()

            # Clean LaTeX commands
            code = self._clean_latex_algorithm(code)

            if code.count("\n") + 1 < self.min_code_lines:
                continue

            context = self._extract_context(text, match.start())
            code_id = f"{doc_id}_algo_{i}"

            blocks.append(
                ExtractedCode(
                    code_id=code_id,
                    doc_id=doc_id,
                    code=code,
                    language="pseudocode",
                    context=context,
                    is_pseudocode=True,
                    metadata={
                        "extraction_method": "algorithm_env",
                        "start_pos": match.start(),
                    },
                )
            )

        return blocks

    def _clean_latex_algorithm(self, code: str) -> str:
        """Clean LaTeX algorithm code."""
        # Remove common LaTeX commands
        code = re.sub(r"\\caption\{.*?\}", "", code)
        code = re.sub(r"\\label\{.*?\}", "", code)
        code = re.sub(r"\\begin\{algorithmic\}", "", code)
        code = re.sub(r"\\end\{algorithmic\}", "", code)
        code = re.sub(r"\\STATE\s+", "", code)
        code = re.sub(r"\\IF\{(.*?)\}", r"if \1:", code)
        code = re.sub(r"\\ENDIF", "", code)
        code = re.sub(r"\\FOR\{(.*?)\}", r"for \1:", code)
        code = re.sub(r"\\ENDFOR", "", code)
        code = re.sub(r"\\WHILE\{(.*?)\}", r"while \1:", code)
        code = re.sub(r"\\ENDWHILE", "", code)
        code = re.sub(r"\\RETURN\s+", "return ", code)

        return code.strip()

    def _detect_language(self, code: str) -> str:
        """Detect programming language from code content."""
        code_lower = code.lower()

        for lang, keywords in self.language_keywords.items():
            if any(kw in code_lower for kw in keywords):
                return lang

        return "unknown"

    def _extract_context(self, text: str, position: int) -> str:
        """Extract surrounding context for code block."""
        start = max(0, position - self.context_window)
        end = min(len(text), position + self.context_window)
        context = text[start:end].strip()

        # Get first paragraph before code
        paragraphs = context.split("\n\n")
        if paragraphs:
            return paragraphs[-1].strip()

        return context

    def link_code_to_paper(self, code: ExtractedCode, paper_metadata: dict) -> ExtractedCode:
        """
        Associate code with paper metadata.

        Args:
            code: ExtractedCode instance
            paper_metadata: Paper metadata dict

        Returns:
            Updated ExtractedCode with paper metadata
        """
        code.metadata.update(
            {
                "paper_title": paper_metadata.get("title"),
                "paper_authors": paper_metadata.get("authors", []),
                "paper_arxiv_id": paper_metadata.get("arxiv_id"),
                "paper_url": paper_metadata.get("pdf_url"),
            }
        )

        return code


class VisualIndexBuilder:
    """Build searchable index of images with Pinecone."""

    def __init__(self, config: dict[str, Any]):
        """
        Initialize visual index builder.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.embedder = MultiModalEmbedder(config.get("embeddings", {}))

        # Pinecone settings
        self.index_name = config.get("index_name", "multimodal-rag")
        self.namespace = config.get("namespace", "images")
        self.api_key = config.get("api_key") or os.environ.get("PINECONE_API_KEY")

        # Initialize Pinecone
        self.pc_client = None
        self.index = None
        if HAS_PINECONE and self.api_key:
            try:
                self.pc_client = Pinecone(api_key=self.api_key)
                self.index = self.pc_client.Index(self.index_name)
                logger.info(f"Visual index connected to Pinecone: {self.index_name}/{self.namespace}")
            except Exception as e:
                logger.error(f"Failed to connect to Pinecone: {e}")

        # Local cache
        self.image_metadata = {}

    def build_index(self, images: list[ExtractedImage]) -> dict[str, Any]:
        """
        Build searchable index of images.

        Args:
            images: List of ExtractedImage objects

        Returns:
            Statistics dictionary
        """
        logger.info(f"Building visual index for {len(images)} images")

        vectors = []
        for image in images:
            try:
                # Generate embedding
                embedding = self.embedder.embed_image(image)

                # Prepare metadata
                metadata = {
                    "image_id": image.image_id,
                    "doc_id": image.doc_id,
                    "page_number": image.page_number,
                    "caption": image.caption or "",
                    "description": image.description or "",
                    "image_type": image.image_type.value,
                    "width": image.width,
                    "height": image.height,
                    "format": image.format,
                }

                # Add to vectors
                vector_id = f"img_{image.image_id}"
                vectors.append(
                    {
                        "id": vector_id,
                        "values": embedding.tolist(),
                        "metadata": metadata,
                    }
                )

                # Cache metadata locally
                self.image_metadata[image.image_id] = {
                    "image": image,
                    "embedding": embedding,
                }

            except Exception as e:
                logger.error(f"Error indexing image {image.image_id}: {e}")

        # Upsert to Pinecone
        indexed_count = 0
        if self.index and vectors:
            try:
                batch_size = 100
                for i in range(0, len(vectors), batch_size):
                    batch = vectors[i : i + batch_size]
                    self.index.upsert(vectors=batch, namespace=self.namespace)
                    indexed_count += len(batch)

                logger.info(f"Indexed {indexed_count} images to Pinecone")
            except Exception as e:
                logger.error(f"Error upserting to Pinecone: {e}")

        stats = {
            "total_images": len(images),
            "indexed_count": indexed_count,
            "image_types": {},
        }

        # Count by type
        for img in images:
            img_type = img.image_type.value
            stats["image_types"][img_type] = stats["image_types"].get(img_type, 0) + 1

        return stats

    def search_by_text(
        self, query: str, k: int = 10, filter_type: ImageType | None = None
    ) -> list[MultiModalSearchResult]:
        """
        Search images using text query.

        Args:
            query: Text query
            k: Number of results
            filter_type: Optional image type filter

        Returns:
            List of search results
        """
        if not self.index:
            logger.warning("Pinecone index not available")
            return []

        try:
            # Embed query
            query_embedding = self.embedder.embed_text(query, use_clip=True)

            # Build filter
            filter_dict = None
            if filter_type:
                filter_dict = {"image_type": {"$eq": filter_type.value}}

            # Search
            results = self.index.query(
                vector=query_embedding.tolist(),
                top_k=k,
                namespace=self.namespace,
                include_metadata=True,
                filter=filter_dict,
            )

            # Convert to MultiModalSearchResult
            search_results = []
            for match in results.get("matches", []):
                metadata = match.get("metadata", {})
                image_id = metadata.get("image_id")

                # Get cached image if available
                cached = self.image_metadata.get(image_id)
                content = cached["image"] if cached else metadata

                search_results.append(
                    MultiModalSearchResult(
                        result_id=match["id"],
                        result_type="image",
                        score=float(match["score"]),
                        content=content,
                        metadata=metadata,
                    )
                )

            return search_results

        except Exception as e:
            logger.error(f"Error searching images: {e}")
            return []

    def search_by_image(self, image: ExtractedImage, k: int = 10) -> list[MultiModalSearchResult]:
        """
        Search for similar images.

        Args:
            image: Query image
            k: Number of results

        Returns:
            List of similar images
        """
        if not self.index:
            logger.warning("Pinecone index not available")
            return []

        try:
            # Embed query image
            query_embedding = self.embedder.embed_image(image)

            # Search
            results = self.index.query(
                vector=query_embedding.tolist(),
                top_k=k + 1,  # +1 to exclude self
                namespace=self.namespace,
                include_metadata=True,
            )

            # Convert results
            search_results = []
            for match in results.get("matches", []):
                # Skip self
                if match["id"] == f"img_{image.image_id}":
                    continue

                metadata = match.get("metadata", {})
                image_id = metadata.get("image_id")
                cached = self.image_metadata.get(image_id)
                content = cached["image"] if cached else metadata

                search_results.append(
                    MultiModalSearchResult(
                        result_id=match["id"],
                        result_type="image",
                        score=float(match["score"]),
                        content=content,
                        metadata=metadata,
                    )
                )

                if len(search_results) >= k:
                    break

            return search_results

        except Exception as e:
            logger.error(f"Error searching similar images: {e}")
            return []


class MultiModalRAG:
    """Multi-modal RAG system combining text, images, and code."""

    def __init__(self, config_path: str = "training/config.yaml"):
        """
        Initialize multi-modal RAG system.

        Args:
            config_path: Path to configuration file
        """
        with open(config_path) as f:
            full_config = yaml.safe_load(f)

        self.config = full_config.get("multimodal", {})

        # Initialize components
        self.image_processor = ImageProcessor(self.config.get("image_processor", {}))
        self.vision_adapter = VisionModelAdapter(self.config.get("vision_model", {}))
        self.embedder = MultiModalEmbedder(self.config.get("embeddings", {}))
        self.code_extractor = CodeBlockExtractor(self.config.get("code_extractor", {}))
        self.visual_index = VisualIndexBuilder(self.config.get("visual_index", {}))

        # Text RAG (from existing system)
        from training.rag_builder import VectorIndexBuilder

        rag_config = full_config.get("rag", {})
        rag_config["namespace"] = self.config.get("text_namespace", "multimodal_text")
        self.text_index = VectorIndexBuilder(rag_config)

        # Code index (separate namespace)
        code_config = rag_config.copy()
        code_config["namespace"] = self.config.get("code_namespace", "multimodal_code")
        self.code_index = VectorIndexBuilder(code_config)

        logger.info("MultiModalRAG initialized")

    async def process_document(
        self,
        pdf_path: str | Path,
        doc_metadata: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """
        Process a document (PDF) and extract all modalities.

        Args:
            pdf_path: Path to PDF file
            doc_metadata: Optional metadata about the document

        Returns:
            Processing statistics
        """
        pdf_path = Path(pdf_path)
        doc_id = pdf_path.stem

        logger.info(f"Processing document: {doc_id}")

        stats = {
            "doc_id": doc_id,
            "images_extracted": 0,
            "images_described": 0,
            "code_blocks_extracted": 0,
            "text_chunks_created": 0,
        }

        # Extract images
        images = self.image_processor.extract_images_from_pdf(pdf_path)
        stats["images_extracted"] = len(images)

        # Generate descriptions for images
        for image in images:
            context = None
            if doc_metadata:
                context = f"Title: {doc_metadata.get('title', '')}\nAbstract: {doc_metadata.get('abstract', '')[:500]}"

            description = await self.vision_adapter.generate_image_description(image, context)
            image.description = description

            # Classify image type
            image.image_type = self.vision_adapter.classify_image_type(description, image.caption)

            stats["images_described"] += 1

        # Build visual index
        if images:
            visual_stats = self.visual_index.build_index(images)
            stats["visual_index"] = visual_stats

        # Extract text (if available in metadata)
        if doc_metadata and "text" in doc_metadata:
            text = doc_metadata["text"]

            # Extract code blocks
            code_blocks = self.code_extractor.extract_code_blocks(text, doc_id)
            stats["code_blocks_extracted"] = len(code_blocks)

            # Index code blocks
            if code_blocks:
                code_chunks = [
                    DocumentChunk(
                        doc_id=code.code_id,
                        chunk_id=0,
                        text=f"Language: {code.language}\n\nCode:\n{code.code}\n\nContext: {code.context or ''}",
                        metadata={
                            "type": "code",
                            "language": code.language,
                            "is_pseudocode": code.is_pseudocode,
                            "doc_id": doc_id,
                        },
                    )
                    for code in code_blocks
                ]

                self.code_index.add_documents(iter(code_chunks))

        logger.info(f"Document processing complete: {stats}")
        return stats

    async def retrieve(
        self,
        query: str,
        k: int = 10,
        modalities: list[str] | None = None,
    ) -> dict[str, list[MultiModalSearchResult]]:
        """
        Retrieve relevant content across all modalities.

        Args:
            query: Search query
            k: Number of results per modality
            modalities: List of modalities to search ("text", "image", "code"), None for all

        Returns:
            Dictionary mapping modality to search results
        """
        if modalities is None:
            modalities = ["text", "image", "code"]

        results = {}

        # Search images
        if "image" in modalities:
            image_results = self.visual_index.search_by_text(query, k=k)
            results["image"] = image_results

        # Search text
        if "text" in modalities:
            text_results = self.text_index.search(query, k=k)
            results["text"] = [
                MultiModalSearchResult(
                    result_id=f"text_{r.doc_id}_{r.chunk_id}",
                    result_type="text",
                    score=r.score,
                    content=r.text,
                    metadata=r.metadata,
                )
                for r in text_results
            ]

        # Search code
        if "code" in modalities:
            code_results = self.code_index.search(query, k=k)
            results["code"] = [
                MultiModalSearchResult(
                    result_id=f"code_{r.doc_id}_{r.chunk_id}",
                    result_type="code",
                    score=r.score,
                    content=r.text,
                    metadata=r.metadata,
                )
                for r in code_results
            ]

        return results

    async def generate_response(
        self,
        query: str,
        context: dict[str, list[MultiModalSearchResult]],
        max_tokens: int = 1024,
    ) -> str:
        """
        Generate response using retrieved multi-modal context.

        Args:
            query: User query
            context: Retrieved multi-modal context
            max_tokens: Maximum tokens for response

        Returns:
            Generated response
        """
        # Build prompt with multi-modal context
        prompt_parts = [
            f"Query: {query}\n",
            "\n=== Retrieved Context ===\n",
        ]

        # Add text context
        if "text" in context and context["text"]:
            prompt_parts.append("\n## Text Sources:\n")
            for i, result in enumerate(context["text"][:3], 1):
                prompt_parts.append(f"{i}. {result.content[:300]}...\n")

        # Add image context
        if "image" in context and context["image"]:
            prompt_parts.append("\n## Relevant Diagrams/Figures:\n")
            for i, result in enumerate(context["image"][:3], 1):
                if isinstance(result.content, ExtractedImage):
                    img = result.content
                    prompt_parts.append(f"{i}. {img.image_type.value}: {img.description[:200]}...\n")
                else:
                    prompt_parts.append(f"{i}. {result.metadata.get('description', '')[:200]}...\n")

        # Add code context
        if "code" in context and context["code"]:
            prompt_parts.append("\n## Code Examples:\n")
            for i, result in enumerate(context["code"][:2], 1):
                prompt_parts.append(f"{i}. {result.content[:300]}...\n")

        prompt_parts.append("\n=== Task ===\n")
        prompt_parts.append(
            "Based on the retrieved context (text, diagrams, and code), provide a comprehensive answer.\n"
        )

        prompt = "".join(prompt_parts)

        # Generate response using vision adapter's client
        try:
            if self.vision_adapter._client:
                response = await self.vision_adapter._client.generate(
                    prompt=prompt,
                    max_tokens=max_tokens,
                    temperature=0.7,
                )
                return response.text
            else:
                return "Vision model not available for response generation."
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"Error generating response: {e}"


# Integration with existing corpus builders
def integrate_multimodal_with_research_corpus(
    research_builder,
    multimodal_rag: MultiModalRAG,
) -> dict[str, Any]:
    """
    Integrate multi-modal processing with research corpus builder.

    Args:
        research_builder: ResearchCorpusBuilder instance
        multimodal_rag: MultiModalRAG instance

    Returns:
        Integration statistics
    """
    logger.info("Integrating multi-modal processing with research corpus")

    stats = {
        "papers_processed": 0,
        "images_extracted": 0,
        "code_blocks_extracted": 0,
    }

    # This would be called after research corpus builder fetches papers
    # For each paper with PDF, extract images and code

    return stats


if __name__ == "__main__":
    # Example usage and testing
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    logger.info("Testing Multi-Modal Knowledge Base")

    # Test configuration
    test_config = {
        "image_processor": {
            "min_size": [100, 100],
            "formats": ["png", "jpg"],
            "images_dir": "./cache/test_images",
        },
        "vision_model": {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 1024,
        },
        "embeddings": {
            "clip_model": "openai/clip-vit-base-patch32",
            "text_model": "sentence-transformers/all-MiniLM-L6-v2",
            "use_clip": True,
        },
        "code_extractor": {
            "min_code_lines": 3,
            "context_window": 200,
        },
    }

    # Test image processor
    if HAS_PYMUPDF or HAS_PDF2IMAGE:
        processor = ImageProcessor(test_config["image_processor"])
        logger.info(f"Image processor ready with backend: {processor.backend}")
    else:
        logger.warning("No PDF processing backend available")

    # Test embedder
    if HAS_CLIP and HAS_SENTENCE_TRANSFORMERS:
        embedder = MultiModalEmbedder(test_config["embeddings"])
        test_text = "MCTS algorithm with neural network value function"
        text_emb = embedder.embed_text(test_text)
        logger.info(f"Text embedding shape: {text_emb.shape}")
    else:
        logger.warning("CLIP or sentence-transformers not available")

    # Test code extractor
    code_extractor = CodeBlockExtractor(test_config["code_extractor"])
    test_doc = """
    Here is an example implementation:

    ```python
    def mcts_search(root, num_simulations):
        for _ in range(num_simulations):
            node = select(root)
            reward = simulate(node)
            backpropagate(node, reward)
        return best_action(root)
    ```

    This algorithm performs Monte Carlo Tree Search.
    """

    extracted_code = code_extractor.extract_code_blocks(test_doc, "test_doc")
    logger.info(f"Extracted {len(extracted_code)} code blocks")
    if extracted_code:
        logger.info(
            f"First code block: language={extracted_code[0].language}, lines={extracted_code[0].code.count(chr(10)) + 1}"
        )

    logger.info("Multi-Modal Knowledge Base test complete!")
