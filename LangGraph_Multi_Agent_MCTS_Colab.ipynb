{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf LangGraph Multi-Agent MCTS Framework\n",
    "\n",
    "## Production Demo with Trained Neural Meta-Controllers\n",
    "\n",
    "This notebook demonstrates the **LangGraph Multi-Agent MCTS Framework** - a sophisticated multi-agent system that combines:\n",
    "\n",
    "- **LangGraph** for explicit state management and agent orchestration\n",
    "- **Monte Carlo Tree Search (MCTS)** for strategic planning and exploration\n",
    "- **Neural Meta-Controllers** (RNN and BERT with LoRA) for intelligent agent routing\n",
    "\n",
    "### \ud83e\udde0 Trained Models\n",
    "- **RNN Meta-Controller**: GRU-based sequential pattern recognition for fast routing\n",
    "- **BERT with LoRA**: Transformer-based text understanding with parameter-efficient fine-tuning\n",
    "\n",
    "### \ud83e\udd16 Agents\n",
    "- **HRM (Hierarchical Reasoning Model)**: Decomposes complex problems hierarchically\n",
    "- **TRM (Tiny Recursive Model)**: Iterative refinement for progressive improvement\n",
    "- **MCTS**: Monte Carlo Tree Search for optimization and strategic exploration\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: [github.com/ianshank/langgraph_multi_agent_mcts](https://github.com/ianshank/langgraph_multi_agent_mcts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Step 1: Environment Setup\n",
    "\n",
    "Clone the repository and install all dependencies. This cell handles:\n",
    "- Repository cloning\n",
    "- Dependency installation (including `nest_asyncio` for async support in Colab)\n",
    "- Path configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Repository path\n",
    "REPO_NAME = \"langgraph_multi_agent_mcts\"\n",
    "REPO_PATH = f\"/content/{REPO_NAME}\"\n",
    "REPO_URL = \"https://github.com/ianshank/langgraph_multi_agent_mcts.git\"\n",
    "\n",
    "# 1. Clone the repository (clean up if re-running)\n",
    "print(\"\ud83d\udce6 Cloning repository...\")\n",
    "!rm -rf {REPO_PATH}\n",
    "!git clone {REPO_URL} {REPO_PATH}\n",
    "\n",
    "# 2. Change to repo directory\n",
    "%cd {REPO_PATH}\n",
    "\n",
    "# 3. Install dependencies from requirements.txt\n",
    "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# 4. Install additional Colab-specific packages\n",
    "print(\"\\n\ud83d\udce6 Installing Colab-specific packages...\")\n",
    "!pip install -q nest_asyncio ipywidgets\n",
    "\n",
    "# 5. Apply nest_asyncio for async support in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 6. Add repo to Python path\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "print(\"\\n\u2705 Setup complete!\")\n",
    "print(f\"\ud83d\udcc1 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd11 Step 2: API Key Configuration (Optional)\n",
    "\n",
    "The neural meta-controllers use **pre-trained local models**, so API keys are **optional**.\n",
    "\n",
    "However, if you want to:\n",
    "- Use LLM-powered agents (HRM/TRM with actual generation)\n",
    "- Enable LangSmith tracing for debugging\n",
    "- Use Weights & Biases for experiment tracking\n",
    "\n",
    "You can configure your API keys below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def set_key(name, required=False):\n",
    "    \"\"\"Set API key from Colab Secrets or manual input.\"\"\"\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        value = userdata.get(name)\n",
    "        if value:\n",
    "            print(f\"\u2705 {name} loaded from Colab Secrets\")\n",
    "            return value\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if required:\n",
    "        return getpass.getpass(f\"Enter your {name}: \")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f {name} not set (optional)\")\n",
    "        return None\n",
    "\n",
    "# Optional API Keys\n",
    "print(\"\ud83d\udd11 Configuring API Keys...\\n\")\n",
    "\n",
    "# OpenAI - for LLM-powered agents (optional)\n",
    "openai_key = set_key(\"OPENAI_API_KEY\")\n",
    "if openai_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "# LangSmith - for tracing (optional but recommended)\n",
    "langchain_key = set_key(\"LANGCHAIN_API_KEY\")\n",
    "if langchain_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-multi-agent-mcts\"\n",
    "\n",
    "# Weights & Biases - for experiment tracking (optional)\n",
    "wandb_key = set_key(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "\n",
    "print(\"\\n\u2705 API key configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Step 3: Load Trained Neural Meta-Controllers\n",
    "\n",
    "Initialize the framework with the pre-trained models:\n",
    "- **RNN Meta-Controller**: Fast, captures sequential patterns (10D features \u2192 3-class routing)\n",
    "- **BERT with LoRA**: Context-aware text understanding for complex routing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Import meta-controllers\n",
    "from src.agents.meta_controller.rnn_controller import RNNMetaController\n",
    "from src.agents.meta_controller.bert_controller_v2 import BERTMetaController\n",
    "from src.agents.meta_controller.base import MetaControllerFeatures\n",
    "\n",
    "print(\"\ud83e\udde0 Initializing Neural Meta-Controllers...\\n\")\n",
    "\n",
    "# Detect device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\ud83d\udcbb Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Initialize RNN Controller\n",
    "print(\"\\n\ud83d\udd04 Loading RNN Meta-Controller...\")\n",
    "rnn_controller = RNNMetaController(name=\"RNNController\", seed=42, device=device)\n",
    "\n",
    "# Load trained weights\n",
    "rnn_model_path = Path(REPO_PATH) / \"models\" / \"rnn_meta_controller.pt\"\n",
    "if rnn_model_path.exists():\n",
    "    checkpoint = torch.load(rnn_model_path, map_location=device, weights_only=True)\n",
    "    rnn_controller.model.load_state_dict(checkpoint)\n",
    "    rnn_controller.model.eval()\n",
    "    print(f\"   \u2705 Loaded trained weights from {rnn_model_path.name}\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f Using untrained model (weights not found)\")\n",
    "\n",
    "# Initialize BERT Controller with LoRA\n",
    "print(\"\\n\ud83e\udd16 Loading BERT Meta-Controller with LoRA...\")\n",
    "bert_controller = BERTMetaController(name=\"BERTController\", seed=42, device=device, use_lora=True)\n",
    "\n",
    "# Load trained weights\n",
    "bert_model_path = Path(REPO_PATH) / \"models\" / \"bert_lora\" / \"final_model\"\n",
    "if bert_model_path.exists():\n",
    "    try:\n",
    "        bert_controller.load_model(str(bert_model_path))\n",
    "        print(f\"   \u2705 Loaded trained LoRA weights from {bert_model_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0\ufe0f Could not load BERT weights: {e}\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f Using untrained model (weights not found)\")\n",
    "\n",
    "print(\"\\n\u2705 Meta-controllers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfae Step 4: Interactive Agent Routing Demo\n",
    "\n",
    "Try the neural meta-controllers! Enter a query and see how the controllers decide which agent to route it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature extraction\n",
    "try:\n",
    "    from src.agents.meta_controller.feature_extractor import FeatureExtractor, FeatureExtractorConfig\n",
    "    config = FeatureExtractorConfig.from_env()\n",
    "    config.device = device\n",
    "    feature_extractor = FeatureExtractor(config)\n",
    "    print(\"\u2705 Using semantic feature extraction\")\n",
    "except Exception as e:\n",
    "    feature_extractor = None\n",
    "    print(f\"\u26a0\ufe0f Using heuristic feature extraction: {e}\")\n",
    "\n",
    "def extract_features(query: str, iteration: int = 0, last_agent: str = \"none\"):\n",
    "    \"\"\"Extract features from a query for the meta-controller.\"\"\"\n",
    "    if feature_extractor:\n",
    "        return feature_extractor.extract_features(query, iteration, last_agent)\n",
    "    \n",
    "    # Fallback heuristic extraction\n",
    "    query_length = len(query)\n",
    "    has_technical = any(word in query.lower() for word in [\"algorithm\", \"code\", \"implement\", \"technical\", \"system\"])\n",
    "    has_comparison = any(word in query.lower() for word in [\"vs\", \"versus\", \"compare\", \"difference\", \"better\"])\n",
    "    has_optimization = any(word in query.lower() for word in [\"optimize\", \"best\", \"improve\", \"maximize\", \"minimize\"])\n",
    "    \n",
    "    hrm_conf = 0.5 + (0.2 if has_technical else 0)\n",
    "    trm_conf = 0.5 + (0.2 if has_comparison else 0)\n",
    "    mcts_conf = 0.5 + (0.2 if has_optimization else 0)\n",
    "    \n",
    "    total = hrm_conf + trm_conf + mcts_conf\n",
    "    return MetaControllerFeatures(\n",
    "        hrm_confidence=hrm_conf/total,\n",
    "        trm_confidence=trm_conf/total,\n",
    "        mcts_value=mcts_conf/total,\n",
    "        consensus_score=0.6,\n",
    "        last_agent=last_agent,\n",
    "        iteration=iteration,\n",
    "        query_length=query_length,\n",
    "        has_rag_context=query_length > 50,\n",
    "        rag_relevance_score=0.7 if query_length > 50 else 0.0,\n",
    "        is_technical_query=has_technical,\n",
    "    )\n",
    "\n",
    "def route_query(query: str, controller_type: str = \"rnn\"):\n",
    "    \"\"\"Route a query using the specified meta-controller.\"\"\"\n",
    "    features = extract_features(query)\n",
    "    \n",
    "    if controller_type.lower() == \"rnn\":\n",
    "        prediction = rnn_controller.predict(features)\n",
    "    else:\n",
    "        prediction = bert_controller.predict(features)\n",
    "    \n",
    "    return prediction, features\n",
    "\n",
    "print(\"\u2705 Routing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83c\udfae Try it yourself!\n",
    "\n",
    "# Example queries - try changing these!\n",
    "example_queries = [\n",
    "    \"What are the key factors when choosing between microservices and monolithic architecture?\",\n",
    "    \"How can we optimize a Python application that processes 10GB of log files daily?\",\n",
    "    \"Compare B-trees vs LSM-trees for write-heavy workloads\",\n",
    "    \"Design a distributed rate limiting system for 100k requests per second\",\n",
    "    \"Explain the difference between supervised and unsupervised learning\",\n",
    "]\n",
    "\n",
    "print(\"\ud83e\udde0 Neural Meta-Controller Routing Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(example_queries, 1):\n",
    "    print(f\"\\n\ud83d\udcdd Query {i}: {query[:60]}...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get predictions from both controllers\n",
    "    rnn_pred, features = route_query(query, \"rnn\")\n",
    "    bert_pred, _ = route_query(query, \"bert\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udd04 RNN Controller:\")\n",
    "    print(f\"     Selected Agent: {rnn_pred.agent.upper()}\")\n",
    "    print(f\"     Confidence: {rnn_pred.confidence:.1%}\")\n",
    "    print(f\"     Probabilities: HRM={rnn_pred.probabilities['hrm']:.1%}, TRM={rnn_pred.probabilities['trm']:.1%}, MCTS={rnn_pred.probabilities['mcts']:.1%}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83e\udd16 BERT Controller:\")\n",
    "    print(f\"     Selected Agent: {bert_pred.agent.upper()}\")\n",
    "    print(f\"     Confidence: {bert_pred.confidence:.1%}\")\n",
    "    print(f\"     Probabilities: HRM={bert_pred.probabilities['hrm']:.1%}, TRM={bert_pred.probabilities['trm']:.1%}, MCTS={bert_pred.probabilities['mcts']:.1%}\")\n",
    "    \n",
    "    # Agreement check\n",
    "    if rnn_pred.agent == bert_pred.agent:\n",
    "        print(f\"\\n  \u2705 Controllers AGREE: {rnn_pred.agent.upper()}\")\n",
    "    else:\n",
    "        print(f\"\\n  \u26a0\ufe0f Controllers DISAGREE: RNN={rnn_pred.agent.upper()}, BERT={bert_pred.agent.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfb2 Step 5: Monte Carlo Tree Search (MCTS) Demo\n",
    "\n",
    "Explore the MCTS engine - the strategic planning component that simulates multiple decision paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from src.framework.mcts.core import MCTSNode, MCTSState, MCTSEngine\n",
    "from src.framework.mcts.config import MCTSConfig, create_preset_config\n",
    "\n",
    "print(\"\ud83c\udfb2 Monte Carlo Tree Search Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create MCTS configuration\n",
    "config = create_preset_config(\"BALANCED\")\n",
    "print(f\"\\n\ud83d\udcca MCTS Configuration (BALANCED preset):\")\n",
    "print(f\"   Iterations: {config.iterations}\")\n",
    "print(f\"   Exploration Weight: {config.exploration_weight}\")\n",
    "print(f\"   Max Depth: {config.max_depth}\")\n",
    "\n",
    "# Define simple action generator for demo\n",
    "def generate_actions(state: MCTSState) -> list[str]:\n",
    "    \"\"\"Generate possible actions from current state.\"\"\"\n",
    "    depth = len(state.state_id.split(\"_\"))\n",
    "    if depth > 3:\n",
    "        return []  # Terminal\n",
    "    return [f\"action_{chr(65+i)}\" for i in range(3)]  # A, B, C\n",
    "\n",
    "def evaluate_state(state: MCTSState) -> float:\n",
    "    \"\"\"Evaluate state value (simplified for demo).\"\"\"\n",
    "    # Simulate some value based on path taken\n",
    "    return random.uniform(0.3, 0.9)\n",
    "\n",
    "def transition(state: MCTSState, action: str) -> MCTSState:\n",
    "    \"\"\"Transition to new state.\"\"\"\n",
    "    return MCTSState(f\"{state.state_id}_{action}\")\n",
    "\n",
    "# Run MCTS\n",
    "print(f\"\\n\ud83c\udfaf Running MCTS simulation...\")\n",
    "root = MCTSNode(state=MCTSState(\"root\"))\n",
    "\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    # Selection - traverse to leaf using UCB1\n",
    "    node = root\n",
    "    while node.children and not node.terminal:\n",
    "        node = node.select_child(config.exploration_weight)\n",
    "    \n",
    "    # Expansion - add children if not terminal\n",
    "    if not node.terminal and node.visits > 0:\n",
    "        actions = generate_actions(node.state)\n",
    "        if actions:\n",
    "            action = random.choice(actions)\n",
    "            child_state = transition(node.state, action)\n",
    "            node = node.add_child(action=action, child_state=child_state)\n",
    "        else:\n",
    "            node.terminal = True\n",
    "    \n",
    "    # Simulation - evaluate\n",
    "    value = evaluate_state(node.state)\n",
    "    \n",
    "    # Backpropagation\n",
    "    while node:\n",
    "        node.visits += 1\n",
    "        node.value_sum += value\n",
    "        node = node.parent\n",
    "\n",
    "# Results\n",
    "print(f\"\\n\ud83d\udcca MCTS Results after {iterations} iterations:\")\n",
    "print(f\"   Root visits: {root.visits}\")\n",
    "print(f\"   Root value: {root.value:.3f}\")\n",
    "print(f\"   Children: {len(root.children)}\")\n",
    "\n",
    "if root.children:\n",
    "    print(f\"\\n\ud83c\udfc6 Best Actions (by visits):\")\n",
    "    sorted_children = sorted(root.children, key=lambda c: c.visits, reverse=True)\n",
    "    for i, child in enumerate(sorted_children[:3], 1):\n",
    "        print(f\"   {i}. {child.action}: visits={child.visits}, value={child.value:.3f}\")\n",
    "    \n",
    "    best = sorted_children[0]\n",
    "    print(f\"\\n   \u2705 Recommended: {best.action} (most robust selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 6: Visualize Routing Probabilities\n",
    "\n",
    "Create a visual comparison of how different queries are routed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Test queries for visualization\n",
    "test_queries = [\n",
    "    \"Implement a binary search tree\",\n",
    "    \"Compare PostgreSQL vs MongoDB\",\n",
    "    \"Optimize the neural network training\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Design a caching strategy for APIs\",\n",
    "]\n",
    "\n",
    "# Collect predictions\n",
    "rnn_probs = []\n",
    "bert_probs = []\n",
    "\n",
    "for query in test_queries:\n",
    "    rnn_pred, _ = route_query(query, \"rnn\")\n",
    "    bert_pred, _ = route_query(query, \"bert\")\n",
    "    \n",
    "    rnn_probs.append([rnn_pred.probabilities['hrm'], \n",
    "                      rnn_pred.probabilities['trm'], \n",
    "                      rnn_pred.probabilities['mcts']])\n",
    "    bert_probs.append([bert_pred.probabilities['hrm'], \n",
    "                       bert_pred.probabilities['trm'], \n",
    "                       bert_pred.probabilities['mcts']])\n",
    "\n",
    "rnn_probs = np.array(rnn_probs)\n",
    "bert_probs = np.array(bert_probs)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "agents = ['HRM', 'TRM', 'MCTS']\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "x = np.arange(len(test_queries))\n",
    "width = 0.25\n",
    "\n",
    "# RNN Controller\n",
    "ax1 = axes[0]\n",
    "for i, (agent, color) in enumerate(zip(agents, colors)):\n",
    "    ax1.bar(x + i*width, rnn_probs[:, i], width, label=agent, color=color, alpha=0.8)\n",
    "ax1.set_xlabel('Query')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('\ud83d\udd04 RNN Meta-Controller Routing')\n",
    "ax1.set_xticks(x + width)\n",
    "ax1.set_xticklabels([f'Q{i+1}' for i in range(len(test_queries))])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# BERT Controller\n",
    "ax2 = axes[1]\n",
    "for i, (agent, color) in enumerate(zip(agents, colors)):\n",
    "    ax2.bar(x + i*width, bert_probs[:, i], width, label=agent, color=color, alpha=0.8)\n",
    "ax2.set_xlabel('Query')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('\ud83e\udd16 BERT Meta-Controller Routing')\n",
    "ax2.set_xticks(x + width)\n",
    "ax2.set_xticklabels([f'Q{i+1}' for i in range(len(test_queries))])\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Neural Meta-Controller Agent Routing Comparison', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Print query legend\n",
    "print(\"\\n\ud83d\udcdd Query Legend:\")\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"   Q{i+1}: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfdb\ufe0f Step 7: Full Framework Demo with Gradio UI\n",
    "\n",
    "Launch the complete Gradio interface for interactive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Run the full Gradio app\n",
    "# This launches the complete UI with all features\n",
    "\n",
    "print(\"\ud83c\udfdb\ufe0f Launching Full Framework UI...\")\n",
    "print(\"\\nNote: This will run the complete Gradio interface.\")\n",
    "print(\"Click the public URL to access from any device.\\n\")\n",
    "\n",
    "# Import and run the app\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"app\", f\"{REPO_PATH}/app.py\")\n",
    "app_module = importlib.util.module_from_spec(spec)\n",
    "\n",
    "# This will initialize the framework and launch Gradio\n",
    "# Comment out the next two lines if you just want to explore the code\n",
    "# spec.loader.exec_module(app_module)\n",
    "# app_module.demo.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\u2139\ufe0f To launch the full UI, uncomment the last two lines above and run this cell.\")\n",
    "print(\"   Or run: !python app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Quick Mini Gradio Demo\n",
    "# A lightweight version for quick testing\n",
    "\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "\n",
    "def process_query_mini(query: str, controller_type: str):\n",
    "    \"\"\"Process a query with the selected controller.\"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Please enter a query.\", \"\", \"\"\n",
    "    \n",
    "    prediction, features = route_query(query, controller_type.lower())\n",
    "    \n",
    "    # Format routing decision\n",
    "    routing = f\"\"\"\ud83e\udde0 **Meta-Controller Decision**\n",
    "\n",
    "**Selected Agent:** `{prediction.agent.upper()}`\n",
    "**Confidence:** {prediction.confidence:.1%}\n",
    "\n",
    "**Routing Probabilities:**\n",
    "- HRM: {prediction.probabilities['hrm']:.1%} {'█' * int(prediction.probabilities['hrm'] * 20)}\n",
    "- TRM: {prediction.probabilities['trm']:.1%} {'█' * int(prediction.probabilities['trm'] * 20)}\n",
    "- MCTS: {prediction.probabilities['mcts']:.1%} {'█' * int(prediction.probabilities['mcts'] * 20)}\n",
    "\"\"\"\n",
    "    \n",
    "    # Format features\n",
    "    features_str = f\"\"\"\ud83d\udcca **Extracted Features**\n",
    "\n",
    "- Query Length: {features.query_length}\n",
    "- Technical Query: {'Yes' if features.is_technical_query else 'No'}\n",
    "- Has RAG Context: {'Yes' if features.has_rag_context else 'No'}\n",
    "- HRM Confidence: {features.hrm_confidence:.3f}\n",
    "- TRM Confidence: {features.trm_confidence:.3f}\n",
    "- MCTS Value: {features.mcts_value:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    # Simulated agent response\n",
    "    agent_responses = {\n",
    "        \"hrm\": f\"[HRM] Breaking down hierarchically: {query[:80]}...\",\n",
    "        \"trm\": f\"[TRM] Applying iterative refinement: {query[:80]}...\",\n",
    "        \"mcts\": f\"[MCTS] Strategic exploration via tree search: {query[:80]}...\",\n",
    "    }\n",
    "    response = agent_responses.get(prediction.agent, \"Unknown agent\")\n",
    "    \n",
    "    return response, routing, features_str\n",
    "\n",
    "# Create mini Gradio interface\n",
    "with gr.Blocks(title=\"LangGraph Multi-Agent MCTS - Mini Demo\") as mini_demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # \ud83c\udfaf LangGraph Multi-Agent MCTS - Quick Demo\n",
    "    \n",
    "    Test the neural meta-controllers with your own queries!\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Enter your question...\",\n",
    "                lines=3\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            controller_select = gr.Radio(\n",
    "                choices=[\"RNN\", \"BERT\"],\n",
    "                value=\"RNN\",\n",
    "                label=\"Controller Type\"\n",
    "            )\n",
    "    \n",
    "    submit_btn = gr.Button(\"\ud83d\ude80 Process Query\", variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        response_output = gr.Textbox(label=\"Agent Response\", lines=3)\n",
    "    \n",
    "    with gr.Row():\n",
    "        routing_output = gr.Markdown(label=\"Routing Decision\")\n",
    "        features_output = gr.Markdown(label=\"Features\")\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=process_query_mini,\n",
    "        inputs=[query_input, controller_select],\n",
    "        outputs=[response_output, routing_output, features_output]\n",
    "    )\n",
    "\n",
    "# Launch with share=True for public URL\n",
    "mini_demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfad Step 8: Training Your Own Models (Optional)\n",
    "\n",
    "Learn how to train the neural meta-controllers on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "from src.training.data_generator import MetaControllerDataGenerator\n",
    "from src.training.train_rnn import RNNTrainer\n",
    "\n",
    "print(\"\ud83c\udfad Training Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate balanced dataset\n",
    "print(\"\\n\ud83d\udce6 Generating synthetic training data...\")\n",
    "generator = MetaControllerDataGenerator(seed=42)\n",
    "features_list, labels_list = generator.generate_balanced_dataset(samples_per_class=50)\n",
    "\n",
    "print(f\"   Total samples: {len(features_list)}\")\n",
    "print(f\"   Classes: {set(labels_list)}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X, y = generator.to_tensor_dataset(features_list, labels_list)\n",
    "print(f\"   Feature tensor shape: {X.shape}\")\n",
    "print(f\"   Label tensor shape: {y.shape}\")\n",
    "\n",
    "# Split dataset\n",
    "splits = generator.split_dataset(X, y, train_ratio=0.7, val_ratio=0.15)\n",
    "print(f\"\\n\ud83d\udcca Dataset splits:\")\n",
    "print(f\"   Training: {splits['X_train'].shape[0]} samples\")\n",
    "print(f\"   Validation: {splits['X_val'].shape[0]} samples\")\n",
    "print(f\"   Test: {splits['X_test'].shape[0]} samples\")\n",
    "\n",
    "# Quick training demo (3 epochs)\n",
    "print(\"\\n\ud83c\udfcb\ufe0f Training RNN model (3 epochs)...\")\n",
    "trainer = RNNTrainer(\n",
    "    hidden_dim=32,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    early_stopping_patience=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "history = trainer.train(\n",
    "    train_data=(splits[\"X_train\"], splits[\"y_train\"]),\n",
    "    val_data=(splits[\"X_val\"], splits[\"y_val\"]),\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Training complete!\")\n",
    "print(f\"   Best validation accuracy: {history['best_val_accuracy']:.2%}\")\n",
    "print(f\"   Best validation loss: {history['best_val_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\ud83e\uddea Evaluating on test set...\")\n",
    "test_loader = trainer.create_dataloader(splits[\"X_test\"], splits[\"y_test\"], shuffle=False)\n",
    "results = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"   Test accuracy: {results['accuracy']:.2%}\")\n",
    "print(f\"   Test loss: {results['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfc1 Step 9: Chess MCTS Demo (Bonus)\n",
    "\n",
    "The framework includes a chess implementation using MCTS - similar to AlphaZero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import chess\n",
    "    from src.games.chess.game import ChessGame\n",
    "    from src.games.chess.mcts import ChessMCTS\n",
    "    \n",
    "    print(\"\u265a Chess MCTS Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create a chess game\n",
    "    game = ChessGame()\n",
    "    print(f\"\\n\ud83c\udfaf Initial position:\")\n",
    "    print(game.board)\n",
    "    \n",
    "    # Create MCTS player\n",
    "    mcts = ChessMCTS(game, iterations=50, exploration_weight=1.414)\n",
    "    \n",
    "    # Get best move\n",
    "    print(f\"\\n\ud83e\udd14 MCTS thinking (50 iterations)...\")\n",
    "    best_move = mcts.get_best_move()\n",
    "    \n",
    "    print(f\"\\n\u2705 Best move: {best_move}\")\n",
    "    \n",
    "    # Show move statistics\n",
    "    print(f\"\\n\ud83d\udcca Move statistics:\")\n",
    "    for move, stats in mcts.get_move_stats()[:5]:\n",
    "        print(f\"   {move}: visits={stats['visits']}, value={stats['value']:.3f}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f Chess demo not available: {e}\")\n",
    "    print(\"   Install with: pip install python-chess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Resources & Next Steps\n",
    "\n",
    "### \ud83d\udcd6 Documentation\n",
    "- [Repository README](https://github.com/ianshank/langgraph_multi_agent_mcts)\n",
    "- [Architecture Documentation](https://github.com/ianshank/langgraph_multi_agent_mcts/blob/main/docs/langgraph_mcts_architecture.md)\n",
    "\n",
    "### \ud83d\udee0\ufe0f Key Files\n",
    "- `app.py` - Main Gradio application\n",
    "- `src/agents/meta_controller/` - Neural meta-controllers\n",
    "- `src/framework/mcts/` - MCTS implementation\n",
    "- `src/training/` - Training pipelines\n",
    "\n",
    "### \ud83c\udfc3 Run Locally\n",
    "```bash\n",
    "git clone https://github.com/ianshank/langgraph_multi_agent_mcts.git\n",
    "cd langgraph_multi_agent_mcts\n",
    "pip install -r requirements.txt\n",
    "python app.py\n",
    "```\n",
    "\n",
    "### \ud83d\udc4b Feedback\n",
    "Open an issue on GitHub or contribute to the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udf89 Notebook complete!\")\n",
    "print(\"\\nYou've explored:\")\n",
    "print(\"  \u2705 Neural Meta-Controllers (RNN and BERT with LoRA)\")\n",
    "print(\"  \u2705 Monte Carlo Tree Search (MCTS)\")\n",
    "print(\"  \u2705 Agent Routing Visualization\")\n",
    "print(\"  \u2705 Training Pipeline\")\n",
    "print(\"  \u2705 Interactive Gradio Demo\")\n",
    "print(\"\\n\ud83d\ude80 Happy experimenting!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
