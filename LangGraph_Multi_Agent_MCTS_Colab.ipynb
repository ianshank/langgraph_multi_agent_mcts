{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf LangGraph Multi-Agent MCTS Framework\n",
    "\n",
    "## Production Demo with Trained Neural Meta-Controllers\n",
    "\n",
    "This notebook demonstrates the **LangGraph Multi-Agent MCTS Framework** - a sophisticated multi-agent system that combines:\n",
    "\n",
    "- **LangGraph** for explicit state management and agent orchestration\n",
    "- **Monte Carlo Tree Search (MCTS)** for strategic planning and exploration\n",
    "- **Neural Meta-Controllers** (RNN and BERT with LoRA) for intelligent agent routing\n",
    "\n",
    "### \ud83e\udde0 Trained Models\n",
    "- **RNN Meta-Controller**: GRU-based sequential pattern recognition for fast routing\n",
    "- **BERT with LoRA**: Transformer-based text understanding with parameter-efficient fine-tuning\n",
    "\n",
    "### \ud83e\udd16 Agents\n",
    "- **HRM (Hierarchical Reasoning Model)**: Decomposes complex problems hierarchically\n",
    "- **TRM (Tiny Recursive Model)**: Iterative refinement for progressive improvement\n",
    "- **MCTS**: Monte Carlo Tree Search for optimization and strategic exploration\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: [github.com/ianshank/langgraph_multi_agent_mcts](https://github.com/ianshank/langgraph_multi_agent_mcts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Step 1: Environment Setup\n",
    "\n",
    "Clone the repository and install all dependencies. This cell handles:\n",
    "- Repository cloning (using Python's `shutil` for safe directory removal)\n",
    "- Dependency installation (including `nest_asyncio` for async support in Colab)\n",
    "- Path configuration\n",
    "\n",
    "### Why `nest_asyncio`?\n",
    "Google Colab already runs an asyncio event loop in the background. When LangGraph agents\n",
    "try to create their own event loops, this causes conflicts. `nest_asyncio` patches the\n",
    "asyncio module to allow nested event loops, enabling async agent operations within Colab's\n",
    "existing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Environment setup cell - clone repository and install dependencies.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# =============================================================================\n",
    "COLAB_CONTENT_DIR: Final[str] = \"/content\"\n",
    "REPO_NAME: Final[str] = \"langgraph_multi_agent_mcts\"\n",
    "REPO_PATH: Final[str] = f\"{COLAB_CONTENT_DIR}/{REPO_NAME}\"\n",
    "REPO_URL: Final[str] = \"https://github.com/ianshank/langgraph_multi_agent_mcts.git\"\n",
    "\n",
    "# =============================================================================\n",
    "# SETUP FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def safe_remove_directory(path: Path) -> None:\n",
    "    \"\"\"Safely remove a directory with validation.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the directory to remove.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If path is outside allowed locations.\n",
    "    \"\"\"\n",
    "    path = path.resolve()\n",
    "    allowed_prefixes = [Path(COLAB_CONTENT_DIR).resolve(), Path(\"/tmp\").resolve()]\n",
    "    \n",
    "    if not any(str(path).startswith(str(prefix)) for prefix in allowed_prefixes):\n",
    "        raise ValueError(f\"Cannot delete directory outside allowed paths: {path}\")\n",
    "    \n",
    "    if path.exists() and path.is_dir():\n",
    "        print(f\"   Removing existing directory: {path}\")\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\ud83d\udce6 Setting up repository...\")\n",
    "repo_dir = Path(REPO_PATH)\n",
    "safe_remove_directory(repo_dir)\n",
    "\n",
    "# Clone repository\n",
    "print(\"\ud83d\udce6 Cloning repository...\")\n",
    "!git clone {REPO_URL} {REPO_PATH}\n",
    "\n",
    "# Change to repo directory\n",
    "%cd {REPO_PATH}\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install Colab-specific packages\n",
    "# - nest_asyncio: Enables nested event loops (required for LangGraph in Colab)\n",
    "# - ipywidgets: Interactive widgets for Jupyter\n",
    "# - matplotlib: Visualization library for routing probability charts\n",
    "print(\"\\n\ud83d\udce6 Installing Colab-specific packages...\")\n",
    "!pip install -q nest_asyncio ipywidgets matplotlib\n",
    "\n",
    "# Apply nest_asyncio for async support\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add repo to Python path\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "print(\"\\n\u2705 Setup complete!\")\n",
    "print(f\"\ud83d\udcc1 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd11 Step 2: API Key Configuration (Optional)\n",
    "\n",
    "The neural meta-controllers use **pre-trained local models**, so API keys are **optional**.\n",
    "\n",
    "However, if you want to:\n",
    "- Use LLM-powered agents (HRM/TRM with actual generation)\n",
    "- Enable LangSmith tracing for debugging\n",
    "- Use Weights & Biases for experiment tracking\n",
    "\n",
    "You can configure your API keys below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"API key configuration with secure handling.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def set_key(name: str, required: bool = False) -> Optional[str]:\n",
    "    \"\"\"Set API key from Colab Secrets or manual input.\n",
    "    \n",
    "    Attempts to load API key from Google Colab's secret manager first,\n",
    "    falling back to secure password input if required.\n",
    "    \n",
    "    Args:\n",
    "        name: The environment variable name for the API key.\n",
    "        required: If True, prompt user for input when not found in secrets.\n",
    "    \n",
    "    Returns:\n",
    "        The API key value or None if not set and not required.\n",
    "    \n",
    "    Examples:\n",
    "        >>> key = set_key(\"OPENAI_API_KEY\")\n",
    "        >>> if key:\n",
    "        ...     os.environ[\"OPENAI_API_KEY\"] = key\n",
    "    \"\"\"\n",
    "    # Try Colab Secrets first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        value = userdata.get(name)\n",
    "        if value:\n",
    "            print(f\"\u2705 {name} loaded from Colab Secrets\")\n",
    "            return value\n",
    "    except ImportError:\n",
    "        # Not running in Colab environment\n",
    "        pass\n",
    "    except (KeyError, AttributeError):\n",
    "        # Secret not found in Colab userdata\n",
    "        pass\n",
    "    \n",
    "    if required:\n",
    "        return getpass.getpass(f\"Enter your {name}: \")\n",
    "    \n",
    "    print(f\"\u26a0\ufe0f {name} not set (optional)\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Configure API Keys\n",
    "print(\"\ud83d\udd11 Configuring API Keys...\\n\")\n",
    "\n",
    "# OpenAI - for LLM-powered agents (optional)\n",
    "openai_key = set_key(\"OPENAI_API_KEY\")\n",
    "if openai_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "# LangSmith - for tracing (optional but recommended)\n",
    "langchain_key = set_key(\"LANGCHAIN_API_KEY\")\n",
    "if langchain_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-multi-agent-mcts\"\n",
    "\n",
    "# Weights & Biases - for experiment tracking (optional)\n",
    "wandb_key = set_key(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "\n",
    "print(\"\\n\u2705 API key configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Step 3: Load Trained Neural Meta-Controllers\n",
    "\n",
    "Initialize the framework with the pre-trained models:\n",
    "- **RNN Meta-Controller**: Fast, captures sequential patterns (10D features \u2192 3-class routing)\n",
    "- **BERT with LoRA**: Context-aware text understanding for complex routing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load trained neural meta-controllers with secure model loading.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from src.agents.meta_controller.rnn_controller import RNNMetaController\n",
    "from src.agents.meta_controller.bert_controller_v2 import BERTMetaController\n",
    "from src.agents.meta_controller.base import MetaControllerFeatures\n",
    "\n",
    "# =============================================================================\n",
    "# CONSTANTS\n",
    "# =============================================================================\n",
    "MIN_PYTORCH_VERSION: str = \"1.13.0\"  # Minimum version with weights_only support\n",
    "\n",
    "print(\"\ud83e\udde0 Initializing Neural Meta-Controllers...\\n\")\n",
    "\n",
    "# Detect device\n",
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\ud83d\udcbb Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "def load_torch_weights_secure(path: Path, map_location: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load PyTorch weights securely with weights_only=True.\n",
    "    \n",
    "    This function enforces secure model loading to prevent arbitrary code\n",
    "    execution from malicious pickle files. Requires PyTorch >= 1.13.0.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the model weights file (.pt or .pth).\n",
    "        map_location: Device to load the model onto ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "        The loaded state dictionary.\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If PyTorch version is too old for secure loading.\n",
    "        FileNotFoundError: If the model file doesn't exist.\n",
    "    \n",
    "    Note:\n",
    "        The weights_only=True parameter prevents pickle deserialization\n",
    "        attacks (CVE-2022-0778 class vulnerabilities). Never use\n",
    "        torch.load() without this parameter on untrusted files.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {path}\")\n",
    "    \n",
    "    # Require secure loading - no fallback to unsafe torch.load()\n",
    "    # PyTorch >= 1.13.0 is required (repo requires torch>=2.1.0)\n",
    "    return torch.load(path, map_location=map_location, weights_only=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD RNN CONTROLLER\n",
    "# =============================================================================\n",
    "print(\"\\n\ud83d\udd04 Loading RNN Meta-Controller...\")\n",
    "rnn_controller = RNNMetaController(name=\"RNNController\", seed=42, device=device)\n",
    "\n",
    "rnn_model_path = Path(REPO_PATH) / \"models\" / \"rnn_meta_controller.pt\"\n",
    "if rnn_model_path.exists():\n",
    "    try:\n",
    "        checkpoint = load_torch_weights_secure(rnn_model_path, map_location=device)\n",
    "        rnn_controller.model.load_state_dict(checkpoint)\n",
    "        rnn_controller.model.eval()\n",
    "        print(f\"   \u2705 Loaded trained weights from {rnn_model_path.name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"   \u26a0\ufe0f PyTorch error loading weights: {e}\")\n",
    "        print(\"      Using untrained model.\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f Weights not found at {rnn_model_path}\")\n",
    "    print(\"      Using untrained model.\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD BERT CONTROLLER\n",
    "# =============================================================================\n",
    "print(\"\\n\ud83e\udd16 Loading BERT Meta-Controller with LoRA...\")\n",
    "bert_controller = BERTMetaController(\n",
    "    name=\"BERTController\", seed=42, device=device, use_lora=True\n",
    ")\n",
    "\n",
    "bert_model_path = Path(REPO_PATH) / \"models\" / \"bert_lora\" / \"final_model\"\n",
    "if bert_model_path.exists():\n",
    "    try:\n",
    "        bert_controller.load_model(str(bert_model_path))\n",
    "        print(f\"   \u2705 Loaded trained LoRA weights from {bert_model_path.name}\")\n",
    "    except (OSError, IOError) as e:\n",
    "        print(f\"   \u26a0\ufe0f File I/O error: {e}\")\n",
    "        print(\"      Check that model files are not corrupted.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"   \u26a0\ufe0f PyTorch runtime error: {e}\")\n",
    "        print(\"      This may be due to CUDA/model architecture mismatch.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"   \u26a0\ufe0f Configuration error: {e}\")\n",
    "        print(\"      Model may be incompatible with current BERT/LoRA config.\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f Weights not found at {bert_model_path}\")\n",
    "    print(\"      Using untrained model.\")\n",
    "\n",
    "print(\"\\n\u2705 Meta-controllers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfae Step 4: Interactive Agent Routing Demo\n",
    "\n",
    "Try the neural meta-controllers! Enter a query and see how the controllers decide which agent to route it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature extraction and routing functions with comprehensive type hints.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import List, Optional, Tuple, Final\n",
    "\n",
    "# =============================================================================\n",
    "# KEYWORD CONSTANTS FOR HEURISTIC FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "TECHNICAL_KEYWORDS: Final[List[str]] = [\n",
    "    \"algorithm\", \"code\", \"implement\", \"technical\", \"system\",\n",
    "    \"function\", \"class\", \"method\", \"api\", \"database\",\n",
    "]\n",
    "\n",
    "COMPARISON_KEYWORDS: Final[List[str]] = [\n",
    "    \"vs\", \"versus\", \"compare\", \"difference\", \"better\",\n",
    "    \"pros\", \"cons\", \"tradeoff\", \"advantage\", \"disadvantage\",\n",
    "]\n",
    "\n",
    "OPTIMIZATION_KEYWORDS: Final[List[str]] = [\n",
    "    \"optimize\", \"best\", \"improve\", \"maximize\", \"minimize\",\n",
    "    \"efficient\", \"performance\", \"faster\", \"reduce\", \"scale\",\n",
    "]\n",
    "\n",
    "# Input validation constants\n",
    "MAX_QUERY_LENGTH: Final[int] = 2000\n",
    "QUERY_DISPLAY_LENGTH: Final[int] = 60\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE EXTRACTOR INITIALIZATION\n",
    "# =============================================================================\n",
    "_feature_extractor: Optional[\"FeatureExtractor\"] = None\n",
    "\n",
    "try:\n",
    "    from src.agents.meta_controller.feature_extractor import (\n",
    "        FeatureExtractor,\n",
    "        FeatureExtractorConfig,\n",
    "    )\n",
    "    config = FeatureExtractorConfig.from_env()\n",
    "    config.device = device\n",
    "    _feature_extractor = FeatureExtractor(config)\n",
    "    print(\"\u2705 Using semantic feature extraction\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f Feature extractor module not found: {e}\")\n",
    "    print(\"   Falling back to heuristic feature extraction.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"\u26a0\ufe0f Feature extractor configuration error: {e}\")\n",
    "    print(\"   Check that FeatureExtractorConfig has from_env() method.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\u26a0\ufe0f Feature extractor initialization failed: {e}\")\n",
    "    print(\"   This may be due to missing embedding models.\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE EXTRACTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def _extract_heuristic_features(\n",
    "    query: str,\n",
    "    iteration: int,\n",
    "    last_agent: str,\n",
    ") -> MetaControllerFeatures:\n",
    "    \"\"\"Extract features using keyword-based heuristics.\n",
    "    \n",
    "    Args:\n",
    "        query: The input query text.\n",
    "        iteration: Current routing iteration (for multi-turn).\n",
    "        last_agent: Name of previously selected agent.\n",
    "    \n",
    "    Returns:\n",
    "        MetaControllerFeatures instance with heuristic-based values.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    query_length = len(query)\n",
    "    \n",
    "    has_technical = any(word in query_lower for word in TECHNICAL_KEYWORDS)\n",
    "    has_comparison = any(word in query_lower for word in COMPARISON_KEYWORDS)\n",
    "    has_optimization = any(word in query_lower for word in OPTIMIZATION_KEYWORDS)\n",
    "    \n",
    "    # Calculate confidence scores based on keyword presence\n",
    "    hrm_conf = 0.5 + (0.2 if has_technical else 0.0)\n",
    "    trm_conf = 0.5 + (0.2 if has_comparison else 0.0)\n",
    "    mcts_conf = 0.5 + (0.2 if has_optimization else 0.0)\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    total = hrm_conf + trm_conf + mcts_conf\n",
    "    \n",
    "    return MetaControllerFeatures(\n",
    "        hrm_confidence=hrm_conf / total,\n",
    "        trm_confidence=trm_conf / total,\n",
    "        mcts_value=mcts_conf / total,\n",
    "        consensus_score=0.6,\n",
    "        last_agent=last_agent,\n",
    "        iteration=iteration,\n",
    "        query_length=query_length,\n",
    "        has_rag_context=query_length > 50,\n",
    "        rag_relevance_score=0.7 if query_length > 50 else 0.0,\n",
    "        is_technical_query=has_technical,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    query: str,\n",
    "    iteration: int = 0,\n",
    "    last_agent: str = \"none\",\n",
    ") -> MetaControllerFeatures:\n",
    "    \"\"\"Extract features from a query for the meta-controller.\n",
    "    \n",
    "    Uses semantic embeddings if available, otherwise falls back to\n",
    "    keyword-based heuristic extraction.\n",
    "    \n",
    "    Args:\n",
    "        query: The input query text.\n",
    "        iteration: Current routing iteration (for multi-turn).\n",
    "        last_agent: Name of previously selected agent.\n",
    "    \n",
    "    Returns:\n",
    "        MetaControllerFeatures instance for routing decision.\n",
    "    \n",
    "    Examples:\n",
    "        >>> features = extract_features(\"Compare PostgreSQL vs MongoDB\")\n",
    "        >>> features.is_technical_query\n",
    "        True\n",
    "    \"\"\"\n",
    "    if _feature_extractor is not None:\n",
    "        return _feature_extractor.extract_features(query, iteration, last_agent)\n",
    "    return _extract_heuristic_features(query, iteration, last_agent)\n",
    "\n",
    "\n",
    "def validate_query(query: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate query input for safety and length.\n",
    "    \n",
    "    Args:\n",
    "        query: The query string to validate.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (is_valid, error_message). If valid, error_message is empty.\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return False, \"Please enter a query.\"\n",
    "    \n",
    "    query = query.strip()\n",
    "    \n",
    "    if len(query) > MAX_QUERY_LENGTH:\n",
    "        return False, f\"Query too long. Maximum {MAX_QUERY_LENGTH} characters.\"\n",
    "    \n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def route_query(\n",
    "    query: str,\n",
    "    controller_type: str = \"rnn\",\n",
    ") -> Tuple[\"MetaControllerPrediction\", MetaControllerFeatures]:\n",
    "    \"\"\"Route a query using the specified meta-controller.\n",
    "    \n",
    "    Args:\n",
    "        query: The input query to route.\n",
    "        controller_type: Either 'rnn' or 'bert'.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (prediction, features).\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If controller_type is not 'rnn' or 'bert'.\n",
    "    \"\"\"\n",
    "    features = extract_features(query)\n",
    "    \n",
    "    controller_type_lower = controller_type.lower()\n",
    "    if controller_type_lower == \"rnn\":\n",
    "        prediction = rnn_controller.predict(features)\n",
    "    elif controller_type_lower == \"bert\":\n",
    "        prediction = bert_controller.predict(features)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid controller_type: {controller_type}\")\n",
    "    \n",
    "    return prediction, features\n",
    "\n",
    "\n",
    "print(\"\u2705 Routing functions ready!\")\n",
    "print(f\"   Technical keywords: {len(TECHNICAL_KEYWORDS)}\")\n",
    "print(f\"   Comparison keywords: {len(COMPARISON_KEYWORDS)}\")\n",
    "print(f\"   Optimization keywords: {len(OPTIMIZATION_KEYWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive routing demo with example queries.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Final\n",
    "\n",
    "# =============================================================================\n",
    "# CONSTANTS\n",
    "# =============================================================================\n",
    "SEPARATOR_LENGTH: Final[int] = 60\n",
    "\n",
    "EXAMPLE_QUERIES: Final[List[str]] = [\n",
    "    \"What are the key factors when choosing between microservices and monolithic architecture?\",\n",
    "    \"How can we optimize a Python application that processes 10GB of log files daily?\",\n",
    "    \"Compare B-trees vs LSM-trees for write-heavy workloads\",\n",
    "    \"Design a distributed rate limiting system for 100k requests per second\",\n",
    "    \"Explain the difference between supervised and unsupervised learning\",\n",
    "]\n",
    "\n",
    "\n",
    "def print_controller_prediction(name: str, prediction: \"MetaControllerPrediction\") -> None:\n",
    "    \"\"\"Print formatted controller prediction results.\n",
    "    \n",
    "    Args:\n",
    "        name: Display name for the controller.\n",
    "        prediction: The prediction result to display.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"     Selected Agent: {prediction.agent.upper()}\")\n",
    "    print(f\"     Confidence: {prediction.confidence:.1%}\")\n",
    "    probs = prediction.probabilities\n",
    "    print(f\"     Probabilities: HRM={probs['hrm']:.1%}, \"\n",
    "          f\"TRM={probs['trm']:.1%}, MCTS={probs['mcts']:.1%}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMO EXECUTION\n",
    "# =============================================================================\n",
    "print(\"\ud83e\udde0 Neural Meta-Controller Routing Demo\")\n",
    "print(\"=\" * SEPARATOR_LENGTH)\n",
    "\n",
    "for i, query in enumerate(EXAMPLE_QUERIES, 1):\n",
    "    print(f\"\\n\ud83d\udcdd Query {i}: {query[:QUERY_DISPLAY_LENGTH]}...\")\n",
    "    print(\"-\" * SEPARATOR_LENGTH)\n",
    "    \n",
    "    # Get predictions from both controllers\n",
    "    rnn_pred, features = route_query(query, \"rnn\")\n",
    "    bert_pred, _ = route_query(query, \"bert\")\n",
    "    \n",
    "    print_controller_prediction(\"\ud83d\udd04 RNN Controller\", rnn_pred)\n",
    "    print_controller_prediction(\"\ud83e\udd16 BERT Controller\", bert_pred)\n",
    "    \n",
    "    # Agreement check\n",
    "    if rnn_pred.agent == bert_pred.agent:\n",
    "        print(f\"\\n  \u2705 Controllers AGREE: {rnn_pred.agent.upper()}\")\n",
    "    else:\n",
    "        print(f\"\\n  \u26a0\ufe0f Controllers DISAGREE: \"\n",
    "              f\"RNN={rnn_pred.agent.upper()}, BERT={bert_pred.agent.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfb2 Step 5: Monte Carlo Tree Search (MCTS) Demo\n",
    "\n",
    "Explore the MCTS engine - the strategic planning component that simulates multiple decision paths.\n",
    "\n",
    "This demo uses the framework's `MCTSConfig` with proper attribute names and seeded RNG for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MCTS demonstration with proper configuration and seeded RNG.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import List, Final\n",
    "\n",
    "from src.framework.mcts.core import MCTSNode, MCTSState\n",
    "from src.framework.mcts.config import MCTSConfig, ConfigPreset, create_preset_config\n",
    "\n",
    "# =============================================================================\n",
    "# CONSTANTS\n",
    "# =============================================================================\n",
    "MCTS_SEED: Final[int] = 42\n",
    "MCTS_DEMO_ITERATIONS: Final[int] = 100\n",
    "MCTS_VALUE_MIN: Final[float] = 0.3\n",
    "MCTS_VALUE_MAX: Final[float] = 0.9\n",
    "NUM_ACTIONS: Final[int] = 3\n",
    "ASCII_OFFSET_A: Final[int] = 65  # ord('A')\n",
    "TOP_ACTIONS_DISPLAY: Final[int] = 3\n",
    "\n",
    "print(\"\ud83c\udfb2 Monte Carlo Tree Search Demo\")\n",
    "print(\"=\" * SEPARATOR_LENGTH)\n",
    "\n",
    "# Initialize seeded RNG for reproducibility\n",
    "rng = random.Random(MCTS_SEED)\n",
    "\n",
    "# Create MCTS configuration using framework's preset\n",
    "# Note: create_preset_config takes ConfigPreset enum, not string\n",
    "config: MCTSConfig = create_preset_config(ConfigPreset.BALANCED)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca MCTS Configuration (BALANCED preset):\")\n",
    "print(f\"   Iterations: {config.num_iterations}\")  # Correct attribute name\n",
    "print(f\"   Exploration Weight (C): {config.exploration_weight}\")\n",
    "print(f\"   Max Tree Depth: {config.max_tree_depth}\")  # Correct attribute name\n",
    "print(f\"   Random Seed: {MCTS_SEED}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MCTS DOMAIN FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_actions(state: MCTSState) -> List[str]:\n",
    "    \"\"\"Generate possible actions from current state.\n",
    "    \n",
    "    Generates up to NUM_ACTIONS actions based on the current state depth.\n",
    "    Returns empty list if maximum tree depth is reached (terminal state).\n",
    "    \n",
    "    Args:\n",
    "        state: Current MCTS state to generate actions from.\n",
    "    \n",
    "    Returns:\n",
    "        List of action strings, or empty list if terminal.\n",
    "    \n",
    "    Note:\n",
    "        In a real application, this would generate domain-specific actions\n",
    "        based on the problem state (e.g., chess moves, planning steps).\n",
    "    \"\"\"\n",
    "    depth = len(state.state_id.split(\"_\"))\n",
    "    if depth > config.max_tree_depth:\n",
    "        return []  # Terminal state\n",
    "    return [f\"action_{chr(ASCII_OFFSET_A + i)}\" for i in range(NUM_ACTIONS)]\n",
    "\n",
    "\n",
    "def evaluate_state(state: MCTSState) -> float:\n",
    "    \"\"\"Evaluate state value using seeded RNG for reproducibility.\n",
    "    \n",
    "    This is a placeholder evaluation function for demonstration purposes.\n",
    "    In production, this would use a trained neural network or domain-specific\n",
    "    heuristics to evaluate the state.\n",
    "    \n",
    "    Args:\n",
    "        state: The MCTS state to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "        Value estimate in range [MCTS_VALUE_MIN, MCTS_VALUE_MAX].\n",
    "    \n",
    "    Note:\n",
    "        Uses seeded RNG instead of random.uniform() to ensure\n",
    "        deterministic behavior for debugging and testing.\n",
    "    \"\"\"\n",
    "    return rng.uniform(MCTS_VALUE_MIN, MCTS_VALUE_MAX)\n",
    "\n",
    "\n",
    "def transition(state: MCTSState, action: str) -> MCTSState:\n",
    "    \"\"\"Transition to new state by applying action.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state.\n",
    "        action: Action to apply.\n",
    "    \n",
    "    Returns:\n",
    "        New MCTSState after applying the action.\n",
    "    \"\"\"\n",
    "    return MCTSState(f\"{state.state_id}_{action}\")\n",
    "\n",
    "\n",
    "def run_mcts_simulation(\n",
    "    root: MCTSNode,\n",
    "    iterations: int,\n",
    "    exploration_weight: float,\n",
    ") -> MCTSNode:\n",
    "    \"\"\"Run MCTS simulation for specified iterations.\n",
    "    \n",
    "    Executes the four MCTS phases: Selection, Expansion, Simulation,\n",
    "    and Backpropagation.\n",
    "    \n",
    "    Args:\n",
    "        root: Root node to start simulation from.\n",
    "        iterations: Number of simulation iterations.\n",
    "        exploration_weight: UCB1 exploration constant.\n",
    "    \n",
    "    Returns:\n",
    "        Updated root node after simulation.\n",
    "    \"\"\"\n",
    "    for _ in range(iterations):\n",
    "        # Selection - traverse to leaf using UCB1\n",
    "        node = root\n",
    "        while node.children and not node.terminal:\n",
    "            node = node.select_child(exploration_weight)\n",
    "        \n",
    "        # Expansion - add children if not terminal\n",
    "        if not node.terminal and node.visits > 0:\n",
    "            actions = generate_actions(node.state)\n",
    "            if actions:\n",
    "                action = rng.choice(actions)\n",
    "                child_state = transition(node.state, action)\n",
    "                node = node.add_child(action=action, child_state=child_state)\n",
    "            else:\n",
    "                node.terminal = True\n",
    "        \n",
    "        # Simulation - evaluate\n",
    "        value = evaluate_state(node.state)\n",
    "        \n",
    "        # Backpropagation - update all ancestors\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    \n",
    "    return root\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN SIMULATION\n",
    "# =============================================================================\n",
    "print(f\"\\n\ud83c\udfaf Running MCTS simulation ({MCTS_DEMO_ITERATIONS} iterations)...\")\n",
    "\n",
    "root = MCTSNode(state=MCTSState(\"root\"))\n",
    "root = run_mcts_simulation(root, MCTS_DEMO_ITERATIONS, config.exploration_weight)\n",
    "\n",
    "# Results\n",
    "print(f\"\\n\ud83d\udcca MCTS Results:\")\n",
    "print(f\"   Root visits: {root.visits}\")\n",
    "print(f\"   Root value: {root.value:.3f}\")\n",
    "print(f\"   Children expanded: {len(root.children)}\")\n",
    "\n",
    "if root.children:\n",
    "    print(f\"\\n\ud83c\udfc6 Best Actions (by visits - most robust selection):\")\n",
    "    sorted_children = sorted(root.children, key=lambda c: c.visits, reverse=True)\n",
    "    \n",
    "    for i, child in enumerate(sorted_children[:TOP_ACTIONS_DISPLAY], 1):\n",
    "        ucb1 = (\n",
    "            child.value + \n",
    "            config.exploration_weight * (root.visits ** 0.5) / (child.visits + 1)\n",
    "        )\n",
    "        print(f\"   {i}. {child.action}: \"\n",
    "              f\"visits={child.visits}, value={child.value:.3f}, UCB1={ucb1:.3f}\")\n",
    "    \n",
    "    best = sorted_children[0]\n",
    "    print(f\"\\n   \u2705 Recommended: {best.action} (highest visit count)\")\n",
    "    print(f\"   \ud83d\udca1 Reproducible with seed={MCTS_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 6: Visualize Routing Probabilities\n",
    "\n",
    "Create a visual comparison of how different queries are routed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization of routing probabilities with refactored chart creation.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Final, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION CONSTANTS\n",
    "# =============================================================================\n",
    "FIGURE_WIDTH: Final[int] = 14\n",
    "FIGURE_HEIGHT: Final[int] = 6\n",
    "BAR_WIDTH: Final[float] = 0.25\n",
    "BAR_ALPHA: Final[float] = 0.8\n",
    "PROBABILITY_MIN: Final[float] = 0.0\n",
    "PROBABILITY_MAX: Final[float] = 1.0\n",
    "GRID_ALPHA: Final[float] = 0.3\n",
    "\n",
    "AGENT_NAMES: Final[List[str]] = ['HRM', 'TRM', 'MCTS']\n",
    "AGENT_COLORS: Final[List[str]] = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "VISUALIZATION_QUERIES: Final[List[str]] = [\n",
    "    \"Implement a binary search tree\",\n",
    "    \"Compare PostgreSQL vs MongoDB\",\n",
    "    \"Optimize the neural network training\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Design a caching strategy for APIs\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_routing_chart(\n",
    "    ax: plt.Axes,\n",
    "    probs_arr: NDArray[np.floating],\n",
    "    title: str,\n",
    ") -> None:\n",
    "    \"\"\"Create a routing probability bar chart on the given axes.\n",
    "    \n",
    "    Args:\n",
    "        ax: Matplotlib axes to draw on.\n",
    "        probs_arr: Array of shape (n_queries, 3) with probabilities.\n",
    "        title: Chart title.\n",
    "    \"\"\"\n",
    "    x = np.arange(len(probs_arr))\n",
    "    \n",
    "    for i, (agent, color) in enumerate(zip(AGENT_NAMES, AGENT_COLORS)):\n",
    "        ax.bar(\n",
    "            x + i * BAR_WIDTH,\n",
    "            probs_arr[:, i],\n",
    "            BAR_WIDTH,\n",
    "            label=agent,\n",
    "            color=color,\n",
    "            alpha=BAR_ALPHA,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Query')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + BAR_WIDTH)\n",
    "    ax.set_xticklabels([f'Q{i+1}' for i in range(len(probs_arr))])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(PROBABILITY_MIN, PROBABILITY_MAX)\n",
    "    ax.grid(axis='y', alpha=GRID_ALPHA)\n",
    "\n",
    "\n",
    "def collect_predictions(\n",
    "    queries: List[str],\n",
    ") -> Tuple[NDArray[np.floating], NDArray[np.floating]]:\n",
    "    \"\"\"Collect routing predictions for all queries.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query strings.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (rnn_probs, bert_probs) arrays.\n",
    "    \"\"\"\n",
    "    rnn_probs: List[List[float]] = []\n",
    "    bert_probs: List[List[float]] = []\n",
    "    \n",
    "    for query in queries:\n",
    "        rnn_pred, _ = route_query(query, \"rnn\")\n",
    "        bert_pred, _ = route_query(query, \"bert\")\n",
    "        \n",
    "        rnn_probs.append([\n",
    "            rnn_pred.probabilities['hrm'],\n",
    "            rnn_pred.probabilities['trm'],\n",
    "            rnn_pred.probabilities['mcts'],\n",
    "        ])\n",
    "        bert_probs.append([\n",
    "            bert_pred.probabilities['hrm'],\n",
    "            bert_pred.probabilities['trm'],\n",
    "            bert_pred.probabilities['mcts'],\n",
    "        ])\n",
    "    \n",
    "    return np.array(rnn_probs), np.array(bert_probs)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE VISUALIZATION\n",
    "# =============================================================================\n",
    "rnn_probs_arr, bert_probs_arr = collect_predictions(VISUALIZATION_QUERIES)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(FIGURE_WIDTH, FIGURE_HEIGHT))\n",
    "\n",
    "create_routing_chart(axes[0], rnn_probs_arr, 'RNN Meta-Controller Routing')\n",
    "create_routing_chart(axes[1], bert_probs_arr, 'BERT Meta-Controller Routing')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\n",
    "    'Neural Meta-Controller Agent Routing Comparison',\n",
    "    y=1.02,\n",
    "    fontsize=14,\n",
    "    fontweight='bold',\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print query legend\n",
    "print(\"\\n\ud83d\udcdd Query Legend:\")\n",
    "for i, query in enumerate(VISUALIZATION_QUERIES):\n",
    "    print(f\"   Q{i+1}: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfdb\ufe0f Step 7: Full Framework Demo with Gradio UI\n",
    "\n",
    "Launch the complete Gradio interface for interactive exploration.\n",
    "\n",
    "**Security Note**: Debug mode is disabled for shared demos to prevent exposing internal details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Full Gradio app launcher (optional).\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\ud83c\udfdb\ufe0f Launching Full Framework UI...\")\n",
    "print(\"\\nNote: This will run the complete Gradio interface.\")\n",
    "print(\"Click the public URL to access from any device.\\n\")\n",
    "\n",
    "# Import and run the app\n",
    "# Uncomment the following lines to launch:\n",
    "\n",
    "# app_path = Path(REPO_PATH) / \"app.py\"\n",
    "# spec = importlib.util.spec_from_file_location(\"app\", app_path)\n",
    "# app_module = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(app_module)\n",
    "# app_module.demo.launch(share=True, debug=False)  # debug=False for security\n",
    "\n",
    "print(\"\u2139\ufe0f To launch the full UI, uncomment the lines above and run this cell.\")\n",
    "print(\"   Or run: !python app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Quick mini Gradio demo with input validation and secure settings.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Tuple, Final\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# =============================================================================\n",
    "# CONSTANTS\n",
    "# =============================================================================\n",
    "TEXTBOX_LINES: Final[int] = 3\n",
    "RESPONSE_TRUNCATE_LENGTH: Final[int] = 80\n",
    "BAR_CHART_MAX_LENGTH: Final[int] = 20\n",
    "\n",
    "\n",
    "def format_probability_bar(name: str, prob: float) -> str:\n",
    "    \"\"\"Format a probability with a visual bar chart.\n",
    "    \n",
    "    Args:\n",
    "        name: Agent name to display.\n",
    "        prob: Probability value (0.0 to 1.0).\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with percentage and visual bar.\n",
    "    \"\"\"\n",
    "    bar = '\u2588' * int(prob * BAR_CHART_MAX_LENGTH)\n",
    "    return f\"- {name}: {prob:.1%} {bar}\"\n",
    "\n",
    "\n",
    "def process_query_mini(\n",
    "    query: str,\n",
    "    controller_type: str,\n",
    ") -> Tuple[str, str, str]:\n",
    "    \"\"\"Process a query with the selected controller.\n",
    "    \n",
    "    Args:\n",
    "        query: User's input query.\n",
    "        controller_type: 'RNN' or 'BERT'.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response, routing_info, features_info).\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    is_valid, error_msg = validate_query(query)\n",
    "    if not is_valid:\n",
    "        return error_msg, \"\", \"\"\n",
    "    \n",
    "    try:\n",
    "        prediction, features = route_query(query, controller_type.lower())\n",
    "    except Exception as e:\n",
    "        # Don't leak internal details in error messages\n",
    "        return f\"\u26a0\ufe0f Error processing query. Please try again.\", \"\", \"\"\n",
    "    \n",
    "    # Format routing decision\n",
    "    routing = f\"\"\"\ud83e\udde0 **Meta-Controller Decision**\n",
    "\n",
    "**Selected Agent:** `{prediction.agent.upper()}`\n",
    "**Confidence:** {prediction.confidence:.1%}\n",
    "\n",
    "**Routing Probabilities:**\n",
    "{format_probability_bar('HRM', prediction.probabilities['hrm'])}\n",
    "{format_probability_bar('TRM', prediction.probabilities['trm'])}\n",
    "{format_probability_bar('MCTS', prediction.probabilities['mcts'])}\n",
    "\"\"\"\n",
    "    \n",
    "    # Format features\n",
    "    features_str = f\"\"\"\ud83d\udcca **Extracted Features**\n",
    "\n",
    "- Query Length: {features.query_length}\n",
    "- Technical Query: {'Yes' if features.is_technical_query else 'No'}\n",
    "- Has RAG Context: {'Yes' if features.has_rag_context else 'No'}\n",
    "- HRM Confidence: {features.hrm_confidence:.3f}\n",
    "- TRM Confidence: {features.trm_confidence:.3f}\n",
    "- MCTS Value: {features.mcts_value:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    # Simulated agent response\n",
    "    agent_responses = {\n",
    "        \"hrm\": f\"[HRM] Breaking down hierarchically: {query[:RESPONSE_TRUNCATE_LENGTH]}...\",\n",
    "        \"trm\": f\"[TRM] Applying iterative refinement: {query[:RESPONSE_TRUNCATE_LENGTH]}...\",\n",
    "        \"mcts\": f\"[MCTS] Strategic exploration: {query[:RESPONSE_TRUNCATE_LENGTH]}...\",\n",
    "    }\n",
    "    response = agent_responses.get(prediction.agent, \"Unknown agent\")\n",
    "    \n",
    "    return response, routing, features_str\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE GRADIO INTERFACE\n",
    "# =============================================================================\n",
    "with gr.Blocks(title=\"LangGraph Multi-Agent MCTS - Mini Demo\") as mini_demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # \ud83c\udfaf LangGraph Multi-Agent MCTS - Quick Demo\n",
    "    \n",
    "    Test the neural meta-controllers with your own queries!\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Enter your question...\",\n",
    "                lines=TEXTBOX_LINES,\n",
    "                max_lines=TEXTBOX_LINES * 2,\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            controller_select = gr.Radio(\n",
    "                choices=[\"RNN\", \"BERT\"],\n",
    "                value=\"RNN\",\n",
    "                label=\"Controller Type\",\n",
    "            )\n",
    "    \n",
    "    submit_btn = gr.Button(\"\ud83d\ude80 Process Query\", variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        response_output = gr.Textbox(\n",
    "            label=\"Agent Response\",\n",
    "            lines=TEXTBOX_LINES,\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        routing_output = gr.Markdown(label=\"Routing Decision\")\n",
    "        features_output = gr.Markdown(label=\"Features\")\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=process_query_mini,\n",
    "        inputs=[query_input, controller_select],\n",
    "        outputs=[response_output, routing_output, features_output],\n",
    "    )\n",
    "\n",
    "# Launch with secure settings\n",
    "# debug=False prevents exposing internal details via public URL\n",
    "mini_demo.launch(share=True, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfad Step 8: Training Your Own Models (Optional)\n",
    "\n",
    "Learn how to train the neural meta-controllers on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training pipeline demo with proper error handling.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Final\n",
    "\n",
    "from src.training.data_generator import MetaControllerDataGenerator\n",
    "from src.training.train_rnn import RNNTrainer\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION CONSTANTS\n",
    "# =============================================================================\n",
    "SAMPLES_PER_CLASS: Final[int] = 50\n",
    "TRAIN_RATIO: Final[float] = 0.7\n",
    "VAL_RATIO: Final[float] = 0.15\n",
    "\n",
    "# Model hyperparameters\n",
    "RNN_HIDDEN_DIM: Final[int] = 32\n",
    "RNN_NUM_LAYERS: Final[int] = 1\n",
    "RNN_DROPOUT: Final[float] = 0.1\n",
    "LEARNING_RATE: Final[float] = 1e-3\n",
    "BATCH_SIZE: Final[int] = 16\n",
    "TRAINING_EPOCHS: Final[int] = 3\n",
    "EARLY_STOPPING_PATIENCE: Final[int] = 2\n",
    "TRAINING_SEED: Final[int] = 42\n",
    "\n",
    "print(\"\ud83c\udfad Training Pipeline Demo\")\n",
    "print(\"=\" * SEPARATOR_LENGTH)\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\ud83d\udce6 Generating synthetic training data...\")\n",
    "generator = MetaControllerDataGenerator(seed=TRAINING_SEED)\n",
    "features_list, labels_list = generator.generate_balanced_dataset(\n",
    "    samples_per_class=SAMPLES_PER_CLASS\n",
    ")\n",
    "\n",
    "print(f\"   Total samples: {len(features_list)}\")\n",
    "print(f\"   Classes: {set(labels_list)}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X, y = generator.to_tensor_dataset(features_list, labels_list)\n",
    "print(f\"   Feature tensor shape: {X.shape}\")\n",
    "print(f\"   Label tensor shape: {y.shape}\")\n",
    "\n",
    "# Split dataset\n",
    "splits = generator.split_dataset(X, y, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO)\n",
    "print(f\"\\n\ud83d\udcca Dataset splits:\")\n",
    "print(f\"   Training: {splits['X_train'].shape[0]} samples\")\n",
    "print(f\"   Validation: {splits['X_val'].shape[0]} samples\")\n",
    "print(f\"   Test: {splits['X_test'].shape[0]} samples\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN MODEL\n",
    "# =============================================================================\n",
    "print(f\"\\n\ud83c\udfcb\ufe0f Training RNN model ({TRAINING_EPOCHS} epochs)...\")\n",
    "\n",
    "trainer = RNNTrainer(\n",
    "    hidden_dim=RNN_HIDDEN_DIM,\n",
    "    num_layers=RNN_NUM_LAYERS,\n",
    "    dropout=RNN_DROPOUT,\n",
    "    lr=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=TRAINING_EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    seed=TRAINING_SEED,\n",
    ")\n",
    "\n",
    "try:\n",
    "    history = trainer.train(\n",
    "        train_data=(splits[\"X_train\"], splits[\"y_train\"]),\n",
    "        val_data=(splits[\"X_val\"], splits[\"y_val\"]),\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\u2705 Training complete!\")\n",
    "    print(f\"   Best validation accuracy: {history['best_val_accuracy']:.2%}\")\n",
    "    print(f\"   Best validation loss: {history['best_val_loss']:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n\ud83e\uddea Evaluating on test set...\")\n",
    "    test_loader = trainer.create_dataloader(\n",
    "        splits[\"X_test\"], splits[\"y_test\"], shuffle=False\n",
    "    )\n",
    "    results = trainer.evaluate(test_loader)\n",
    "    \n",
    "    print(f\"   Test accuracy: {results['accuracy']:.2%}\")\n",
    "    print(f\"   Test loss: {results['loss']:.4f}\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n\u274c Training failed: {e}\")\n",
    "    print(\"   Consider reducing batch size or checking GPU memory.\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n\u274c Data validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfc1 Step 9: Chess MCTS Demo (Bonus)\n",
    "\n",
    "The framework includes a chess implementation using MCTS - similar to AlphaZero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Chess MCTS demo with clear installation instructions.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Final\n",
    "\n",
    "# =============================================================================\n",
    "# CHESS DEMO CONSTANTS\n",
    "# =============================================================================\n",
    "CHESS_MCTS_ITERATIONS: Final[int] = 50\n",
    "CHESS_EXPLORATION_WEIGHT: Final[float] = 1.414  # sqrt(2)\n",
    "TOP_MOVES_DISPLAY: Final[int] = 5\n",
    "\n",
    "try:\n",
    "    import chess\n",
    "    from src.games.chess.game import ChessGame\n",
    "    from src.games.chess.mcts import ChessMCTS\n",
    "    \n",
    "    print(\"\u265a Chess MCTS Demo\")\n",
    "    print(\"=\" * SEPARATOR_LENGTH)\n",
    "    \n",
    "    # Create a chess game\n",
    "    game = ChessGame()\n",
    "    print(f\"\\n\ud83c\udfaf Initial position:\")\n",
    "    print(game.board)\n",
    "    \n",
    "    # Create MCTS player\n",
    "    mcts = ChessMCTS(\n",
    "        game,\n",
    "        iterations=CHESS_MCTS_ITERATIONS,\n",
    "        exploration_weight=CHESS_EXPLORATION_WEIGHT,\n",
    "    )\n",
    "    \n",
    "    # Get best move\n",
    "    print(f\"\\n\ud83e\udd14 MCTS thinking ({CHESS_MCTS_ITERATIONS} iterations)...\")\n",
    "    best_move = mcts.get_best_move()\n",
    "    \n",
    "    print(f\"\\n\u2705 Best move: {best_move}\")\n",
    "    \n",
    "    # Show move statistics\n",
    "    print(f\"\\n\ud83d\udcca Top {TOP_MOVES_DISPLAY} move statistics:\")\n",
    "    for move, stats in mcts.get_move_stats()[:TOP_MOVES_DISPLAY]:\n",
    "        print(f\"   {move}: visits={stats['visits']}, value={stats['value']:.3f}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"\u26a0\ufe0f Chess demo not available\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"\\n\ud83d\udca1 To enable the chess demo:\")\n",
    "    print(\"   1. Run: !pip install python-chess\")\n",
    "    print(\"   2. Restart runtime: Runtime > Restart runtime\")\n",
    "    print(\"   3. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Resources & Next Steps\n",
    "\n",
    "### \ud83d\udcd6 Documentation\n",
    "- [Repository README](https://github.com/ianshank/langgraph_multi_agent_mcts)\n",
    "- [Architecture Documentation](https://github.com/ianshank/langgraph_multi_agent_mcts/blob/main/docs/langgraph_mcts_architecture.md)\n",
    "\n",
    "### \ud83d\udee0\ufe0f Key Files\n",
    "- `app.py` - Main Gradio application\n",
    "- `src/agents/meta_controller/` - Neural meta-controllers\n",
    "- `src/framework/mcts/` - MCTS implementation\n",
    "- `src/training/` - Training pipelines\n",
    "\n",
    "### \ud83c\udfc3 Run Locally\n",
    "```bash\n",
    "git clone https://github.com/ianshank/langgraph_multi_agent_mcts.git\n",
    "cd langgraph_multi_agent_mcts\n",
    "pip install -r requirements.txt\n",
    "python app.py\n",
    "```\n",
    "\n",
    "### \ud83d\udc4b Feedback\n",
    "Open an issue on GitHub or contribute to the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook completion summary.\"\"\"\n",
    "print(\"\ud83c\udf89 Notebook complete!\")\n",
    "print(\"\\nYou've explored:\")\n",
    "print(\"  \u2705 Neural Meta-Controllers (RNN and BERT with LoRA)\")\n",
    "print(\"  \u2705 Monte Carlo Tree Search (MCTS) with MCTSEngine\")\n",
    "print(\"  \u2705 Agent Routing Visualization\")\n",
    "print(\"  \u2705 Training Pipeline\")\n",
    "print(\"  \u2705 Interactive Gradio Demo (secure mode)\")\n",
    "print(\"\\n\ud83d\ude80 Happy experimenting!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
