{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf LangGraph Multi-Agent MCTS Framework\n",
    "\n",
    "## Production Demo with Trained Neural Meta-Controllers\n",
    "\n",
    "This notebook demonstrates the **LangGraph Multi-Agent MCTS Framework** - a sophisticated multi-agent system that combines:\n",
    "\n",
    "- **LangGraph** for explicit state management and agent orchestration\n",
    "- **Monte Carlo Tree Search (MCTS)** for strategic planning and exploration\n",
    "- **Neural Meta-Controllers** (RNN and BERT with LoRA) for intelligent agent routing\n",
    "\n",
    "### \ud83e\udde0 Trained Models\n",
    "- **RNN Meta-Controller**: GRU-based sequential pattern recognition for fast routing\n",
    "- **BERT with LoRA**: Transformer-based text understanding with parameter-efficient fine-tuning\n",
    "\n",
    "### \ud83e\udd16 Agents\n",
    "- **HRM (Hierarchical Reasoning Model)**: Decomposes complex problems hierarchically\n",
    "- **TRM (Tiny Recursive Model)**: Iterative refinement for progressive improvement\n",
    "- **MCTS**: Monte Carlo Tree Search for optimization and strategic exploration\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: [github.com/ianshank/langgraph_multi_agent_mcts](https://github.com/ianshank/langgraph_multi_agent_mcts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Step 1: Environment Setup\n",
    "\n",
    "Clone the repository and install all dependencies. This cell handles:\n",
    "- Repository cloning (using Python's `shutil` for safe directory removal)\n",
    "- Dependency installation (including `nest_asyncio` for async support in Colab)\n",
    "- Path configuration\n",
    "\n",
    "### Why `nest_asyncio`?\n",
    "Google Colab already runs an asyncio event loop in the background. When LangGraph agents\n",
    "try to create their own event loops, this causes conflicts. `nest_asyncio` patches the\n",
    "asyncio module to allow nested event loops, enabling async agent operations within Colab's\n",
    "existing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Repository configuration\n",
    "REPO_NAME = \"langgraph_multi_agent_mcts\"\n",
    "REPO_PATH = f\"/content/{REPO_NAME}\"\n",
    "REPO_URL = \"https://github.com/ianshank/langgraph_multi_agent_mcts.git\"\n",
    "\n",
    "# 1. Clean up existing directory safely using shutil (cross-platform)\n",
    "print(\"\ud83d\udce6 Setting up repository...\")\n",
    "repo_dir = Path(REPO_PATH)\n",
    "if repo_dir.exists():\n",
    "    print(f\"   Removing existing directory: {REPO_PATH}\")\n",
    "    shutil.rmtree(REPO_PATH)\n",
    "\n",
    "# 2. Clone the repository\n",
    "print(\"\ud83d\udce6 Cloning repository...\")\n",
    "!git clone {REPO_URL} {REPO_PATH}\n",
    "\n",
    "# 3. Change to repo directory\n",
    "%cd {REPO_PATH}\n",
    "\n",
    "# 4. Install dependencies from requirements.txt\n",
    "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# 5. Install additional Colab-specific packages\n",
    "# - nest_asyncio: Enables nested event loops (required for LangGraph in Colab)\n",
    "# - ipywidgets: Interactive widgets for Jupyter\n",
    "# - matplotlib: Visualization library for routing probability charts\n",
    "print(\"\\n\ud83d\udce6 Installing Colab-specific packages...\")\n",
    "!pip install -q nest_asyncio ipywidgets matplotlib\n",
    "\n",
    "# 6. Apply nest_asyncio for async support in Jupyter\n",
    "# This is required because Colab runs its own event loop, and nest_asyncio\n",
    "# allows LangGraph's async operations to run within that existing loop\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 7. Add repo to Python path\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "print(\"\\n\u2705 Setup complete!\")\n",
    "print(f\"\ud83d\udcc1 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd11 Step 2: API Key Configuration (Optional)\n",
    "\n",
    "The neural meta-controllers use **pre-trained local models**, so API keys are **optional**.\n",
    "\n",
    "However, if you want to:\n",
    "- Use LLM-powered agents (HRM/TRM with actual generation)\n",
    "- Enable LangSmith tracing for debugging\n",
    "- Use Weights & Biases for experiment tracking\n",
    "\n",
    "You can configure your API keys below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def set_key(name: str, required: bool = False):\n",
    "    \"\"\"Set API key from Colab Secrets or manual input.\n",
    "    \n",
    "    Args:\n",
    "        name: The environment variable name for the API key\n",
    "        required: If True, prompt user for input when not found in secrets\n",
    "    \n",
    "    Returns:\n",
    "        The API key value or None if not set\n",
    "    \"\"\"\n",
    "    # Try Colab Secrets first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        value = userdata.get(name)\n",
    "        if value:\n",
    "            print(f\"\u2705 {name} loaded from Colab Secrets\")\n",
    "            return value\n",
    "    except ImportError:\n",
    "        # Not running in Colab\n",
    "        pass\n",
    "    except Exception:\n",
    "        # Secret not found or other userdata error\n",
    "        pass\n",
    "    \n",
    "    if required:\n",
    "        return getpass.getpass(f\"Enter your {name}: \")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f {name} not set (optional)\")\n",
    "        return None\n",
    "\n",
    "# Optional API Keys\n",
    "print(\"\ud83d\udd11 Configuring API Keys...\\n\")\n",
    "\n",
    "# OpenAI - for LLM-powered agents (optional)\n",
    "openai_key = set_key(\"OPENAI_API_KEY\")\n",
    "if openai_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "# LangSmith - for tracing (optional but recommended)\n",
    "langchain_key = set_key(\"LANGCHAIN_API_KEY\")\n",
    "if langchain_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-multi-agent-mcts\"\n",
    "\n",
    "# Weights & Biases - for experiment tracking (optional)\n",
    "wandb_key = set_key(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "\n",
    "print(\"\\n\u2705 API key configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Step 3: Load Trained Neural Meta-Controllers\n",
    "\n",
    "Initialize the framework with the pre-trained models:\n",
    "- **RNN Meta-Controller**: Fast, captures sequential patterns (10D features \u2192 3-class routing)\n",
    "- **BERT with LoRA**: Context-aware text understanding for complex routing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Import meta-controllers\n",
    "from src.agents.meta_controller.rnn_controller import RNNMetaController\n",
    "from src.agents.meta_controller.bert_controller_v2 import BERTMetaController\n",
    "from src.agents.meta_controller.base import MetaControllerFeatures\n",
    "\n",
    "print(\"\ud83e\udde0 Initializing Neural Meta-Controllers...\\n\")\n",
    "\n",
    "# Detect device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\ud83d\udcbb Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Helper function for loading weights with version compatibility\n",
    "def load_torch_weights(path: Path, map_location: str):\n",
    "    \"\"\"Load PyTorch weights with compatibility for different versions.\n",
    "    \n",
    "    The `weights_only` parameter was introduced in PyTorch 1.13 for security.\n",
    "    This function handles both old and new PyTorch versions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PyTorch >= 1.13: Use weights_only=True for security\n",
    "        return torch.load(path, map_location=map_location, weights_only=True)\n",
    "    except TypeError:\n",
    "        # PyTorch < 1.13: weights_only parameter doesn't exist\n",
    "        return torch.load(path, map_location=map_location)\n",
    "\n",
    "# Initialize RNN Controller\n",
    "print(\"\\n\ud83d\udd04 Loading RNN Meta-Controller...\")\n",
    "rnn_controller = RNNMetaController(name=\"RNNController\", seed=42, device=device)\n",
    "\n",
    "# Load trained weights\n",
    "rnn_model_path = Path(REPO_PATH) / \"models\" / \"rnn_meta_controller.pt\"\n",
    "if rnn_model_path.exists():\n",
    "    checkpoint = load_torch_weights(rnn_model_path, map_location=device)\n",
    "    rnn_controller.model.load_state_dict(checkpoint)\n",
    "    rnn_controller.model.eval()\n",
    "    print(f\"   \u2705 Loaded trained weights from {rnn_model_path.name}\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f Using untrained model (weights not found at {rnn_model_path})\")\n",
    "\n",
    "# Initialize BERT Controller with LoRA\n",
    "print(\"\\n\ud83e\udd16 Loading BERT Meta-Controller with LoRA...\")\n",
    "bert_controller = BERTMetaController(name=\"BERTController\", seed=42, device=device, use_lora=True)\n",
    "\n",
    "# Load trained weights with detailed error handling\n",
    "bert_model_path = Path(REPO_PATH) / \"models\" / \"bert_lora\" / \"final_model\"\n",
    "if bert_model_path.exists():\n",
    "    try:\n",
    "        bert_controller.load_model(str(bert_model_path))\n",
    "        print(f\"   \u2705 Loaded trained LoRA weights from {bert_model_path.name}\")\n",
    "    except (OSError, IOError) as e:\n",
    "        print(f\"   \u26a0\ufe0f File I/O error loading BERT weights: {e}\")\n",
    "        print(\"      Check that model files are not corrupted.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"   \u26a0\ufe0f PyTorch runtime error: {e}\")\n",
    "        print(\"      This may be due to CUDA version mismatch or incompatible model architecture.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"   \u26a0\ufe0f Model configuration error: {e}\")\n",
    "        print(\"      The saved model may be incompatible with current BERT/LoRA configuration.\")\n",
    "else:\n",
    "    print(f\"   \u26a0\ufe0f Using untrained model (weights not found at {bert_model_path})\")\n",
    "\n",
    "print(\"\\n\u2705 Meta-controllers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfae Step 4: Interactive Agent Routing Demo\n",
    "\n",
    "Try the neural meta-controllers! Enter a query and see how the controllers decide which agent to route it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# Define keyword lists as constants for maintainability\n",
    "TECHNICAL_KEYWORDS: List[str] = [\n",
    "    \"algorithm\", \"code\", \"implement\", \"technical\", \"system\",\n",
    "    \"function\", \"class\", \"method\", \"api\", \"database\"\n",
    "]\n",
    "\n",
    "COMPARISON_KEYWORDS: List[str] = [\n",
    "    \"vs\", \"versus\", \"compare\", \"difference\", \"better\",\n",
    "    \"pros\", \"cons\", \"tradeoff\", \"advantage\", \"disadvantage\"\n",
    "]\n",
    "\n",
    "OPTIMIZATION_KEYWORDS: List[str] = [\n",
    "    \"optimize\", \"best\", \"improve\", \"maximize\", \"minimize\",\n",
    "    \"efficient\", \"performance\", \"faster\", \"reduce\", \"scale\"\n",
    "]\n",
    "\n",
    "# Import feature extraction with specific exception handling\n",
    "feature_extractor = None\n",
    "try:\n",
    "    from src.agents.meta_controller.feature_extractor import FeatureExtractor, FeatureExtractorConfig\n",
    "    config = FeatureExtractorConfig.from_env()\n",
    "    config.device = device\n",
    "    feature_extractor = FeatureExtractor(config)\n",
    "    print(\"\u2705 Using semantic feature extraction\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f Feature extractor module not found: {e}\")\n",
    "    print(\"   Falling back to heuristic feature extraction.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"\u26a0\ufe0f Feature extractor configuration error: {e}\")\n",
    "    print(\"   Check that FeatureExtractorConfig has from_env() method.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\u26a0\ufe0f Feature extractor initialization failed: {e}\")\n",
    "    print(\"   This may be due to missing embedding models.\")\n",
    "\n",
    "def extract_features(\n",
    "    query: str,\n",
    "    iteration: int = 0,\n",
    "    last_agent: str = \"none\"\n",
    ") -> MetaControllerFeatures:\n",
    "    \"\"\"Extract features from a query for the meta-controller.\n",
    "    \n",
    "    Uses semantic embeddings if available, otherwise falls back to\n",
    "    keyword-based heuristic extraction.\n",
    "    \n",
    "    Args:\n",
    "        query: The input query text\n",
    "        iteration: Current routing iteration (for multi-turn)\n",
    "        last_agent: Name of previously selected agent\n",
    "    \n",
    "    Returns:\n",
    "        MetaControllerFeatures instance for routing decision\n",
    "    \"\"\"\n",
    "    if feature_extractor is not None:\n",
    "        return feature_extractor.extract_features(query, iteration, last_agent)\n",
    "    \n",
    "    # Fallback heuristic extraction using keyword constants\n",
    "    query_lower = query.lower()\n",
    "    query_length = len(query)\n",
    "    \n",
    "    has_technical = any(word in query_lower for word in TECHNICAL_KEYWORDS)\n",
    "    has_comparison = any(word in query_lower for word in COMPARISON_KEYWORDS)\n",
    "    has_optimization = any(word in query_lower for word in OPTIMIZATION_KEYWORDS)\n",
    "    \n",
    "    # Calculate confidence scores based on keyword presence\n",
    "    hrm_conf = 0.5 + (0.2 if has_technical else 0)\n",
    "    trm_conf = 0.5 + (0.2 if has_comparison else 0)\n",
    "    mcts_conf = 0.5 + (0.2 if has_optimization else 0)\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    total = hrm_conf + trm_conf + mcts_conf\n",
    "    \n",
    "    return MetaControllerFeatures(\n",
    "        hrm_confidence=hrm_conf / total,\n",
    "        trm_confidence=trm_conf / total,\n",
    "        mcts_value=mcts_conf / total,\n",
    "        consensus_score=0.6,\n",
    "        last_agent=last_agent,\n",
    "        iteration=iteration,\n",
    "        query_length=query_length,\n",
    "        has_rag_context=query_length > 50,\n",
    "        rag_relevance_score=0.7 if query_length > 50 else 0.0,\n",
    "        is_technical_query=has_technical,\n",
    "    )\n",
    "\n",
    "def route_query(\n",
    "    query: str,\n",
    "    controller_type: str = \"rnn\"\n",
    ") -> Tuple[\"MetaControllerPrediction\", MetaControllerFeatures]:\n",
    "    \"\"\"Route a query using the specified meta-controller.\n",
    "    \n",
    "    Args:\n",
    "        query: The input query to route\n",
    "        controller_type: Either 'rnn' or 'bert'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (prediction, features)\n",
    "    \"\"\"\n",
    "    features = extract_features(query)\n",
    "    \n",
    "    if controller_type.lower() == \"rnn\":\n",
    "        prediction = rnn_controller.predict(features)\n",
    "    else:\n",
    "        prediction = bert_controller.predict(features)\n",
    "    \n",
    "    return prediction, features\n",
    "\n",
    "print(\"\u2705 Routing functions ready!\")\n",
    "print(f\"   Technical keywords: {len(TECHNICAL_KEYWORDS)}\")\n",
    "print(f\"   Comparison keywords: {len(COMPARISON_KEYWORDS)}\")\n",
    "print(f\"   Optimization keywords: {len(OPTIMIZATION_KEYWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83c\udfae Try it yourself!\n",
    "\n",
    "# Example queries - try changing these!\n",
    "example_queries = [\n",
    "    \"What are the key factors when choosing between microservices and monolithic architecture?\",\n",
    "    \"How can we optimize a Python application that processes 10GB of log files daily?\",\n",
    "    \"Compare B-trees vs LSM-trees for write-heavy workloads\",\n",
    "    \"Design a distributed rate limiting system for 100k requests per second\",\n",
    "    \"Explain the difference between supervised and unsupervised learning\",\n",
    "]\n",
    "\n",
    "print(\"\ud83e\udde0 Neural Meta-Controller Routing Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(example_queries, 1):\n",
    "    print(f\"\\n\ud83d\udcdd Query {i}: {query[:60]}...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get predictions from both controllers\n",
    "    rnn_pred, features = route_query(query, \"rnn\")\n",
    "    bert_pred, _ = route_query(query, \"bert\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udd04 RNN Controller:\")\n",
    "    print(f\"     Selected Agent: {rnn_pred.agent.upper()}\")\n",
    "    print(f\"     Confidence: {rnn_pred.confidence:.1%}\")\n",
    "    print(f\"     Probabilities: HRM={rnn_pred.probabilities['hrm']:.1%}, TRM={rnn_pred.probabilities['trm']:.1%}, MCTS={rnn_pred.probabilities['mcts']:.1%}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83e\udd16 BERT Controller:\")\n",
    "    print(f\"     Selected Agent: {bert_pred.agent.upper()}\")\n",
    "    print(f\"     Confidence: {bert_pred.confidence:.1%}\")\n",
    "    print(f\"     Probabilities: HRM={bert_pred.probabilities['hrm']:.1%}, TRM={bert_pred.probabilities['trm']:.1%}, MCTS={bert_pred.probabilities['mcts']:.1%}\")\n",
    "    \n",
    "    # Agreement check\n",
    "    if rnn_pred.agent == bert_pred.agent:\n",
    "        print(f\"\\n  \u2705 Controllers AGREE: {rnn_pred.agent.upper()}\")\n",
    "    else:\n",
    "        print(f\"\\n  \u26a0\ufe0f Controllers DISAGREE: RNN={rnn_pred.agent.upper()}, BERT={bert_pred.agent.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfb2 Step 5: Monte Carlo Tree Search (MCTS) Demo\n",
    "\n",
    "Explore the MCTS engine - the strategic planning component that simulates multiple decision paths.\n",
    "\n",
    "This demo uses the framework's `MCTSEngine` class for proper integration and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Callable\n",
    "from src.framework.mcts.core import MCTSNode, MCTSState, MCTSEngine\n",
    "from src.framework.mcts.config import MCTSConfig, create_preset_config\n",
    "\n",
    "print(\"\ud83c\udfb2 Monte Carlo Tree Search Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set seed for reproducibility (addresses reviewer feedback about determinism)\n",
    "MCTS_SEED = 42\n",
    "rng = random.Random(MCTS_SEED)\n",
    "\n",
    "# Create MCTS configuration using framework's preset\n",
    "config = create_preset_config(\"BALANCED\")\n",
    "print(f\"\\n\ud83d\udcca MCTS Configuration (BALANCED preset):\")\n",
    "print(f\"   Iterations: {config.iterations}\")\n",
    "print(f\"   Exploration Weight (C): {config.exploration_weight}\")\n",
    "print(f\"   Max Depth: {config.max_depth}\")\n",
    "print(f\"   Random Seed: {MCTS_SEED}\")\n",
    "\n",
    "# Define domain functions for MCTS\n",
    "def generate_actions(state: MCTSState) -> List[str]:\n",
    "    \"\"\"Generate possible actions from current state.\n",
    "    \n",
    "    In a real application, this would generate domain-specific actions\n",
    "    based on the problem state (e.g., chess moves, planning steps).\n",
    "    \"\"\"\n",
    "    depth = len(state.state_id.split(\"_\"))\n",
    "    if depth > config.max_depth:\n",
    "        return []  # Terminal state\n",
    "    return [f\"action_{chr(65 + i)}\" for i in range(3)]  # A, B, C\n",
    "\n",
    "def evaluate_state(state: MCTSState) -> float:\n",
    "    \"\"\"Evaluate state value using seeded RNG for reproducibility.\n",
    "    \n",
    "    Uses the seeded random generator instead of random.uniform()\n",
    "    to ensure deterministic behavior for debugging and testing.\n",
    "    \"\"\"\n",
    "    # Use seeded RNG for reproducibility\n",
    "    return rng.uniform(0.3, 0.9)\n",
    "\n",
    "def transition(state: MCTSState, action: str) -> MCTSState:\n",
    "    \"\"\"Transition to new state by applying action.\"\"\"\n",
    "    return MCTSState(f\"{state.state_id}_{action}\")\n",
    "\n",
    "def is_terminal(state: MCTSState) -> bool:\n",
    "    \"\"\"Check if state is terminal.\"\"\"\n",
    "    return len(state.state_id.split(\"_\")) > config.max_depth\n",
    "\n",
    "# Create and run MCTS using the framework's MCTSEngine\n",
    "print(f\"\\n\ud83c\udfaf Running MCTS simulation using MCTSEngine...\")\n",
    "\n",
    "# Initialize root\n",
    "root = MCTSNode(state=MCTSState(\"root\"))\n",
    "\n",
    "# Run MCTS iterations using the framework's algorithm\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    # Selection - traverse to leaf using UCB1\n",
    "    node = root\n",
    "    while node.children and not node.terminal:\n",
    "        node = node.select_child(config.exploration_weight)\n",
    "    \n",
    "    # Expansion - add children if not terminal\n",
    "    if not node.terminal and node.visits > 0:\n",
    "        actions = generate_actions(node.state)\n",
    "        if actions:\n",
    "            # Use seeded RNG for action selection\n",
    "            action = rng.choice(actions)\n",
    "            child_state = transition(node.state, action)\n",
    "            node = node.add_child(action=action, child_state=child_state)\n",
    "        else:\n",
    "            node.terminal = True\n",
    "    \n",
    "    # Simulation - evaluate using seeded function\n",
    "    value = evaluate_state(node.state)\n",
    "    \n",
    "    # Backpropagation - update all ancestors\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.value_sum += value\n",
    "        node = node.parent\n",
    "\n",
    "# Results\n",
    "print(f\"\\n\ud83d\udcca MCTS Results after {iterations} iterations:\")\n",
    "print(f\"   Root visits: {root.visits}\")\n",
    "print(f\"   Root value: {root.value:.3f}\")\n",
    "print(f\"   Children expanded: {len(root.children)}\")\n",
    "\n",
    "if root.children:\n",
    "    print(f\"\\n\ud83c\udfc6 Best Actions (by visits - most robust selection):\")\n",
    "    sorted_children = sorted(root.children, key=lambda c: c.visits, reverse=True)\n",
    "    for i, child in enumerate(sorted_children[:3], 1):\n",
    "        ucb1 = child.value + config.exploration_weight * (root.visits ** 0.5) / (child.visits + 1)\n",
    "        print(f\"   {i}. {child.action}: visits={child.visits}, value={child.value:.3f}, UCB1={ucb1:.3f}\")\n",
    "    \n",
    "    best = sorted_children[0]\n",
    "    print(f\"\\n   \u2705 Recommended: {best.action} (highest visit count)\")\n",
    "    print(f\"   \ud83d\udca1 Reproducible with seed={MCTS_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 6: Visualize Routing Probabilities\n",
    "\n",
    "Create a visual comparison of how different queries are routed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Test queries for visualization\n",
    "test_queries: List[str] = [\n",
    "    \"Implement a binary search tree\",\n",
    "    \"Compare PostgreSQL vs MongoDB\",\n",
    "    \"Optimize the neural network training\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Design a caching strategy for APIs\",\n",
    "]\n",
    "\n",
    "# Collect predictions\n",
    "rnn_probs: List[List[float]] = []\n",
    "bert_probs: List[List[float]] = []\n",
    "\n",
    "for query in test_queries:\n",
    "    rnn_pred, _ = route_query(query, \"rnn\")\n",
    "    bert_pred, _ = route_query(query, \"bert\")\n",
    "    \n",
    "    rnn_probs.append([\n",
    "        rnn_pred.probabilities['hrm'], \n",
    "        rnn_pred.probabilities['trm'], \n",
    "        rnn_pred.probabilities['mcts']\n",
    "    ])\n",
    "    bert_probs.append([\n",
    "        bert_pred.probabilities['hrm'], \n",
    "        bert_pred.probabilities['trm'], \n",
    "        bert_pred.probabilities['mcts']\n",
    "    ])\n",
    "\n",
    "rnn_probs_arr = np.array(rnn_probs)\n",
    "bert_probs_arr = np.array(bert_probs)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "agents = ['HRM', 'TRM', 'MCTS']\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "x = np.arange(len(test_queries))\n",
    "width = 0.25\n",
    "\n",
    "# RNN Controller\n",
    "ax1 = axes[0]\n",
    "for i, (agent, color) in enumerate(zip(agents, colors)):\n",
    "    ax1.bar(x + i*width, rnn_probs_arr[:, i], width, label=agent, color=color, alpha=0.8)\n",
    "ax1.set_xlabel('Query')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('RNN Meta-Controller Routing')\n",
    "ax1.set_xticks(x + width)\n",
    "ax1.set_xticklabels([f'Q{i+1}' for i in range(len(test_queries))])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# BERT Controller\n",
    "ax2 = axes[1]\n",
    "for i, (agent, color) in enumerate(zip(agents, colors)):\n",
    "    ax2.bar(x + i*width, bert_probs_arr[:, i], width, label=agent, color=color, alpha=0.8)\n",
    "ax2.set_xlabel('Query')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('BERT Meta-Controller Routing')\n",
    "ax2.set_xticks(x + width)\n",
    "ax2.set_xticklabels([f'Q{i+1}' for i in range(len(test_queries))])\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Neural Meta-Controller Agent Routing Comparison', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Print query legend\n",
    "print(\"\\n\ud83d\udcdd Query Legend:\")\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"   Q{i+1}: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfdb\ufe0f Step 7: Full Framework Demo with Gradio UI\n",
    "\n",
    "Launch the complete Gradio interface for interactive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Run the full Gradio app\n",
    "# This launches the complete UI with all features\n",
    "\n",
    "print(\"\ud83c\udfdb\ufe0f Launching Full Framework UI...\")\n",
    "print(\"\\nNote: This will run the complete Gradio interface.\")\n",
    "print(\"Click the public URL to access from any device.\\n\")\n",
    "\n",
    "# Import and run the app\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"app\", f\"{REPO_PATH}/app.py\")\n",
    "app_module = importlib.util.module_from_spec(spec)\n",
    "\n",
    "# This will initialize the framework and launch Gradio\n",
    "# Comment out the next two lines if you just want to explore the code\n",
    "# spec.loader.exec_module(app_module)\n",
    "# app_module.demo.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\u2139\ufe0f To launch the full UI, uncomment the last two lines above and run this cell.\")\n",
    "print(\"   Or run: !python app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Quick Mini Gradio Demo\n",
    "# A lightweight version for quick testing\n",
    "\n",
    "import gradio as gr\n",
    "from typing import Tuple\n",
    "\n",
    "def process_query_mini(query: str, controller_type: str) -> Tuple[str, str, str]:\n",
    "    \"\"\"Process a query with the selected controller.\n",
    "    \n",
    "    Args:\n",
    "        query: User's input query\n",
    "        controller_type: 'RNN' or 'BERT'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response, routing_info, features_info)\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Please enter a query.\", \"\", \"\"\n",
    "    \n",
    "    prediction, features = route_query(query, controller_type.lower())\n",
    "    \n",
    "    # Format routing decision\n",
    "    routing = f\"\"\"\ud83e\udde0 **Meta-Controller Decision**\n",
    "\n",
    "**Selected Agent:** `{prediction.agent.upper()}`\n",
    "**Confidence:** {prediction.confidence:.1%}\n",
    "\n",
    "**Routing Probabilities:**\n",
    "- HRM: {prediction.probabilities['hrm']:.1%} {'\u2588' * int(prediction.probabilities['hrm'] * 20)}\n",
    "- TRM: {prediction.probabilities['trm']:.1%} {'\u2588' * int(prediction.probabilities['trm'] * 20)}\n",
    "- MCTS: {prediction.probabilities['mcts']:.1%} {'\u2588' * int(prediction.probabilities['mcts'] * 20)}\n",
    "\"\"\"\n",
    "    \n",
    "    # Format features\n",
    "    features_str = f\"\"\"\ud83d\udcca **Extracted Features**\n",
    "\n",
    "- Query Length: {features.query_length}\n",
    "- Technical Query: {'Yes' if features.is_technical_query else 'No'}\n",
    "- Has RAG Context: {'Yes' if features.has_rag_context else 'No'}\n",
    "- HRM Confidence: {features.hrm_confidence:.3f}\n",
    "- TRM Confidence: {features.trm_confidence:.3f}\n",
    "- MCTS Value: {features.mcts_value:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    # Simulated agent response\n",
    "    agent_responses = {\n",
    "        \"hrm\": f\"[HRM] Breaking down hierarchically: {query[:80]}...\",\n",
    "        \"trm\": f\"[TRM] Applying iterative refinement: {query[:80]}...\",\n",
    "        \"mcts\": f\"[MCTS] Strategic exploration via tree search: {query[:80]}...\",\n",
    "    }\n",
    "    response = agent_responses.get(prediction.agent, \"Unknown agent\")\n",
    "    \n",
    "    return response, routing, features_str\n",
    "\n",
    "# Create mini Gradio interface\n",
    "with gr.Blocks(title=\"LangGraph Multi-Agent MCTS - Mini Demo\") as mini_demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # \ud83c\udfaf LangGraph Multi-Agent MCTS - Quick Demo\n",
    "    \n",
    "    Test the neural meta-controllers with your own queries!\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Enter your question...\",\n",
    "                lines=3\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            controller_select = gr.Radio(\n",
    "                choices=[\"RNN\", \"BERT\"],\n",
    "                value=\"RNN\",\n",
    "                label=\"Controller Type\"\n",
    "            )\n",
    "    \n",
    "    submit_btn = gr.Button(\"\ud83d\ude80 Process Query\", variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        response_output = gr.Textbox(label=\"Agent Response\", lines=3)\n",
    "    \n",
    "    with gr.Row():\n",
    "        routing_output = gr.Markdown(label=\"Routing Decision\")\n",
    "        features_output = gr.Markdown(label=\"Features\")\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=process_query_mini,\n",
    "        inputs=[query_input, controller_select],\n",
    "        outputs=[response_output, routing_output, features_output]\n",
    "    )\n",
    "\n",
    "# Launch with share=True for public URL\n",
    "mini_demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfad Step 8: Training Your Own Models (Optional)\n",
    "\n",
    "Learn how to train the neural meta-controllers on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "from src.training.data_generator import MetaControllerDataGenerator\n",
    "from src.training.train_rnn import RNNTrainer\n",
    "\n",
    "print(\"\ud83c\udfad Training Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate balanced dataset\n",
    "print(\"\\n\ud83d\udce6 Generating synthetic training data...\")\n",
    "generator = MetaControllerDataGenerator(seed=42)\n",
    "features_list, labels_list = generator.generate_balanced_dataset(samples_per_class=50)\n",
    "\n",
    "print(f\"   Total samples: {len(features_list)}\")\n",
    "print(f\"   Classes: {set(labels_list)}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X, y = generator.to_tensor_dataset(features_list, labels_list)\n",
    "print(f\"   Feature tensor shape: {X.shape}\")\n",
    "print(f\"   Label tensor shape: {y.shape}\")\n",
    "\n",
    "# Split dataset\n",
    "splits = generator.split_dataset(X, y, train_ratio=0.7, val_ratio=0.15)\n",
    "print(f\"\\n\ud83d\udcca Dataset splits:\")\n",
    "print(f\"   Training: {splits['X_train'].shape[0]} samples\")\n",
    "print(f\"   Validation: {splits['X_val'].shape[0]} samples\")\n",
    "print(f\"   Test: {splits['X_test'].shape[0]} samples\")\n",
    "\n",
    "# Quick training demo (3 epochs)\n",
    "print(\"\\n\ud83c\udfcb\ufe0f Training RNN model (3 epochs)...\")\n",
    "trainer = RNNTrainer(\n",
    "    hidden_dim=32,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    early_stopping_patience=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "history = trainer.train(\n",
    "    train_data=(splits[\"X_train\"], splits[\"y_train\"]),\n",
    "    val_data=(splits[\"X_val\"], splits[\"y_val\"]),\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Training complete!\")\n",
    "print(f\"   Best validation accuracy: {history['best_val_accuracy']:.2%}\")\n",
    "print(f\"   Best validation loss: {history['best_val_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\ud83e\uddea Evaluating on test set...\")\n",
    "test_loader = trainer.create_dataloader(splits[\"X_test\"], splits[\"y_test\"], shuffle=False)\n",
    "results = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"   Test accuracy: {results['accuracy']:.2%}\")\n",
    "print(f\"   Test loss: {results['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfc1 Step 9: Chess MCTS Demo (Bonus)\n",
    "\n",
    "The framework includes a chess implementation using MCTS - similar to AlphaZero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import chess\n",
    "    from src.games.chess.game import ChessGame\n",
    "    from src.games.chess.mcts import ChessMCTS\n",
    "    \n",
    "    print(\"\u265a Chess MCTS Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create a chess game\n",
    "    game = ChessGame()\n",
    "    print(f\"\\n\ud83c\udfaf Initial position:\")\n",
    "    print(game.board)\n",
    "    \n",
    "    # Create MCTS player\n",
    "    mcts = ChessMCTS(game, iterations=50, exploration_weight=1.414)\n",
    "    \n",
    "    # Get best move\n",
    "    print(f\"\\n\ud83e\udd14 MCTS thinking (50 iterations)...\")\n",
    "    best_move = mcts.get_best_move()\n",
    "    \n",
    "    print(f\"\\n\u2705 Best move: {best_move}\")\n",
    "    \n",
    "    # Show move statistics\n",
    "    print(f\"\\n\ud83d\udcca Move statistics:\")\n",
    "    for move, stats in mcts.get_move_stats()[:5]:\n",
    "        print(f\"   {move}: visits={stats['visits']}, value={stats['value']:.3f}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(\"\u26a0\ufe0f Chess demo not available\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"\\n\ud83d\udca1 To enable, run: !pip install python-chess\")\n",
    "    print(\"   Then restart the runtime and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Resources & Next Steps\n",
    "\n",
    "### \ud83d\udcd6 Documentation\n",
    "- [Repository README](https://github.com/ianshank/langgraph_multi_agent_mcts)\n",
    "- [Architecture Documentation](https://github.com/ianshank/langgraph_multi_agent_mcts/blob/main/docs/langgraph_mcts_architecture.md)\n",
    "\n",
    "### \ud83d\udee0\ufe0f Key Files\n",
    "- `app.py` - Main Gradio application\n",
    "- `src/agents/meta_controller/` - Neural meta-controllers\n",
    "- `src/framework/mcts/` - MCTS implementation\n",
    "- `src/training/` - Training pipelines\n",
    "\n",
    "### \ud83c\udfc3 Run Locally\n",
    "```bash\n",
    "git clone https://github.com/ianshank/langgraph_multi_agent_mcts.git\n",
    "cd langgraph_multi_agent_mcts\n",
    "pip install -r requirements.txt\n",
    "python app.py\n",
    "```\n",
    "\n",
    "### \ud83d\udc4b Feedback\n",
    "Open an issue on GitHub or contribute to the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udf89 Notebook complete!\")\n",
    "print(\"\\nYou've explored:\")\n",
    "print(\"  \u2705 Neural Meta-Controllers (RNN and BERT with LoRA)\")\n",
    "print(\"  \u2705 Monte Carlo Tree Search (MCTS) with MCTSEngine\")\n",
    "print(\"  \u2705 Agent Routing Visualization\")\n",
    "print(\"  \u2705 Training Pipeline\")\n",
    "print(\"  \u2705 Interactive Gradio Demo\")\n",
    "print(\"\\n\ud83d\ude80 Happy experimenting!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
