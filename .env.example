# LangGraph Multi-Agent MCTS Configuration
# ==========================================
# Copy this file to .env and configure your settings.
# SECURITY WARNING: Never commit the actual .env file to version control!

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================

# Select your LLM provider (openai, anthropic, or lmstudio)
LLM_PROVIDER=openai

# ==============================================================================
# API KEYS (SENSITIVE - KEEP SECURE)
# ==============================================================================
# WARNING: These are sensitive credentials. Never share or commit these values.
# Use a secrets manager in production environments.

# OpenAI API Key (required if LLM_PROVIDER=openai)
# Get your key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key (required if LLM_PROVIDER=anthropic)
# Get your key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Braintrust API Key (optional, for experiment tracking)
# Get your key from: https://www.braintrust.dev/
# BRAINTRUST_API_KEY=sk-your-braintrust-api-key-here

# Pinecone API Key (optional, for vector storage)
# Get your key from: https://www.pinecone.io/
# PINECONE_API_KEY=pcsk_your-pinecone-api-key-here
# PINECONE_HOST=https://your-index.svc.your-environment.pinecone.io

# LangSmith API Key (optional, for tracing and RAG evaluation)
# Get your key from: https://smith.langchain.com/
# LANGSMITH_API_KEY=lsv2_pt_your-langsmith-api-key-here

# Weights & Biases API Key (optional, for experiment tracking)
# Get your key from: https://wandb.ai/authorize
# WANDB_API_KEY=your-wandb-api-key-here

# ==============================================================================
# LOCAL LLM CONFIGURATION (LM Studio)
# ==============================================================================
# Use this for local inference without cloud API calls

# LM Studio API base URL
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# ==============================================================================
# MCTS (Monte Carlo Tree Search) CONFIGURATION
# ==============================================================================

# Enable/disable MCTS for agent decision-making
MCTS_ENABLED=true

# MCTS implementation variant
# Options: baseline (default), neural (requires PyTorch)
MCTS_IMPL=baseline

# Number of MCTS iterations (higher = better quality, slower)
# Recommended: 50-500 for typical use cases
# Range: 1-10000
MCTS_ITERATIONS=100

# MCTS exploration weight (UCB1 constant)
# Higher values favor exploration, lower values favor exploitation
# Default 1.414 (sqrt(2)) is theoretically optimal for many cases
# Range: 0.0-10.0
MCTS_C=1.414

# ==============================================================================
# REPRODUCIBILITY
# ==============================================================================

# Random seed for reproducible results (optional)
# Set to a specific number for deterministic behavior
# Leave unset (commented) for random initialization
# SEED=42

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================

# Application log level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ==============================================================================
# EXPERIMENT TRACKING & EVALUATION
# ==============================================================================

# LangSmith Configuration
# Enable LangChain tracing for evaluation
LANGCHAIN_TRACING_V2=false
LANGSMITH_PROJECT=langgraph-mcts
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# Weights & Biases Configuration
WANDB_PROJECT=langgraph-mcts
# WANDB_ENTITY=your-username-or-team
WANDB_MODE=online  # Options: online, offline, disabled

# ==============================================================================
# OBSERVABILITY (OpenTelemetry)
# ==============================================================================

# OpenTelemetry OTLP exporter endpoint (optional)
# Used for distributed tracing and metrics
# Example: http://localhost:4317 for local collector
# Leave blank to disable telemetry
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# ==============================================================================
# AWS S3 STORAGE CONFIGURATION
# ==============================================================================
# Used for artifact storage and persistence

# S3 bucket name for storing artifacts (optional)
# S3_BUCKET=your-mcts-artifacts-bucket

# S3 key prefix for artifact organization
S3_PREFIX=mcts-artifacts

# AWS region for S3 operations
S3_REGION=us-east-1

# Note: AWS credentials should be configured via:
# - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables
# - AWS credentials file (~/.aws/credentials)
# - IAM role (for EC2/ECS/Lambda deployments)
# Do NOT set AWS credentials in this file!

# ==============================================================================
# NETWORK SECURITY SETTINGS
# ==============================================================================

# HTTP request timeout in seconds
# Prevents hanging on slow/unresponsive services
HTTP_TIMEOUT_SECONDS=30

# Maximum HTTP request retries
# Provides resilience for transient failures
HTTP_MAX_RETRIES=3

# ==============================================================================
# INPUT VALIDATION SECURITY
# ==============================================================================

# Maximum allowed query length in characters
# Prevents denial-of-service via oversized inputs
MAX_QUERY_LENGTH=10000

# Rate limit for API requests per minute
# Set based on your infrastructure capacity
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# ==============================================================================
# PRODUCTION DEPLOYMENT NOTES
# ==============================================================================
#
# For production deployments:
#
# 1. SECRETS MANAGEMENT:
#    - Use a secrets manager (AWS Secrets Manager, HashiCorp Vault, etc.)
#    - Inject secrets as environment variables at runtime
#    - Never store actual secrets in .env files in production
#
# 2. ACCESS CONTROL:
#    - Ensure .env file has restrictive permissions (chmod 600)
#    - Add .env to .gitignore
#    - Use IAM roles instead of long-lived credentials where possible
#
# 3. MONITORING:
#    - Enable OpenTelemetry for observability
#    - Monitor rate limits and adjust based on usage patterns
#    - Set up alerts for authentication failures
#
# 4. NETWORK SECURITY:
#    - Use HTTPS for all external communications
#    - Configure appropriate timeouts
#    - Implement circuit breakers for external services
#
# ==============================================================================
